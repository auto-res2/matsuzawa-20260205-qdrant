{
  "research_topic": "Automated prompt optimization through iterative refinement and performance feedback",
  "queries": [
    "prompt optimization",
    "iterative prompt refinement",
    "feedback-driven prompting"
  ],
  "research_study_list": [
    {
      "title": "Pareto Prompt Optimization",
      "full_text": "Astronomy & Astrophysics manuscript no. ngc5068 ©ESO 2024 February 22, 2024 Possible origins of anomalous Hi gas around MHONGOOSE galaxy, NGC 5068 J. Healy,1 ⋆, W.J.G. de Blok,1,2,3, F.M. Maccagni1,4, P. Amram5, L. Chemin6, F. Combes7,8, B.W. Holwerda9, P. Kamphuis10, D.J. Pisano3, E. Schinnerer11, K. Spekkens12,13, L. Verdes-Montenegro14, F. Walter11, E.A.K. Adams1,2, B.K. Gibson15, D. Kleiner1,4, S. Veronese1,2, N. Zabel3, J. English16 and C. Carignan3,17,18 1Netherlands Institute for Radio Astronomy (ASTRON), Oude Hoogeveensedijk 4, 7991 PD Dwingeloo, The Netherlands 2Kapteyn Astronomical Institute, University of Groningen, PO Box 800, 9700 A V Groningen, The Netherlands, 3Department of Astronomy, University of Cape Town, Private Bag X3, 7701 Rondebosch, South Africa, 4INAF – Osservatorio Astronomico di Cagliari, via della Scienza 5, 09047, Selargius (CA), Italy, 5Aix-Marseille Univ., CNRS, CNES, LAM, 38 rue Frédéric Joliot Curie, 13338 Marseille, France 6Instituto de Astrofisica, Facultad de Ciencias Exactas,Universidad Andres Bello, Chile 7LERMA, Observatoire de Paris, PSL research Université, CNRS, Sorbonne Université, 75104, Paris, France, 8Collège de France, 11 Place Marcelin Berthelot, 75005, Paris, France, 9University of Louisville, Department of Physics and Astronomy, 102 Natural Science Building, 40292 KY Louisville, USA, 10Ruhr University Bochum, Faculty of Physics and Astronomy, Astronomical Institute (AIRUB), 44780 Bochum, Germany, 11Max-Planck-Institut für Astronomie, Königstuhl 17, D-69117, Heidelberg, Germany, 12Department of Physics and Space Science, Royal Military College of Canada P.O. Box 17000, Station Forces Kingston, ON K7K 7B4, Canada, 13Department of Physics, Engineering Physics and Astronomy, Queen’s University, Kingston, ON K7L 3N6, Canada, 14Instituto de Astrofísica de Andalucía (CSIC), Glorieta de la Astronomia s/n, 18008 Granada, Spain, 15E.A. Milne Centre for Astrophysics, University of Hull, HU6 7RX, United Kingdom, 16Department of Physics and Astronomy, University of Manitoba, Winnipeg, Manitoba R3T 2N2, Canada, 17 Département de physique, Université de Montréal, Complexe des sciences MIL, 1375 Avenue Thérèse-Lavoie-Roux Montréal, Qc, Canada H2V 0B3, 18Laboratoire de Physique et de Chimie de l’Environnement, Observatoire d’Astrophysique de l’Université Ouaga I Pr Joseph Ki- Zerbo (ODAUO), BP 7021, Ouaga 03, Burkina Faso 12 February 2024 ABSTRACT The existing reservoirs of neutral atomic hydrogen gas (H i) in galaxies are insufficient to have maintained the observed levels of star formation without some kind of replenishment. This refuelling of the H i reservoirs is likely to occur at column densities an order of magnitude lower than previous observational limits (N H i,limit ∼1019 cm−2 at 30′′ resolution over a linewidth of 20 km s−1). In this paper, we present recent deep H i observations of NGC 5068, a nearby isolated star-forming galaxy observed by MeerKAT as part of the MHONGOOSE survey. With these new data, we are able to detect low column density Hi around NGC 5068 with a 3σdetection limit of NH i = 6.4 ×1017 cm−2 at 90′′resolution over a 20 km s−1 linewidth. The high sensitivity and resolution of the MeerKAT data reveal a complex morphology of the H i in this galaxy – a regularly rotating inner disk coincident with the main star-forming disk of the galaxy, a warped outer disk of low column density gas (N H i <9 ×1019 cm−2), in addition to clumps of gas on the north west side of the galaxy. We employ a simple two disk model that describe the inner and outer disks, and are able to identify anomalous gas that deviates from the rotation of the main galaxy. The morphology and the kinematics of the anomalous gas suggest a possible extra-galactic origin. We explore a number of possible origin scenarios that may explain the anomalous gas, and conclude that fresh accretion is the most likely scenario. Key words. galaxy ISM – galaxy evolution 1. Introduction How galaxies have been able to sustain the observed levels of star formation is one of the key open questions in galaxy evolu- tion. In order for the current observed rate of star formation in the local universe to be maintained, galaxies must refuel the gas reservoirs. How exactly this refuelling takes place is still an open question, however, it has been shown that minor mergers alone cannot supply the required amount of gas necessary to sustain the observed levels of star formation (e.g. Di Teodoro & Fra- ⋆ healy AT astron.nl ternali 2014), meaning that some form of gas accretion must be ongoing (Larson 1972; Sancisi et al. 2008; Ho et al. 2019). There are a number of di fferent ways this accretion of gas can occur: gas-rich mergers, accretion of recycled gas (“fountain model” e.g. Fraternali et al. 2002; Melioli et al. 2008, 2009), or accretion of gas from the cosmic web (van de V oort et al. 2011a; Wetzel & Nagai 2015; Cornuault et al. 2018; Ho et al. 2019; Iza et al. 2022). Simulations have shown that accretion of fresh gas into galaxies generally comes in two modes (Kereš et al. 2005, 2009; Nelson et al. 2013, 2015; Huang et al. 2019). In “hot mode” ac- Article number, page 1 of 17 arXiv:2402.13749v1  [astro-ph.GA]  21 Feb 2024A&A proofs:manuscript no. ngc5068 cretion, the gas is shock heated as it collides with the hydrostatic hot halo near the virial radius, if the gas reaches high enough densities, it can cool and settle onto the disk. However, in “cold mode” accretion, the gas is accreted along filaments or in clumps that are not shock heated. Accretion along filaments has been shown to be more important when considering how the gas is accreted onto the galaxies (e.g. Kereš et al. 2009; van de V oort et al. 2011a; van de V oort & Schaye 2012; Hafen et al. 2020; Ca- diou et al. 2022). Simulations suggest that these filaments have H i column densities of 10 17 to 1018 cm−2 (van de V oort et al. 2011a; van de V oort & Schaye 2012; van de V oort et al. 2019; Ramesh et al. 2023) which has been impossible to observe with the necessary resolution until very recently. Unambiguously identifying accreting gas observationally is extremely difficult due to the low density of the accreting gas and the numerous different possible explanations for “accretion- like” features observed in the gas distribution of nearby galax- ies. Deep observations of the H i in nearby spiral galaxies have identified the presence of extra-planar gas (EPG). This provided an opportunity to study the connection been the gas in the halo and the galaxy disk. EPG has now been identified in many spiral galaxies (see review by Sancisi et al. 2008; Marasco et al. 2019). In edge-on galaxies, such as NGC 891, the EPG is a clearly vis- ible, extended component lagging in velocity (e.g. Swaters et al. 1997; Oosterloo et al. 2007). In studies of more face-on galax- ies such as NGC 2403, the presence of the EPG is inferred by the anomalous velocities of the H i (e.g. Fraternali et al. 2001; de Blok et al. 2014; Li et al. 2023; Veronese et al. 2023). EPG has been shown to account for 10 to 20% of the total Hi mass of the host galaxies (Hess et al. 2009; Gentile et al. 2013; Vargas et al. 2017; Marasco et al. 2019). There has been a concerted e ffort to identify the origin of EPG, particularly in relatively isolated galaxies (Heald et al. 2011; Gentile et al. 2013). While fountain models have been used to explain the presence of EPG in many galaxies (e.g. Li et al. 2023), the fountain models cannot explain all the observed EPG, suggesting other modes of accretion must also be occurring (e.g. NGC 2297, Hess et al. 2009). In order to understand the relationship between the EPG, disk gas, and star formation, deep, high resolution H i observa- tions in combination with multiwavelength tracers of past and ongoing star formation are needed for a representative sample of nearby (<20 Mpc) galaxies. High resolution (sub kiloparsec scale) H i observations enable same-scale comparisons between the H i morphology and kinematics to tracers of star formation activity (e.g. UV and mid- or near-infrared imaging, molecular gas imaging). The high sensitivity is necessary to detect the low column density EPG H i gas. The limitations of previous gen- erations of radio telescopes meant that surveys set out to tackle these questions had to optimise for either resolution or sensitiv- ity. Two of these H i surveys are The H i Nearby Galaxy Sur- vey (THINGS, Walter et al. 2008) and the Westerbork Hydrogen Accretion in LOcal GAlaxieS (HALOGAS, Heald et al. 2011). THINGS is an H i survey with the Very Large Array (VLA) of 34 nearby (D <15 Mpc) gas-rich spiral and dwarf galaxies. The survey made use of the VLA in B, C, and D configurations, which provided the high spatial resolution in combination with the high spectral resolution of the VLA correlator, but H i col- umn density sensitivity was limited to NH i ∼4.5 ×1020 cm−2 at 3σover 20 km s−1 linewidth at the highest resolution of 6′′(Wal- ter et al. 2008). Some key results from THINGS include detailed analyses by Bigiel et al. (2008) and Leroy et al. (2008) of the correlation between H i and molecular hydrogen (H2) at sub-kpc scales with star formation, showing that star formation efficiency is driven by the presence of molecular gas. Bigiel et al. (2008) also confirmed that there is a surface density at which H i satu- rates, and the gas is only molecular. While Leroy et al. (2008) found that where the H i component is the dominant gas phase, star formation e fficiency decreases with increasing radius from the centre of the galaxy. The high resolution imaging also made it possible to detect radial motions related to the EPG compo- nent in a smaller sub-sample of THINGS galaxies (Schmidt et al. 2016). Detecting the radial motions of the gas is key to disentan- gling how gas is transported from the outskirts to the inner star forming regions of galaxies with ongoing accretion. The first systematic search for evidence of accretion of cold gas onto galaxies was the HALOGAS survey, conducted us- ing the Westerbork Synthesis Radio Telescope, which was de- signed to detect the low column density H i and characterise the morphology and kinematics of the detected EPG. The 24 nearby (D <20 Mpc) galaxies chosen were moderately inclined or edge-on as this makes it easier to model and study in detail the morphology and kinematics of the EPG. The deep observations (10 ×12 hours per galaxy) provided high sensitivity H i maps (NH i ∼1.1 ×1019 cm−2 at 3σ for a linewidth of 20 km s −1 at a resolution of 30′′, Heald et al. 2011). Studies of the HALOGAS galaxies have provided much of what is now known about EPG in galaxies as discussed above. Another one of the important re- sults from HALOGAS has been the identification of clouds of anomalous gas (Heald & Team 2014; Kamphuis et al. 2022), similar to the high velocity clouds (HVC) around the Milky Way (Wakker & Van Woerden 1997). Kamphuis et al. (2022) also set important limits on the rate at which Hi can be accreted via these clouds showing that it is much lower than the global accretion rate which is estimated to be 0.2 M⊙/yr by Sancisi et al. (2008). The limitations of the previous surveys have been overcome by the arrival of MeerKAT (Jonas & MeerKAT Team 2016) where it is no longer necessary to choose between sensitivity and resolution. The MeerKAT H i Observations of Nearby Galac- tic Objects - Observing Southern Emitters (MHONGOOSE; de Blok et al. 2016, 2020) is one of the MeerKAT Large Survey Projects, having been awarded a total of 1650 hours to observe the H i in 30 nearby galaxies (55 hours per galaxy). The MHON- GOOSE galaxies were selected from an overlap of galaxies ob- served as part of both the H i Parkes All Sky Survey (HIPASS; Barnes et al. 2001; Meyer et al. 2004) and the Survey for Ion- ization in Neutral Gas Galaxies (SINGG; Meurer et al. 2006). This gave a first estimate of the total H i mass of the sources, and ensured the existence of ultraviolet to infrared observations of the galaxies. All 30 galaxies were also required to be within 30 Mpc, and were chosen to avoid dense environments such as the nearby Fornax and Virgo clusters. In this work, we present the new MeerKAT Hi observations of one of the MHONGOOSE galaxies: NGC 5068 (HIPASS J1318-21). NGC 5068 is a nearby, d = 5.2 Mpc (Anand et al. 2021), close to face-on (i = 35.7 ±10.9◦, Lang et al. 2020) barred spi- ral galaxy (Rosolowsky et al. 2021). It is an actively star form- ing galaxy of intermediate stellar mass (M ⋆ = 2.29 ×109 M⊙, Leroy et al. 2019), located well within the scatter of the so-called star formation “main sequence”. Fig. 1 shows a composite opti- cal/FUV image of NGC 5068 which highlights the star-forming regions in the outer regions of the galaxy. Studies of the H i in this galaxy, have up until now, only made use of observations by single dish telescopes (Koribalski et al. 2004; Sorgho et al. 2019; Sardone et al. 2021) which lack the resolution to identify details in the morphology of the Hi disk. This work provides a first look at the high resolution details of the Hi distribution of NGC 5068 Article number, page 2 of 17Healy et al.: NGC 5068 13h19m20s 00s 18m40s 20s □20◦55′ □21◦00′ 05′ 10′ Right Ascension (J2000) Declination (J2000) 10 kpc Fig. 1. Composite optical /FUV image of NGC 5068. The DECam Legacy Survey (DECaLS; Dey et al. 2019) g,r,z provides the RGB colour. The far UV observed by GALEX as part of the SUNGG Sur- vey (Wong et al. 2016) highlights the pink star formation regions. while still maintaining an Hi column density sensitivity equal to that of the single dish observations. This paper is laid out as follows: in Sec. 2, we present the new MeerKAT Hi observations of NGC 5068 and the process by which we calibrated and imaged the data. In Sec. 3 we discuss the global Hi properties of the galaxy, and compare to previously published observations. A detailed discussion on the kinematics of the H i follows in Sec. 4. Finally, in Sec. 5, we explore dif- ferent possible origins of the anomalous low column density H i gas detected in NGC 5068. 2. MeerKAT observations NGC 5068 was observed as part of MHONGOOSE in ten 5.5 hour tracks (inclusive of time spent on the calibrators) for a total on-source time of 50 hours. In each track, the primary cali- brator, J0408-6545 (rising tracks) or J1939-6342 (setting tracks), was observed for 10 min. The secondary calibrator, J1311-2216, was observed every 50 min for 2 min. The observations were carried out at night between April 2021 and March 2022 us- ing an average of 62 of the 64 antenna which provide baselines of 29 to 7800 m. Each MeerKAT dish has a diameter of 13 .5 m which corresponds to a full width at half max (FWHM) of the primary beam of 55 arcmin at ν= 1420.405 MHz; for more de- tails see de Blok et al. (2024). Each track was observed using the narrow-band (107 MHz) mode of MeerKAT. With this mode, the observations have a native channel width of 3 .3 kHz or 0 .7 km s−1 at 1420 MHz. We bin the channels by a factor of two, to a channel width of 6.6 kHz or 1 .4 km s−1, before starting the calibration process. Due to the shape of the MeerKAT spectral response (van der Byl et al. 2022) individual 0.7 km s−1 channels are independent, implying that our binned 1.4 km s−1 channels are as well. Table 1.General properties of NGC 5068. α, δ(J2000) 13 h18m54.5s −21◦02′17′′ vsys (km s−1) 667 .8 ±1.3a PA (◦) 342 .4 ±3.2b i (◦) 35 .7 ±10.9b D25 (′) 7 .03c d (Mpc) 5 .20 ±0.22d M⋆ (M⊙) 2 .29 ×109 e SFR (M⊙/year) 0 .275 ±0.127 e Notes: a calculated from the global profile in this work, b based on optical properties from Lang et al. (2020), c B-band diameter at 25 mag /arcsec2 isophote from Lauberts & Valentijn (1989), d TRGB from Anand et al. (2021), e Leroy et al. (2019) All 10 tracks were calibrated using the same procedure with version 1.0.5 of the Containerized Automated Radio Astronomy Calibration ( CARAcal) pipeline (Józsa et al. 2020). CARAcal provides a single environment in which to carry out the usual cal- ibration and reduction steps: flagging of data, cross-calibration, splitting of the target, self-calibration, and later the continuum subtraction and spectral line imaging and deconvolution. At each step, the pipeline produces diagnostic plots which help to iden- tify any issues that may arise. These are consolidated outside of CARAcal into a calibration report for each track that is automat- ically uploaded and stored on a private GitHub team repository where each report is used to quality check the observation. 2.1. Self calibration and continuum subtraction We used self-calibration of the continuum to further improve the quality of the calibration. For this we used a continuum image created from the frequency range 1390-1422 MHz, taking care to flag the frequency range covered by the H i emission from both the Galaxy and the target, NGC 5068. Four rounds of self- calibration were used, where in each round a progressively lower signal-to-noise (S/N) threshold is employed to create a mask us- ing the Source Finding Application (SoFiA, Serra et al. 2015), which thus increases the number of sources to be included in the progressively better sky model. The continuum emission of NGC 5068 is bright and extended, and we note the importance of including all of the diffuse continuum emission of NGC 5068 in the sky model. Not doing so leads to the appearance of “ghosts” in the data, mimicking faint H i structures (Grobler et al. 2014; Wijnholds et al. 2016). The model is also used in the first contin- uum subtraction step for the H i spectral line cube. Any residual continuum emission is subtracted using a 1st order polynomial fit to the line-free channels. 2.2. Hi imaging The H i cubes were imaged using WSClean (Offringa et al. 2014; Offringa & Smirnov 2017) as part of CARAcal. We employed a 3-step iterative strategy to cleaning the H i data: first, a low- resolution cube was created and cleaned using a 5 σrms clip WSClean auto-mask. From this low-resolution cube, a mask of the source was created using SoFiA-2 (Westmeier et al. 2021); second, using the newly created low resolution mask for the cleaning, we created a new cube at the desired resolution from which an updated clean-mask was created usingSoFiA-2; in the Article number, page 3 of 17A&A proofs:manuscript no. ngc5068 third step, the final Hi cubes that are presented in this work were created and cleaned using the updated mask from the previous step. In each step, we cleaned the data to 0.5σrms . This procedure is described in more detail in the MHONGOOSE paper present- ing the full survey sample (de Blok et al. 2024). We make full use of MeerKAT’s resolution and sensitivity capabilities by creating a set of six H i cubes with a 1 .5◦ field of view (FoV) that range in resolution (7 ′′ to 90′′ which corre- spond to 0.34 kpc to 4 kpc). This is done using different combi- nations of robust weighting (r) and Gaussian tapering (t), which varies the noise and resolution, and thus the H i column density sensitivity (NH i ∼6.8 ×1019 cm−2 to 6.4 ×1017 cm−2 at 3σover 20 km s−1 – see Table 2). The combinations are listed in the left- most column of Table 2. Using this broad range in sensitivity and resolution, we can characterise the morphology of the H i. Also listed for completeness in Table 2 is the resolution (second column), the σrms based on 1.4 km s−1 wide channels (fourth col- umn), 3σ over 20 km s−1 column density sensitivity (fifth col- umn), and the column density at S /N= 3 in the total intensity maps (sixth column) for each cube. 3. Global Hi properties 3.1. Morphology of the Hi disk The six H i cubes enable us to explore both the details of the dense H i in the galaxy disk, as well as the low column den- sity EPG. For each cube we create a suite of moment maps by using SoFiA-2 to identify all detectable H i emission in the ve- locity range (200 < vrad (km s−1) < 1143) covered by the data cube. We used the smooth and clip algorithm to identify all sig- nal above a threshold of 3 .5σ using using all combinations of the following spatial and spectral smoothing kernels: spatially, these are the original resolution and a spatial smoothing ker- nel equal to the synthesised beam; spectrally, we used the orig- inal resolution and kernels of 9 channels ( ∼12 km s−1), and 25 channels (∼35 km s−1). The source finding yields the H i emis- sion associated with only the target galaxy, NGC 5068, within a field of view of 1 .5◦ which corresponds to a radius of 125 kpc. The primary beam 1 corrected moment maps are created from the masked data cubes, however we further limit the first mo- ment (intensity-weighted mean velocity field) and second mo- ment (sometimes referred to as the velocity dispersion map) maps to only include the pixels where the integrated H i column density (NH i) is greater than the S/N = 3 threshold. This thresh- old is determined for each resolution map by using the S/N map output by SoFiA-2 where S/N = PS dv/(σrms dv √Nchan), S dv is the moment zero, σrms is the noise per channel of the Hi cube, dv is the channel width, and Nchan is the number of channels of the H i cube that contributes to each pixel of the zeroth moment map. This is only valid under the assumption that the channels are independent which in this case is true as the observed chan- nels were binned in pairs and no Hanning smoothing was used. Following Meyer et al. (2017, Eq. 75), we calculate the NH i,S/N=3 threshold by calculating the median column density of the pixels which are in the range 2 .75 < S/N < 3.25 in the S/N map. De- spite the differences in how the values are calculated, the NH i,3σ and NH i,S/N=3 values presented in Table 3 column 5 and 6 re- spectively, are similar although the N H i,S/N=3 values tend to be slightly higher. This is due to a combination of the smooth and clip algorithm used to identify emission associated with a detec- 1 The MeerKAT primary beam FWHM is 55 ′ at z = 0, this entirely contains the H i of NGC 5068 which spans roughly 30′. A B C Fig. 2.Top: Greyscale MeerLICHT q-band image and colour with con- tours from the six di fferent resolution H i cubes. The contours corre- spond to colours in the colourbar which indicate the resolution and the H i column density at the S /N=3 contour in the intensity maps. Three clouds associated with the galaxy are labelled A,B,C. Dotted magenta ellipse indicates the optical extent of the galaxy. The beam for resolu- tion (contour) is shown in the corresponding colour in the box in the bottom left corner.Bottom: H i emission combined with Fig. 1. The red, orange, and yellow show moment 0 maps of the H i emission from the 64.7′′(r05_t60), 29.6′′(r10_t00), and 7.5′′(r00_t00) resolution H i cubes respectively. Production of this image followed English (2017), including the technique of masking in order to combine the data sets and reveal the optical data’s Hii regions (light pink). tion, and the linking lengths used in SoFiA-2 to add pixels con- taining emission to the detection mask which results in a mask that is wider than what would be created using a simple 3σclip. It is worth noting that the column density limits presented in Ta- ble 3 are significantly lower than the previous generation of H i Article number, page 4 of 17Healy et al.: NGC 5068 Table 2.Properties of the H i cubes. Cube Resolution Pixel size σa rms NH i (3σover 20 km s−1)c NH i (S/N= 3)d (arcsec ×arcsec) (arcsec) (mJy /beam) (cm −2) (cm −2) r = 0.0, t = 0′′b 8.1 ×6.9 2 0.215 6 .8 ×1019 9.1 ×1019 r = 0.5, t = 0′′ 13.4 ×9.4 3 0.169 2 .3 ×1019 3.1 ×1019 r = 1.0, t = 0′′ 26.0 ×17.9 5 0.148 5 .6 ×1018 6.5 ×1018 r = 1.5, t = 0′′ 34.3 ×25.6 7 0.153 3 .1 ×1018 3.4 ×1018 r = 0.5, t = 60′′ 65.3 ×63.8 20 0.241 1 .0 ×1018 1.1 ×1018 r = 1.0, t = 90′′ 93.8 ×91.7 30 0.313 6 .4 ×1017 7.0 ×1017 Notes: a this is measured per 1.4 km s−1 channel. b r is the robust weighting, and t the Gaussian taper. c based on 3σdetection in 14 channels. d mean column density at the S/N=3 contour in the moment 0 map, see Section 3.1 for details. □20◦50′ 55′ □21◦00′ 05′ 10′ 15′ Declination (J2000) 10′ N E 2 5 10 20 50 100 200 NHI (1019 cm□2) 13h19m30s 00s 18m30s 00s □20◦50′ 55′ □21◦00′ 05′ 10′ 15′ Right Ascension (J2000) Declination (J2000) 10′ N E 0.2 0.5 1 2 5 10 20 50 100 200 NHI (1019 cm□2) Fig. 3.Integrated intensity (moment 0) maps of NGC 5068. In the top panel is the map from one of the high resolution cubes (r05_t0) at 11′′, and in the bottom panel is a map from one of the low resolution cubes (r05_t60) at 64′′. The dashed magenta ellipse is centred on the optical centre of the galaxy and represents the extent of the optical disk. The contours increase as σS/N=3 ×2n, n = 0,2,4,6,.... The lowest contour plotted is the S /N= 3 contour, NH i = 3.1 ×1019 cm−2 for the top panel and NH i = 1.1 ×1018 cm−2 for the bottom panel, these S /N= 3 (n = 0) values are indicated on the respective colour bars by the orange line. The red ellipse in the bottom left corner of each panel represents the beam. column density limits, and reflect the excellent quality of these deep MeerKAT data. The top panel of Fig. 2 shows the N H i (S/N= 3) contours of the H i distribution seen in the different resolutions plotted on the MeerLICHT (Bloemen et al. 2016) q-band2 image which has a 5σ surface brightness limit of 24 .61 mag/arcsec2. MeerLICHT is a 0.6m optical telescope, located in Sutherland, South Africa, with a 1.6◦×1.6◦FoV (∼1.7×FWHM of the MeerKAT primary beam at z = 0). The bottom panel of Fig. 2 combines Hi moment maps with Fig. 1 and shows spatial relationships between the star-forming regions and atomic gas structures in the form of Hi densities or cavities. The upper panel shows the extent of the gas at different column density levels, with the low column density gas extending out to more than 2.5 times the optical radius, de- fined by the B-band 25 mag/arcsec2 isophote (Lauberts & Valen- tijn 1989), of the galaxy which is approximately 10 kpc. Fig. 2 also shows finger-like tendrils of H i extending out from the optical disk at column densities of N H i ∼3.1 to 9.7 ×1019 cm−2 (green contour). Also of interest are the low-column density clumps on the north-western side of the galaxy which were iden- tified by eye. These clumps appear as distinct clouds below H i column densities of NH i ∼3 ×1019 cm−2 but become connected to the galaxy H i disk in the 60′′(pink) and 90′′(purple) resolu- tion cubes which is likely a resolution e ffect. The more promi- nent clumps have been labelled A, B, C in the top panel of Fig. 2. While Fig. 2 provides an overall picture of the total H i intensity distribution, to be able to study the di fferent features, it is nec- essary to examine the individual moment 0 maps at the various resolutions. Figure 3 shows the moment 0 (Hi intensity) maps at two dif- ferent resolutions: 11′′(0.5 kpc) and 64′′(3 kpc). In the top panel the high resolution ( r05_t0 which corresponds to the robust (r) = 0.5 and taper ( t) = 0′′ cube) map highlights the clumpy nature of the H i, in particular the finger-like tendrils and the clouds on the outskirts of the galaxy. The optical disk is repre- sented by the dotted magenta ellipse measured as the diameter of the 25 mag/arcsec2 isophote (D25) in the B-band by Lauberts & Valentijn (1989). In the bottom panel of Fig. 3, the lower resolu- tion (r05_t60) map shows the extent of the low column density gas which connects the clouds visible in the higher resolution map to the H i in the galaxy. The individual channel maps provide a clearer picture of the fingers and clouds. Fig. 4 shows a selection of channels taken from the r = 1, no taper ( r10_t0) cube, which has a resolu- tion of 21.6′′, that has been Hanning-smoothed with a 5 channel 2 The MeerLICHT q-band is a wide-band that covers theg and r bands. Article number, page 5 of 17A&A proofs:manuscript no. ngc5068 °20±500 550 °21±000 050 100 150 578.0 km/s N E 587.0 km/s N E 596.0 km/s N E 605.0 km/s N E °20±500 550 °21±000 050 100 150 614.0 km/s N E 623.0 km/s N E 632.0 km/s N E 641.0 km/s N E °20±500 550 °21±000 050 100 150 650.0 km/s N E 659.0 km/s N E 668.0 km/s N E 677.0 km/s N E °20±500 550 °21±000 050 100 150 686.0 km/s N E 695.0 km/s N E 704.0 km/s N E 713.0 km/s N E 13h19m30s 00s 18m30s 00s °20±500 550 °21±000 050 100 150 722.0 km/s N E 13h19m30s 00s 18m30s 00s 731.0 km/s N E 13h19m30s 00s 18m30s 00s 740.0 km/s N E 13h19m30s 00s 18m30s 00s 749.0 km/s N E Declination (J2000) Right Ascension (J2000) Fig. 4.Channel maps taken from the r10_t0 cube that has been smoothed with a 5 channel Hanning window and regridded to 3 km s−1 channels. Every third 3 km s−1 channel for a 170 km s−1 range centred on the systemic velocity is shown. The blue contour indicates 3 σrms , the grayscale contours are 3σrms ×2n, n = 2,4,6,... The magenta cross indicates the optical centre of the galaxy, and dotted magenta ellipse indicates the extent of the optical disk ( D25) of the galaxy. The green arrows point to “fingers” in the H i. The orange ellipse in the lower left corner of each panel represents the 21.6′′beam. kernel and regridded to a channel width of 3 km s −1 to increase the signal to noise of the lowest column density features. This smoothed version of the r10_t0 cube provides a good balance between resolution and column density sensitivity. We note two distinct features that are evident in the channel maps: the first be- ing two “fingers”, one which points out towards the north west Article number, page 6 of 17Healy et al.: NGC 5068 Table 3.Global H i properties of NGC 5068 measured from the global profile. Parameter Value S int 167.82 ±0.15 Jy km s−1 MH i (total) 1 .07 ±0.09 ×109 M⊙ w50 67.1 ±0.6 km s−1 w20 109.1 ±2.0 km s−1 vsys 667.8 ±1.3 km s−1 (NW) and the other pointing towards the south east (SE) – both indicated by the green arrows in the Figure. The second feature is that there appears to be two disks: one inner disk that has a major axis running approximately NW-SE and is coincidental with the optical disk, and the second outer disk which appears to be more elliptical and has a major axis running approximately NE-SW. The global profile, extracted from the r = 1.0, t = 90′′ (r10_t90) cube using the associated SoFiA-2 mask, is shown in panel (a) of Fig. 5. For each resolution cube, we determine the mass encapsulated by the mask, the values are shown in panel (b). It is clear that the H i mass does not change significantly between the largest four resolutions which suggests that all the H i mass for the system has been identified. The robust weight- ing used to create the lower resolution H i cubes emphasises the shorter baselines making these measurements more suitable for computing the total H i mass. From the global profile in Fig. 5, we calculated the total in- tegrated flux, the line width at 50% ( w50) and 20% ( w20) of the peak flux are measured directly from the profile, and we deter- mine the systemic velocity to be the velocity at the mid point of the w20. These values are listed in Table 3. While the largest con- tribution to the flux uncertainty are from the systematics arising as part of the calibration process, the uncertainties on the global profile are calculated base on the noise properties of the corre- sponding H i cube. 3.2. Comparison with previous observations Here we compare the total H i flux of NGC 5068 with those obtained from single-dish measurements from the H i Parkes All Sky Survey (HIPASS, Meyer et al. 2004; Koribalski et al. 2004) and targeted observations using the Green Bank Tele- scope (GBT Sorgho et al. 2019; Sardone et al. 2021). Sorgho et al. (2019) and Sardone et al. (2021) analysed a sub-sample of the MHONGOOSE galaxies using GBT. Although they use the same GBT spectra, they process the data di fferently which re- sulted in slightly di fferent global profiles and measurements of the integrated flux for NGC 5068. The total integrated flux mea- surements from each survey are listed in Table 4, and the global profiles are also plotted in Fig. 5(a). As is evident from Table 4, there are differences in measured fluxes between the di fferent single-dish measurements. Despite the differences in the integrated fluxes, the Hi global profile line widths (w20 and w50) measured for NGC 5068 by Sorgho et al. (2019) and Sardone et al. (2021) from the GBT observations, and by Koribalski et al. (2004) from HIPASS are consistent with what we measure from the MeerKAT spectra. While the shape and amplitude of the Sardone et al. (2021) global profile is consistent with the MeerKAT spectrum, the 550 600 650 700 750 800 cz (km/s) 0.0 0.5 1.0 1.5 2.0 Flux density (Jy) (a) GBT (S2019) GBT (S2021) HIPASS MeerKAT r00 t0 r05 t0 r10 t0 r15 t0 r05 t60 r10 t90 HI cube name 0.95 1.00 1.05 MHI (×109 M⊙) (b) Fig. 5. (a) Global H i profile of NGC 5068 from MeerKAT (black), HIPASS (dashed grey; Koribalski et al. 2004), GBT (solid grey and dot-dash grey; Sorgho et al. 2019; Sardone et al. 2021). The green arrow indicates the systemic velocity of the galaxy. (b) The H i mass of NGC 5068 measured within the SoFiA-2 mask for each di fferent resolution cube. The open black square indicates which cube was used to create the global profile shown in the top panel. quoted integrated flux is inconsistent. The higher flux density measurement quoted in Sardone et al. (2021) is due to how the value was calculated: they determined the total flux density of the galaxy by measuring the flux in annuli with increasing ra- dius from the galaxy centre, and then summing the flux in all the annuli. This method of determining the total flux of the galaxy is not the same as the integrated flux based on the global profile. We recalculated the integrated flux from the Sardone et al. global profile to be S H i = 157.7 Jy km s−1 which is consistent within 10% of the MeerKAT value. Given that we do not expect the HIPASS observations of NGC 5068 to missing any flux, it is therefore curious that there is such a large discrepancy between the total integrated flux mea- sured from the MeerKAT spectrum and that which is reported by Koribalski et al. (2004). However, the HIPASS data are known to underestimate the H i emission of large galaxies as a result of the bandpass correction method (Barnes et al. 2001) which likely subtracted some of the H i (see de Blok et al. (2024) for a detailed comparison with all 30 MHONGOOSE galaxies). 4. Kinematics of the Hi In Fig. 6, we present the intensity weighted mean velocity field (moment 1) from the r10_t0 cube at 21 .6′′. For this analysis, we use the r10_t0 cube as it provides a good combination of resolution and H i column density sensitivity. The systemic ve- locity (vsys, listed in Table 3) is measured as the central velocity Article number, page 7 of 17A&A proofs:manuscript no. ngc5068 Table 4.The total integrated flux and linewidth measurements for NGC 5068 from different H i surveys Survey Integrated flux w20 w50 (Jy km s−1) (km s −1) (km s −1) HIPASS (Koribalski et al. 2004) 128.6±12.7 111.0 70.0 GBT (Sorgho et al. 2019) 177 .0±0.7 – 69.3 GBT (Sardone et al. 2021) 191.5 a 108.0 67.9 MeerKAT (this work) 167 .82±0.15 109 .1±2.0 67 .1±0.6 Notes: a S H i = 157.7 Jy km s−1 measured from the global profile assuming a gain of 1.86 K/Jy from Sardone et al. (2021) 13h19m30s 00s 18m30s 00s □20◦55′ □21◦00′ 05′ 10′ 15′ Right Ascension (J2000) Declination (J2000) N E 600 650 700 Moment 1 (km/s) Fig. 6. Velocity field (moment 1) for NGC 5068. Pixels with an H i column density below NH i = 5.9 ×1018 cm−2 (S/N=3) are masked. The colourbar in the bottom of the image indicates the velocity of the gas, the grey contours are spaced 10 km s−1 apart with the black contour in- dicating the systemic velocity, vsys = 667 km s−1. The dotted magenta ellipse is centred on the optical centre of the galaxy and represents the optical size of the galaxy. of the global profile – this is a reasonable assumption given the symmetry of the global profile. Thevsys is indicated by the green arrow in Fig. 5, and by the thick black contour in Fig. 6. The H i kinematics presented in Fig. 6 are rather complex, but there ap- pears to be at least three main components to the velocity field: (1) a regularly rotating inner disk with a mass measured within the ΣH i = 1 M⊙pc−2 contour of Mdisk = 9.7 ×108 M⊙; (2) a sep- arate, inclined warped “disk” which extends to larger radii than the optical disk, that has an H i mass of M outer = 8.9 ×107 M⊙, calculated as all the H i outside the inner disk (including the 3rd component); (3) the north /north western quadrant which is home to the clumpy clouds identified in Fig. 2 which make up on average 8.9 ×104 M⊙each, with the most massive being C at 1.6 ×105 M⊙. The first component is the regularly rotating main inner disk that is coincident with the optical body represented by the ma- genta ellipse in Fig. 6. We find that the ΣH i = 1 M⊙pc−2 contour (which is roughly equivalent to 1.2 ×1020 cm−2, the second low- est contour in the top panel of Fig. 3) neatly encircles this region of the velocity field. This contour ( ΣH i = 1 M⊙pc−2) is usually used to determine the H i diameter of galaxies (e.g. Wang et al. -10 -5 0 5 10 750 700 650 600 550 Oﬀset from centre (arcmin) Velocity (km/s) 0 10 20 50 NHI (×1018 cm□2) Fig. 7.Position-velocity diagram extracted with a width of 21.6′′which corresponds to the size of the beam. The slice is extracted along optical major axis. The inset on the top right of the figure shows the velocity field from Fig. 6, the grey line running from the magenta star to the filled circle represent the path along which the PV slice was extracted, the magenta star and circle are located in the lower left and upper right corners indicating the direction of the slice, these symbols are repeated lower left and right corners of the main figure. The horizontal dashed green line indicates the systemic velocity, while the light green filled circles show the moment 1 velocity at each position along the major axis. The dashed vertical magenta lines correspond to the edge of the optical disk represented by the dotted magenta ellipse in the inset. The blue and orange contours in the colour bar correspond to the blue and greyscale contours on the PV slice. Table 5.H i mass measurements of the different components of the H i disk of NGC 5068. Component H i mass (M⊙) MH i,total 1.07 ×109 Mdisk 9.7 ×108 Mouter 8.9 ×107 Average Mclumps 8.9 ×104 MH i (residual clumpy gas) 2 .6 ×107 2016), a parameter that has a very tight correlation with the H i mass. The contour is also roughly 20 to 50% larger than the op- tical disk (see top panel of Fig. 3), which is consistent with other measurements of late type galaxies (Bosma 2016). Compared to the total H i mass of the system (see Table 5), the inner disk (Mdisk = 9.7 ×108 M⊙) clearly makes up the majority ( ∼90%) of the H i mass of the galaxy. A position velocity (PV) diagram extracted along the optical major axis (see Table 1) is presented in Fig. 7. The PV slice is Article number, page 8 of 17Healy et al.: NGC 5068 -10 -5 0 5 10 750 700 650 600 550 Oﬀset from centre (arcmin) Velocity (km/s) 0 10 20 50 NHI (×1018 cm□2) Fig. 8.Same as Fig. 7 but at an angle of 224◦through the outer disk. extracted using the width of the beam (21.6 ′′) from the r10_t0 cube that has been Hanning smoothed and regridded to 3 km s−1. The PV slice shows that while in the region corresponding to the optical disk (denoted by the vertical magenta lines) there is 13h19m30s 00s 18m30s 00s □20◦55′ □21◦00′ 05′ 10′ 15′ Right Ascension (J2000) Declination (J2000) N E 0 5 10 15 20 25 30 35 Moment 2 (km/s) Fig. 9.Moment 2 map for NGC 5068 at 21.6′′resolution. Pixels with an H i column density below the S/N=3 threshold of NH i = 5.9 ×1018 cm−2 are masked. The colourbar in the bottom of the image indicates the mo- ment 2 velocity of the gas, the thin grey contours are spaced 5 km s −1 apart. The dotted magenta ellipse is centred on the optical centre of the galaxy and represents the optical size of the galaxy. regular rotation as seen in the velocity field (Fig. 6). The light green circles overlaid on the PV slice in Fig. 7 indicate the mo- ment 1 velocity in each line of sight. Anomalous gas is clearly visible in the PV slice, indicated by the regions of gas deviat- ing from the regular rotation and extending to higher and lower velocities. The sudden dip in velocity on the approaching side of the galaxy at roughly −1.4′offset from the centre has similar characteristics to the H i holes identified in NGC 6946 (Kam- phuis & Sancisi 1993; Boomsma et al. 2008), and M31 and M33 (Deul & den Hartog 1990). While the kinematics of the feature are similar to the NGC 6946 holes, there is no obvious hole in the morphology of the H i. Thus it is more likely that the fea- ture in the PV diagram is indicative of some kind of expanding feature or shell or a hole that has not blown out yet which is why there is no corresponding feature in the intensity map. Fea- tures such as holes or shells in the H i distribution are thought to be caused by energetic processes such as supernovae or stellar winds. Since this dip in the PV slice coincides with emission in the continuum and far-UV , both tracers of ongoing and recent star formation, it is likely the aforementioned processes are re- sponsible for the feature. Another noteworthy feature of the PV slice is that there is clearly low column density extraplanar gas distributed throughout the disk. The second component is comprised of the low column den- sity gas (N H i <1019 cm−2) at radii larger than the inner disk. The gas in this component appears to have disk-like kinemat- ics with a position angle of ∼224◦. In Fig. 8 we present a PV diagram through the centre of NGC 5068 at this angle which shows clearly the rotation of this component at larger radii than the inner disk (represented by the vertical magenta lines in the figure). The elliptical geometry of this component suggests that it is more inclined than the inner disk – approximately i ∼53◦. A closer look at the kinematics in Fig. 6 show that the outer re- gions are twisted in an S-shape which suggests a warp. Deep r- (µ3σ = 26.5 mag/arcsec2) and g-band (µ3σ = 27.2 mag/arcsec2) imaging from the DECam Legacy Survey (DECaLS, Dey et al. 2019) Data Release 10 (DR10) shows that there is no stellar counterpart associated with this gas. The differences in the position angle and inclination between this component and the inner disk suggest that this outer disk is not as a result of gas being swept out of the inner disk, but rather it has a separate origin. Given the kinematics and the geome- try of the outer disk relative to the inner disk, we describe it as a warped inclined outer disk which could possibly be a warped polar disk. Characteristically polar disks are highly inclined rela- tive to the inner disk and the position angles can also be very dif- ferent to that of the inner disk – see the prototypical examples of NGC 4650A (Arnaboldi et al. 1997) and NGC 660 (Gottesman & Mahon 1990; van Driel et al. 1995). Without proper modelling (see Deg et al. under review, for detailed modelling of polar disk galaxy candidates), the polar disk scenario is difficult to confirm. Throughout the rest of this work we will primarily refer to this component as the outer disk. The northern region of the galaxy containing the clouds marked A,B,C in Fig. 2 and Fig. 6 makes up the third compo- nent. The kinematics of this region are not consistent with either the inner or outer disks, and thus we consider them a separate component. 4.1. High moment 2 ring In Fig. 9, we present the second moment map at 21 .6′′ resolu- tion for NGC 5068. The moment 2 can be used as a measure of the linewidth of the H i along each line of sight. In the case Article number, page 9 of 17A&A proofs:manuscript no. ngc5068 0 5 10 15 20 550 600 650 700 750 800 Distance along path (arcmin) Velocity (km/s) P1 P2 P3 P4 0 5 10 15 20 NHI (1018 cm□2) 13h19m30s 00s 18m30s 00s □20◦55′ □21◦00′ 05′ 10′ P1 P2 P3 P4 0 4 8 12 16 20 24 28 32 Moment 2 (km/s) 550 600 650 700 750 800 0 1 2 3 P1 σmom2 = 19.3 km/s 550 600 650 700 750 800 □0.25 0.00 0.25 0.50 0.75 P2 σmom2 = 33.8 km/s 550 600 650 700 750 800 0 2 4 P3 σmom2 = 23.6 km/s 550 600 650 700 750 800 0.0 0.2 0.4 P4 σmom2 = 29.4 km/s Velocity (km/s) Flux density (mJy/beam) Fig. 10.Top left: Position-velocity (PV) diagram through the high moment 2 region of NGC 5068. The blue and orange lines in the grayscale colourbar correspond to the blue and grayscale contours. The green dashed line indicates the systemic velocity. Top right: moment 2 map of the galaxy with the black line tracing the path along which the PV slice was extracted, starting from the blue star and ending at the magenta triangle. Bottom: H i line profiles extracted at different locations along the PV slice, the extracted locations are indicated by the magenta arrows in the PV diagram. The location of each spectrum is also indicated by the labelled open circles on the moment 2 map. Green arrows at the bottom of the line profiles indicate the systemic velocity of the galaxy, while the black arrows point to the moment 1 velocity. that the line profiles are Gaussian, then the moment 2 represents the velocity dispersion. Normally the velocity dispersion across the star forming disk is on average∼10 to 15 km s−1 (e.g. Leroy et al. 2008). The median moment 2 value within the optical disk region (indicated by the dashed magenta ellipse in Fig. 9) is σv ∼18 km s−1. Outside the optical disk, there are regions where the moment 2 values are significantly higher ( σv >25 km s−1) than in the inner disk. There are a number of processes that can add energy to the interstellar medium of galaxies, and thus drive turbulence leading to an increase in the velocity disper- sion. Many of these processes are associated with star formation activity, such as stellar feedback and supernova explosions (e.g. Tamburro et al. 2009; Krumholz & Burkhart 2016). Since the high moment 2 values in NGC 5068 are found outside the stellar disk, it is likely that something other than star formation related processes is responsible for the high moment 2 values. In order to investigate the source of the high moment 2 val- ues, we take a PV slice 1 beam wide (21 .6′′) through the ring of high moment 2 values on the outside edge of the optical disk region, where values range from σmom2 ∼22 to 35 km s−1. The PV diagram is presented in the top panel of Fig. 10. It is clear from this PV diagram, as well as the major axis PV diagram in Fig. 7, that there is low column density gas that spans a wide velocity range. It is probable that this broad low column den- sity component is present throughout the H i disk of this system but is hidden in the moment 2 map due to much brighter narrow components, particularly in the inner disk region. However, this is not likely the cause of the high moment 2 ring on the edge of the optical disk. In Fig. 10 we show four line profiles (P1 to P4) extracted at different points along the PV slice. These profiles show that there are multiple distinct overlapping components along the line of sight. This is particularly obvious in P2 and P4. Given that we already know that there are multiple components making up the overall H i disk for NGC 5068, it is likely that the multi- component profiles responsible for the high moment 2 ring are as a result of a superposition of the inner and outer disks. This could also explain the o ffset between the peaks of the di fferent components in P2 and P4. 4.2. Modelling the H i kinematics In order to separate out the inner and outer disks from the third component so that we can investigate the origin of the Hi in this system, it is necessary to create a model that describes the mor- phology and kinematics of the two disks. Modelling the velocity Article number, page 10 of 17Healy et al.: NGC 5068 □20◦50′ 55′ □21◦00′ 05′ 10′ 15′ 578.0 km/s  587.0 km/s  596.0 km/s  605.0 km/s □20◦50′ 55′ □21◦00′ 05′ 10′ 15′ 614.0 km/s  623.0 km/s  632.0 km/s  641.0 km/s □20◦50′ 55′ □21◦00′ 05′ 10′ 15′ 650.0 km/s  659.0 km/s  668.0 km/s  677.0 km/s □20◦50′ 55′ □21◦00′ 05′ 10′ 15′ 686.0 km/s  695.0 km/s  704.0 km/s  713.0 km/s 13h19m30s 00s 18m30s 00s □20◦50′ 55′ □21◦00′ 05′ 10′ 15′ 722.0 km/s 13h19m30s 00s 18m30s 00s 731.0 km/s 13h19m30s 00s 18m30s 00s 740.0 km/s 13h19m30s 00s 18m30s 00s 749.0 km/s Declination (J2000) Right Ascension (J2000) Fig. 11.Same as Fig. 4. The red contours (0.5σdata rms ×2n, n = 0,2,4,..) represent the inner and outer disk models generated with Galmod. field of NGC 5068 using a tilted ring (Rogstad et al. 1974) model is not a trivial task, especially considering the degeneracy of pa- rameters such as position angle, inclination, and rotation velocity of almost face-on galaxies such as this one (Józsa et al. 2007). As discussed in the previous section, there are at least three components to the total H i disk of this galaxy, however using a version of the G ipsy (Allen et al. 1985; van der Hulst et al. 1992) task Galmod as implemented in 3dBarolo (di Teodoro & Fraternali 2015), we have built a simple toy model describing the rotation of the inner and outer disks. For the inner disk, we used the inclination and position angle listed in Table 1 as measured from near-infrared imaging. The inclination of the outer disk was Article number, page 11 of 17A&A proofs:manuscript no. ngc5068 13h19m30s 00s 18m30s 00s □20◦55′ □21◦00′ 05′ 10′ Right Ascension (J2000) Declination (J2000) 0.0 0.2 0.4 0.6 0.8 1.0 Σ HI (M⊙/pc2) 13h19m30s 00s 18m30s 00s □20◦55′ □21◦00′ 05′ 10′ Right Ascension □60 □40 □20 0 20 40 60 Oﬀset from moment 1 (km/s) Fig. 12.Left: H i gas surface density of the clumpy gas not contained in the model overlaid on an GALEX FUV image of NGC 5068. Right: the velocity field of the clumpy gas overlaid on the MeerKAT continuum image, the colour shows difference in moment 1 velocity of the clumpy gas and the total velocity field shown in Fig. 6. set to a constant of 53◦which was estimated from the geometry of the outer H i contours. We experimented with varying the velocity dispersion – us- ing the average velocity dispersion in the different annuli, as well as different constant values (6 to 20 km s−1), but found it had no real impact on the final model. Thus the velocity dispersion was set to a constant 15 km s−1 for both the inner and outer disks as this is similar to the median value of the moment 2 map. The po- sition angles for the warped outer disk, and the rotation veloci- ties for both the inner and outer disk were derived from fits of the parameters to the data using the fitting task, 3dFit, in 3dBarolo. The centre for both the inner and outer disk was set to the optical centre which is consistent with the centre of the H i. The resulting model is shown by the red contours overplot- ted on the channel maps in Fig. 11. The flux of the model is nor- malised by that of the data, and is therefore not an independent parameter. The threshold of the lowest red contour in Fig. 11 is chosen to match the 3 σrms (blue) contour of the data. We note that this simple model describes the two disks fairly well apart from the fingers noted in Fig. 4 and the clouds (A,B,C) to the N/NW of the galaxy which we did not include in the model. In order to find the emission that is not well described by the model, we blank the data where the model is greater than 0.5σrms or greater, this value corresponds to the lowest red con- tour in Fig. 11 which was chosen to match the 3σrms data (blue) contour. This gives a “residual” data cube from which we can create the usual moment maps of the clumpy gas – the gas that is not described by either disk model. We impose two further re- strictions on what we consider as part of the clumpy gas. The first requirement is to only include high signal-to-noise features in the residual maps, we do this by creating a S /N map using a linewidth of 20 km s−1. All pixels in the moment 0 map with S/N<5 are excluded. The second requirement is that the values of the corresponding moment 2 map must be mom2 <40 km s−1 – this is to exclude pixels where the emission could be due to wings of the line not being properly modelled. It would be inter- esting to see if any of the clumpy gas is related to star formation. We therefore compare the moment maps of the anomalous gas with GALEX FUV observations and radio continuum observa- tions. Both are known to trace emission related to recent or ongo- ing star formation. The FUV image is taken from the Survey for Ultraviolet emission in Neutral Gas Galaxies (SUNGG, Wong et al. 2016) which collated new and archival targeted observa- tions for a sample of nearby galaxies. The H i surface density of the clumpy gas is shown overlaid on a GALEX FUV image of NGC 5068 in the left panel of Fig. 12, and the associated ve- locity field is overlaid on the MeerKAT continuum image in the right panel of Fig. 12. From both panels of Fig. 12, we can see that spatially, the clumpy gas is mostly located outside of the main star forming disk, which suggests that this gas is likely not currently involved with any star formation activity. The total mass of this clumpy gas is M clumpy = 2.6 ×107 M⊙ (see Table 5). Since the this gas was not included in the model, it is clearly not rotating with ei- ther the inner disk or the outer disk. The velocity field does show a velocity gradient in the different clumps, however it is unclear what the cause of the gradient may be. However it is evident that the clumpy gas on the northern side of the galaxy is responsible for driving the peculiar morphology of the velocity field in that region – see Fig. 6. 5. Origin of the anomalous gas In the previous section, we have separated out the three compo- nents that comprise the H i of this system: 1–the inner disk that is spatially coincident with the optical disk; 2– the outer disk which has a kinematic warp, and due to its geometry relative to the inner disk could be considered a polar disk; 3–the clumpy gas that is not described by either disk component. In this section we discuss a number of scenarios that could explain the origin of the peculiar behaviour of the H i in this system. 5.1. Interactions with nearby group galaxies Figure 13 shows the eight spectroscopically confirmed galax- ies within a 5 ◦ radius of NGC 5068. NGC 5068 was identi- fied as part of a loose group of galaxies (Pisano et al. 2011), PGC 46400 (Kourkchi & Tully 2017). Karachentsev et al. (2017) measured the distances to NGC 5068 (5 .2 ±0.2 Mpc), LEDA 44681 (7.2 ±0.3 Mpc), UGCA 320 (6.03 Mpc), and UGCA 319 (5.75 Mpc) using the redshift-independent tip of the red giant branch method. They conclude that UGCA 320 and UGCA 319 are an isolated pair that are dynamically separate from NGC 5068 and LEDA 44681. The nearest two galaxies, 2MASX J13292099-2110452 and LEDA 169678, have a projected separation of 405 kpc and a ve- Article number, page 12 of 17Healy et al.: NGC 5068 locity separation of 17 km s −1 and 87 km s−1 respectively. De- spite these two close neighbours, NGC 5068 is classified as an isolated galaxy using the criteria of Verdes-Montenegro et al. (2005): there are no other galaxies with diameters between 1 /4 and 4 times the diameter of the target that lie within a radius that is 20 times the diameter of the potential neighbour. This is based on the assumption that for a galaxy with a diameter of 25 kpc, an interloper of similar mass moving at a “field velocity” of 150 km s−1 would take 3 Gyr to close a separation of 20 times the interlopers diameter (∼500 kpc)3. The right panels of Fig. 13 show the compositegiz images of the group galaxies (excluding NGC 5068) as observed by DE- CaLS DR10. The 5 ×5 arcmin2 colour images show a collection of blue galaxies that are significantly smaller on the sky than NGC 5068 which has a semi-major axis of 7 .03′ (see Table 1). Over the redshift range of these galaxies, 1 ′′ ranges from 40 pc to 57 pc. Based on the optical colours, sizes, and morphologies of this collection of galaxies, it is not likely even the closest two neighbours discussed above have ever interacted with NGC 5068. 5.2. Stripped gas from a passing neighbour What if the anomalous gas is the result of a gas rich unidenti- fied neighbour passing by NGC 5068 and losing all of its gas to NGC 5068? In such scenario the gas may have been stripped off the neighbour which continued on past NGC 5068, and part of the gas settled into the potential of NGC 5068 as the outer disk with the remainder of the gas remaining as clumps beyond the outer disk. We do not believe this to be a likely origin of the anomalous gas. Below we discuss why the scenario is improba- ble using simple assumptions based on the mass of the Hi in the outer disk. If the interaction had taken place, it would have been more than 1 Gyr in the past as the outer gas has already completed at least one orbit as it has settled into a disk-like component within the potential of NGC 5068. Given the H i mass of the outer gas (M H i = 8.9 ×107 M⊙), the stellar mass of the neighbour would be roughly 4.45 ×107 M⊙, this assumes M H i/M⋆ = 2, which is con- sistent with the Small Magellanic Cloud (Stanimirovi ´c et al. 1999) and other dwarf galaxies (Huang et al. 2012). The stellar mass ratio between the two galaxies is 1 : 50, which means that the neighbour would not likely leave an imprint on NGC 5068 as a result of the interaction. Using the M H i–DH i relation from Wang et al. (2016, Eq. 2), and assuming the diameter of the optical disk is 0 .8 ×DH i, we determine the neighbour is roughly Dopt ∼4 kpc (∼ 1.4′ at 5 .2 Mpc). With a conservative stellar mass to light ratio of 0.6, we calculate the mean r-band surface brightness of the neighbour to beµr = 23.7 mag arcsec−2. If this neighbour passed by NGC 5068 with a maximum velocity of v = 100 km s−1, it would be located within a 100 kpc (36 ′) radius after 1 Gyr. The 5 σ detection threshold in the DECaLS r-band image is µr = 26.3 mag arcsec−2 which means that the neighbour would be visible in the DECaLS data, however there is no likely candi- date in the DECaLS imaging. 3 Gas orbital time of NGC 5068 is ∼1 Gyr, meaning that any gas would have settled following such an interaction. 5.3. Minor merger with a gas rich galaxy Since there is no neighbour in our search area, what if NGC 5068 underwent a merger with a gas-rich low mass satellite? Given the H i mass difference between the inner and outer disks, this would suggest a merging ratio of 1:10. Such an event would have to have taken place more than 1 Gyr ago as the outer H i has set- tled in the potential of NGC 5068. Minor mergers are usually defined as where the lower mass galaxy is more than four times less massive than the larger galaxy. Some studies (Lotz et al. 2008; Conselice 2006) have suggested that the timescales for minor mergers can vary from τ∼0.3 to 1 Gyr assuming an ini- tial separation of 30 kpc. More recently, Conselice et al. (2022) have shown using hydronamical simulations that the timescale of mergers is also dependent on both redshift and mass ratio of the merging galaxies. Based on the Conselice et al. (2022) study, the merger timescale for az ∼0 merger with a mass ratio of 1:10, could range between τ∼1.7 to 2 Gyr, however this assumes that the galaxies have M ⋆ >109 M⊙. Given that the H i in the NGC 5068 system has already settled, it is likely that if a minor merger had occurred it would have happened more than 2 Gyr ago. De- spite the mass difference between the constituent galaxies, minor mergers are known to have an impact on the morphology and kinematics of the larger galaxy (e.g. Kazantzidis et al. 2009; Qu et al. 2011; Martin et al. 2018; Ghosh et al. 2022). The optical morphology of NGC 5068 is undisturbed, and does not appear to have undergone any recent interactions. How- ever it is possible that in such a scenario the main stellar disk of NGC 5068 was undisturbed. Mergers are thought to be one of the origin scenarios of polar disks (e.g. Stanonik et al. 2009, and references therein). While there is no clearly observable stellar component within the available deep optical imaging from DE- CaLS associated with the outer disk, it is possible that the outer disk could be caused by a minor merger and the associated stellar disk has been stretched out below detection limits. This scenario, however, does not explain the clouds which are a largely located on the northern side of the galaxy. If all the anomalous gas (all H i gas not associated with the inner disk) from NGC 5068 could be traced back to one event such as a minor merger, given the time that has likely elapsed (>2 Gyr), one would expect that the clouds would have settled into a more symmetric distribution around the galaxy. While we cannot conclusively rule out a minor merger as a possible origin scenario given that this scenario does not neatly explain all the gas, we do not do assign this scenario a high probability of being the origin of the anomalous gas. 5.4. Fountain triggered accretion Galactic fountains can be an important mechanism by which gas is recycled within a galaxy system (see, for example, Shapiro et al. 1976; Fraternali & Binney 2006): gas is expelled from the main disk by processes associated with star formation such as su- pernovae and solar winds. The expelled gas mixes with the hotter gas in the halo and then the mixed gas cools and condenses be- fore it is re-accreted onto the galaxy. This fountain driven accre- tion of gas means that the accreted gas contains both the original expelled gas, and gas from the halo, the net flow of the gas is into the galaxy. Given that this process arises as a result of activity in the stellar disk, the accreted gas is likely to be coincident with the stellar disk. Simulations suggest that gas accreted onto the galaxy subse- quent to mixing with gas ejected from the galaxy disk through fountain processes are likely to have a higher metal content than Article number, page 13 of 17A&A proofs:manuscript no. ngc5068 13h36m 24m 12m 00m □16◦ □18◦ □20◦ □22◦ □24◦ □26◦ Right Ascension (J2000) Declination (J2000) 2MASX J13292099-2110452 651 km/s MCG-04-31-038 685 km/s LEDA 169678 581 km/s LEDA 44681 827 km/s UGCA 320 745 km/s UGCA 319 741 km/s MCG-03-34-002 960 km/s LEDA 886203 NGC 5068 667.7 km/s Pisano+2011 Kourkchi+2017 Other 500 kpc MeerKAT FOV 300 kpc Group R1t AMIGA Isolation Galaxy R2t (Rv) 2MASX J13292099-2110452 cz = 650 km/s MCG-04-31-038 cz = 684 km/s LEDA 169678 cz = 580 km/s LEDA 44681 cz = 827 km/s UGCA 320 cz = 744 km/s UGCA 319 cz = 741 km/s MCG-03-34-002 cz = 960 km/s LEDA 886203 cz = 729 km/s Fig. 13.Nearby galaxies around NGC 5068. The galaxies marked with blue squares or orange circles were identified as group members with NGC 5068 by Pisano et al. (2011) and Kourkchi & Tully (2017). Left panel: sky distribution of the galaxies, the circles represent regions of di fferent sizes used to determine isolation or identify group members. The R1t and R2t circles are taken from Kourkchi & Tully (2017) and are used to group sources together. The AMIGA (Verdes-Montenegro et al. 2005) isolation radius for NGC 5068 is represented by the dotted cyan circle. For reference, also plotted is the MeerKAT field of view (dashed green circle), and the brown dashed circle represents a radius of 300 kpc. The colour giz images on the right are 5 ×5 arcmin2 cutouts taken from DECaLS DR10. primordial gas (see review by Almeida et al. 2014; Brook et al. 2014; Tumlinson et al. 2017) which is thought to occur along fil- aments into the galaxy. Thus looking at the gas phase metallicity of the star forming regions on the edge of the galaxy disk, par- ticularly where the anomalous gas and the star forming regions overlap, may provide some insight into the origin of gas (Howk et al. 2018). NGC 5068 has been observed by a number of multi- wavelength surveys, but two surveys in particular have looked at the gas-phase metallicity of the star forming regions in NGC 5068: PHANGS-MUSE (Emsellem et al. 2022; Williams et al. 2022) and TYPHOON (M. Seibert et al. in prep; Grasha et al. 2022). Both Williams et al. (2022) and Grasha et al. (2022) show that the metallicity of the gas in NGC 5068 cannot be explained by the simple radial model that describes the negative metallic- ity gradient seen in star-forming galaxies. Grasha et al. (2022, Fig. 3) in particular show that the H ii regions on the northern side of the galaxy are more metal poor than regions at the same radius on the southern side of the galaxy. Given the distribution of the star forming regions through- out NGC 5068 (see the background FUV and continuum images in Fig. 12), if fountain-driven accretion was responsible for the anomalous gas in NGC 5068, it is plausible that the distribution of the clumpy gas not explained by the disks would be more equally distributed around the galaxy. We conclude that while it is very likely that there are fountain-processes ongoing in NGC 5068, it is not the likely source of the anomalous H i gas. 5.5. Accretion of gas along a filament The final scenario that we consider is that the anomalous gas is a result of gas accretion external to the galaxy halo. How this accretion occurs, and what the observational signatures of such accretion look like are still unclear. Simulations have shown that in galaxies the size of NGC 5068, accretion is expected to occur along filaments, condensing into clumps and clouds as it nears the galaxy disk (Kaufmann et al. 2006; van de V oort et al. 2011b; Wetzel & Nagai 2015; Cornuault et al. 2018; Iza et al. 2022). Several observational signatures have been suggested as proof of these accretion modes. Due to the gas condensing as it reaches the H i disk, it is expected that the gas is accreted in clouds or clumps. Clouds or clumps with anomalous velocities have been observed around the Milky Way, these clouds, are known as intermediate (IVC) and high (HVC) velocity clouds (see review by Wakker & Van Woerden 1997; Wakker et al. 1999, and references therein). The clouds around the Milky Way have been shown to trace the flow of accreting gas, both as a result of extragalactic accretion (Wakker et al. 2007, 2008; Peek et al. 2008), and re-accretion of gas relating to a galactic fountain (McClure-Griffiths et al. 2013; Marasco et al. 2022). Article number, page 14 of 17Healy et al.: NGC 5068 The left panel of Fig. 12 shows that the anomalous gas associated with NGC 5068 is clumpy, it should be noted that majority of this clumpy gas is very low column density (NH i <1019 cm−2). As discussed in Section 3, the clouds on the north western side of the galaxy range in H i mass from MH i ∼104 M⊙to MH i ∼105 M⊙which is consistent with the Hi masses of the IVC/HVC complexes around the Galaxy (Wakker et al. 2008). The velocity field of the anomalous gas relative to the galaxy velocity field (see Fig. 6) is shown in the right panel of Fig. 12. The velocities of the anomalous gas show that the clouds coincident with the star-forming disk deviate from the overall rotation of the main H i disk. In Section 5.3, we mentioned how mergers are one of the formation scenarios of polar disks. More recently, it has been shown that accretion could also trigger the formation of a polar disk, and this case an associated stellar component would not be expected (Macciò et al. 2006; Brook et al. 2008; Stanonik et al. 2009). Another feature present in polar disks when accre- tion is considered the origin, is kinematic warps (e.g. Brook et al. 2008). While the origin of the warps are not yet clear, it has been shown that they can be explained as the result of skewed angu- lar momentum of gas accreting due to cosmic infall (Ostriker & Binney 1989; Jiang & Binney 1999; Sánchez-Salcedo 2006; Rand & Benjamin 2008). Both the morphology and the kinematics of the anomalous gas in combination of the warped (possibly polar) outer disk of NGC 5068 point to ongoing accretion. It is interesting to note to that some of the clumpy gas is coincident with the optical disk right where there is some of the brightest star formation on the northern ridge of the galaxy. These star forming regions also have some of the lowest gas phase metallicity in the maps presented in Grasha et al. (2022). Putting all of this together, we propose the following sce- nario: gas is being accreted along a filament that is aligned with the north/north western edge of the galaxy. This gas condenses into the clumps/clouds when it gets close to the galaxy disk, over time this accretion has given rise to what is now observable as the outer disk. Within this scenario, it is possible that not all the clouds feed the outer disk, but some are pulled into the star- forming disk enhancing the the star formation on the northern side of galaxy. 6. Summary In this paper we have presented new MeerKAT observations of the H i in NGC 5068 taken as part of the MeerKAT Large Sur- vey Project, MHONGOOSE. The combination of the impressive sensitivity and resolution of these data have revealed a number of interesting features in the H i disk of NGC 5068. We have identified three separate components to the total H i disk: the in- ner disk, the outer disk which shows signatures of a kinematic warp and given the geometry could be a polar disk, and lastly the clumpy region to the north western side of the galaxy. We con- structed a model that contained a regularly rotating inner disk plus a more inclined warped outer disk that described the major- ity of the gas kinematics adequately. Using the model, we were able to isolate a significant amount of clumpy gas that was not well explained by the model of the disks. This clumpy gas ac- counts for ∼ 2% of the total H i mass of the system and also contains the clumps to the north/north west of the galaxy. We explored a number of different possible origin scenarios for the anomalous gas: (a) Interaction with the nearby group galaxies – the projected distances between the galaxies make this an unlikely sce- nario. (b) Stripped gas from a passing neighbour – assuming the neigh- bour survived the passage, we find no evidence for its exis- tence. (c) A minor merger with a gas rich neighbour – we cannot con- clusively exclude this scenario, but do not believe it is the most likely scenario as it does not explain the presence of the clumpy gas on only one side of the galaxy. (d) Fountain triggered accretion – accretion is likely, but this scenario would suggest a more metal enrichment than what is observed in the outer star forming regions, and for the clumpy gas distribution to follow the locations star forming regions. (e) Accretion of gas along a filament – this scenario neatly ex- plains the kinematics and morphology of all the anomalous H i. While the last scenario is the most likely based on the data we have presented in this paper, further detailed modelling of the gas is needed to understand if and how the gas is transported be- tween the inner and outer disks. Unambiguous detection of cold mode accretion of gas onto galaxies will require the detection of H i gas at column densities of NH i ∼5 ×1018 cm−2 or lower, and at cloud scale resolution in combination of other tracers of cold gas such as Lyman- α (van de V oort & Schaye 2012; Ao et al. 2020; Kacprzak 2017). Simulations and some recent observa- tions of high redshift galaxies suggest that Lyman- α emission traces cold gas being accreted onto the galaxies (e.g. Dijkstra & Loeb 2009; Daddi et al. 2021). However, with the currently available facilities, the ongoing MHONGOOSE survey provides the best chance of detecting and studying in detail, the low col- umn density H i that could be linked to cold gas accretion. Acknowledgements. We acknowledge useful discussions on the interpretation of this data with members of the MHONGOOSE team. JH thanks Nathan Deg for helpful discussions on polar ring galaxies. We thank the MeerLICHT team for their help with the MeerLICHT imaging, in particular: Paul Vreeswijk, Danielle Pieterse, Steven Bloemen, Paul Groot, and Patrick Woudt. Thanks to Filippo Fraternali and Thijs van der Hulst for useful discussions about the interpretation of the H i in this galaxy. This project has received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme grant agreement no. 882793, project name Meer- Gas. PK acknowledges financial support by the German Federal Ministry of Education and Research (BMBF) Verbundforschung grant 05A20PC4 (Verbund- projekt D-MeerKAT-II). DJP and NZ are supported through the South African Research Chairs Initiative of the Department of Science and Technology and National Research Foundation. KS acknowledges support from the Natural Sci- ences and Engineering Research Council of Canada (NSERC). BKG acknowl- edges the financial support of the European Union’s Horizon 2020 Research and Innovation Programme (ChETEC-INFRA – Project no. 101008324). LVM acknowledges financial support from the grant CEX2021-001131-S funded by MCIN/AEI/ 10.13039/501100011033, from the grant PID2021-123930OB-C21 funded by MCIN /AEI/10.13039/501100011033, by “ERDF A way of making Europe” and by the European Union. LC acknowledges the financial support from the Chilean Agencia Nacional de Investigación y Desarrollo through the grant Fondecyt Regular 1210992. This paper makes use of MeerKAT data. The MeerKAT telescope is operated by the South African Radio Astronomy Obser- vatory, which is a facility of the National Research Foundation, an agency of the Department of Science and Innovation. This research made use of Astropy,4 a community-developed core Python package for Astronomy (Robitaille et al. 2013; The Astropy Collaboration et al. 2018). Part of the data published here have been reduced using the CARACal pipeline, partially supported by ERC Starting grant number 679627 “FORNAX”, MAECI Grant Number ZA18GR02, DST-NRF Grant Number 113121 as part of the ISARP Joint Research Scheme, and BMBF project 05A17PC2 for D-MeerKAT. Information about CARACal can be obtained online under the URL: https://caracal.readthedocs.io. 4 http://www.astropy.org Article number, page 15 of 17A&A proofs:manuscript no. ngc5068 References Allen, R. J., Ekers, R. D., & Terlouw, J. P. 1985, in Data Anal. Astron. Proc. 1st Work. held Erice, Italy, 1984, Ettore Majorana Int. Sci. Ser., ed. V . D. Gesù, L. Scarsi, P. Crane, J. Friedman, & S. Levialdi (New York: Plenum Press), 271 Almeida, J. S., Elmegreen, B. G., Muñoz-Tuñón, C., & Elmegreen, D. M. 2014, Astron. Astrophys. Rev., 22, 71 Anand, G. S., Lee, J. C., Van Dyk, S. D., et al. 2021, Mon. Not. R. Astron. Soc., 501, 3621 Ao, Y ., Zheng, Z., Henkel, C., et al. 2020, Nat. Astron., 4, 670 Arnaboldi, M., Oosterloo, T., Combes, F., Freeman, K. C., & Koribalski, B. 1997, Astron. J., 113, 585 Barnes, D. G., Staveley-Smith, L., De Blok, W. J. G., et al. 2001, Mon. Not. R. Astron. Soc., 322, 486 Bigiel, F., Leroy, A., Walter, F., et al. 2008, Astron. J., 136, 2846 Bloemen, S., Groot, P., Woudt, P., et al. 2016, in Ground-based Airborne Telesc. VI, V ol. 9906 (SPIE), 990664 Boomsma, R., Oosterloo, T. A., Fraternali, F., Van Der Hulst, J. M., & Sancisi, R. 2008, Astron. Astrophys., 490, 555 Bosma, A. 2016, 3 Brook, C. B., Governato, F., Quinn, T., et al. 2008, Astrophys. J., 689, 678 Brook, C. B., Stinson, G., Gibson, B. K., et al. 2014, Mon. Not. R. Astron. Soc., 443, 3809 Cadiou, C., Dubois, Y ., & Pichon, C. 2022, Mon. Not. R. Astron. Soc., 514, 5429 Conselice, C. J. 2006, Astrophys. J., 638, 686 Conselice, C. J., Mundy, C. J., Ferreira, L., & Duncan, K. 2022, Astrophys. J., 940, 168 Cornuault, N., Lehnert, M. D., Boulanger, F., & Guillard, P. 2018, Astron. As- trophys., 610, A75 Daddi, E., Valentino, F., Rich, R. M., et al. 2021, Astron. Astrophys., 649, A78 de Blok, W., Healy, J., Maccagni, F., & et al. 2024, Astron. Astrophys. de Blok, W. J., Adams, E. A., Amram, P., et al. 2016, in Proc. Sci. (Sissa Medi- alab Srl) de Blok, W. J., Athanassoula, E., Bosma, A., et al. 2020, Astron. Astrophys., 643 de Blok, W. J., Keating, K. M., Pisano, D. J., et al. 2014, Astron. Astrophys., 569, A68 Deul, E. R. & den Hartog, R. H. 1990, Astron. Astrophys., 229, 362 Dey, A., Schlegel, D. J., Lang, D., et al. 2019, Astron. J., 157, 168 Di Teodoro, E. M. & Fraternali, F. 2014, Astron. Astrophys., 567, A68 di Teodoro, E. M. & Fraternali, F. 2015, Mon. Not. R. Astron. Soc., 451, 3021 Dijkstra, M. & Loeb, A. 2009, Mon. Not. R. Astron. Soc., 400, 1109 Emsellem, E., Schinnerer, E., Santoro, F., et al. 2022, Astron. Astrophys., 659, A191 English, J. 2017, Canvas and cosmos: Visual art techniques applied to astronomy data Fraternali, F. & Binney, J. J. 2006, Mon. Not. R. Astron. Soc., 366, 449 Fraternali, F., Oosterloo, T., Sancisi, R., & van Moorsel, G. 2001, Astrophys. J., 562, L47 Fraternali, F., van Moorsel, G., Sancisi, R., & Oosterloo, T. 2002, Astron. J., 123, 3124 Gentile, G., Józsa, G. I., Serra, P., et al. 2013, Astron. Astrophys., 554, A125 Ghosh, S., Saha, K., Jog, C. J., Combes, F., & Di Matteo, P. 2022, Mon. Not. R. Astron. Soc., 511, 5878 Gottesman, S. T. & Mahon, M. E. 1990, Int. Astron. Union Colloq., 124, 209 Grasha, K., Chen, Q. H., Battisti, A. J., et al. 2022, Astrophys. J., 929, 118 Grobler, T. L., Nunhokee, C. D., Smirnov, O. M., van Zyl, A. J., & de Bruyn, A. G. 2014, Mon. Not. R. Astron. Soc., 439, 4030 Hafen, Z., Faucher-Giguère, C. A., Daniel Anglés-Alcázar, et al. 2020, Mon. Not. R. Astron. Soc., 494, 3581 Heald, G., Józsa, G., Serra, P., et al. 2011, Astron. Astrophys., 526, A118 Heald, G. & Team, H. 2014, in Proc. Int. Astron. Union, V ol. 10 (Cambridge University Press), 69–72 Hess, K. M., Pisano, D. J., Wilcots, E. M., & Chengalur, J. N. 2009, Astrophys. J., 699, 76 Ho, S. H., Martin, C. L., & Turner, M. L. 2019, Astrophys. J., 875, 54 Howk, J. C., Rueff, K. M., Lehner, N., et al. 2018, Astrophys. J., 856, 166 Huang, S., Haynes, M. P., Giovanelli, R., & Brinchmann, J. 2012, Astrophys. J., 756, 113 Huang, S., Katz, N., Davé, R., et al. 2019, Mon. Not. R. Astron. Soc., 484, 2021 Iza, F. G., Scannapieco, C., Nuza, S. E., et al. 2022, Mon. Not. R. Astron. Soc., 517, 832 Jiang, I. G. & Binney, J. 1999, Mon. Not. R. Astron. Soc., 303, L7 Jonas, J. L. & MeerKAT Team. 2016, in MeerKAT Sci. Pathw. to SKA, ed. R. Taylor, F. Camilo, L. Leeuw, & K. Moodley (Stellenbosch: Proceedings of Science), 1 Józsa, G. I., Kenn, F., Klein, U., & Oosterloo, T. A. 2007, Astron. Astrophys., 468, 731 Józsa, G. I. G., White, S. V ., Thorat, K., et al. 2020, ASPC, 527, 635 Kacprzak, G. G. 2017, in Gas Accretion onto Galaxies, ed. A. Fox & R. Dav\\’{e} (Springer), 145–165 Kamphuis, J. & Sancisi, R. 1993, Astron. Astrophys., 273, L31 Kamphuis, P., Jütte, E., Heald, G. H., et al. 2022, Astron. Astrophys., 668 [arXiv:2210.09383] Karachentsev, I. D., Makarova, L. N., Tully, R. B., et al. 2017, Mon. Not. R. Astron. Soc. Lett., 469, L113 Kaufmann, T., Mayer, L., Wadsley, J., Stadel, J., & Moore, B. 2006, Mon. Not. R. Astron. Soc., 370, 1612 Kazantzidis, S., Zentner, A. R., Kravtsov, A. V ., Bullock, J. S., & Debattista, V . P. 2009, Astrophys. J., 700, 1896 Kereš, D., Katz, N., Fardal, M., Davé, R., & Weinberg, D. H. 2009, Mon. Not. R. Astron. Soc., 395, 160 Kereš, D., Katz, N., Weinberg, D. H., & Davé, R. 2005, Mon. Not. R. Astron. Soc., 363, 2 Koribalski, B. S., Staveley-Smith, L., Kilborn, V . A., et al. 2004, Astron. J., 128, 16 Kourkchi, E. & Tully, R. B. 2017, Astrophys. J., 843, 16 Krumholz, M. R. & Burkhart, B. 2016, Mon. Not. R. Astron. Soc., 458, 1671 Lang, P., Meidt, S. E., Rosolowsky, E., et al. 2020, Astrophys. J., 897, 122 Larson, R. B. 1972, Nature, 236, 21 Lauberts, A. & Valentijn, E. A. 1989, The Messenger, 56, 31 Leroy, A. K., Sandstrom, K. M., Lang, D., et al. 2019, Astrophys. J. Suppl. Ser., 244, 24 Leroy, A. K., Walter, F., Brinks, E., et al. 2008, Astron. J., 136, 2782 Li, A., Fraternali, F., Marasco, A., et al. 2023, Mon. Not. R. Astron. Soc., 520, 147 Lotz, J. M., Jonsson, P., Cox, T. J., & Primack, J. R. 2008, Mon. Not. R. Astron. Soc., 391, 1137 Macciò, A. V ., Moore, B., & Stadel, J. 2006, Astrophys. J., 636, L25 Marasco, A., Fraternali, F., Heald, G., et al. 2019, Astron. Astrophys., 631, A50 Marasco, A., Fraternali, F., Lehner, N., & Howk, J. C. 2022, Mon. Not. R. Astron. Soc., 515, 4176 Martin, G., Kaviraj, S., Devriendt, J. E., Dubois, Y ., & Pichon, C. 2018, Mon. Not. R. Astron. Soc., 480, 2266 McClure-Griffiths, N. M., Green, J. A., Hill, A. S., et al. 2013, Astrophys. J. Lett., 770 [arXiv:1304.7538] Melioli, C., Brighenti, F., D’Ercole, A., & De Gouveia Dal Pino, E. M. 2008, Mon. Not. R. Astron. Soc., 388, 573 Melioli, C., Brighenti, F., D’Ercole, A., & De Gouveia Dal Pino, E. M. 2009, Mon. Not. R. Astron. Soc., 399, 1089 Meurer, G. R., Hanish, D. J., Ferguson, H. C., et al. 2006, Astrophys. J. Suppl. Ser., 165, 307 Meyer, M., Robotham, A., Obreschkow, D., et al. 2017, Publ. Astron. Soc. Aust. [arXiv:1705.04210] Meyer, M. J., Zwaan, M. A., Webster, R. L., et al. 2004, The HIPASS catalogue - I. Data presentation Nelson, D., Genel, S., V ogelsberger, M., et al. 2015, Mon. Not. R. Astron. Soc., 448, 59 Nelson, D., V ogelsberger, M., Genel, S., et al. 2013, Mon. Not. R. Astron. Soc., 429, 3353 Offringa, A. R., McKinley, B., Hurley-Walker, N., et al. 2014, Mon. Not. R. Astron. Soc., 444, 606 Offringa, A. R. & Smirnov, O. 2017, Mon. Not. R. Astron. Soc., 471, 301 Oosterloo, T., Fraternali, F., & Sancisi, R. 2007, Astron. J., 134, 1019 Ostriker, E. C. & Binney, J. J. 1989, Mon. Not. R. Astron. Soc., 237, 785 Peek, J. E. G., Putman, M. E., & Sommer-Larsen, J. 2008, Astrophys. J., 674, 227 Pisano, D. J., Barnes, D. G., Staveley-Smith, L., et al. 2011, Astrophys. Journal, Suppl. Ser., 197, 28 Qu, Y ., Di Matteo, P., Lehnert, M. D., Van Driel, W., & Jog, C. J. 2011, Astron. Astrophys., 535, A5 Ramesh, R., Nelson, D., & Pillepich, A. 2023, Mon. Not. R. Astron. Soc., 518, 5754 Rand, R. J. & Benjamin, R. A. 2008, Astrophys. J., 676, 991 Robitaille, T. P., Tollerud, E. J., Greenfield, P., et al. 2013, Astron. Astrophys., 558, 33 Rogstad, D. H., Lockart, I. A., & Wright, M. C. H. 1974, Astrophys. J., 193, 309 Rosolowsky, E., Hughes, A., Leroy, A. K., et al. 2021, Mon. Not. R. Astron. Soc., 502, 1218 Sánchez-Salcedo, F. J. 2006, Mon. Not. R. Astron. Soc., 365, 555 Sancisi, R., Fraternali, F., Oosterloo, T., & Van Der Hulst, T. 2008, Astron. As- trophys. Rev., 15, 189 Sardone, A., Pisano, D. J., Pingel, N. M., et al. 2021, Astrophys. J., 910, 69 Schmidt, T. M., Bigiel, F., Klessen, R. S., & de Blok, W. J. 2016, Mon. Not. R. Astron. Soc., 457, 2642 Serra, P., Westmeier, T., Giese, N., et al. 2015, Mon. Not. R. Astron. Soc., 448, 1922 Shapiro, P. R., Field, G. B., Shapiro, P. R., & Field, G. B. 1976, ApJ, 205, 762 Article number, page 16 of 17Healy et al.: NGC 5068 Sorgho, A., Carignan, C., Pisano, D. J., et al. 2019, Mon. Not. R. Astron. Soc. V ol. 482, Issue 1, p.1248-1269, 482, 1248 Stanimirovi´c, S., Staveley-Smith, L., Dickey, J. M., Sault, R. J., & Snowden, S. L. 1999, Mon. Not. R. Astron. Soc., 302, 417 Stanonik, K., Platen, E., Aragón-Calvo, M. A., et al. 2009, Astrophys. J. Lett., 696, L6 Swaters, R. A., Sancisi, R., & van der Hulst, J. M. 1997, Astrophys. J., 491, 140 Tamburro, D., Rix, H. W., Leroy, A. K., et al. 2009, Astron. J., 137, 4424 The Astropy Collaboration, Price-Whelan, A. M., Sip ˝ocz, B. M., et al. 2018, Astron. J., 156, 123 Tumlinson, J., Peeples, M. S., & Werk, J. K. 2017, The Circumgalactic Medium van de V oort, F. & Schaye, J. 2012, Mon. Not. R. Astron. Soc., 423, 2991 van de V oort, F., Schaye, J., Booth, C. M., & Dalla Vecchia, C. 2011a, Mon. Not. R. Astron. Soc., 415, 2782 van de V oort, F., Schaye, J., Booth, C. M., Haas, M. R., & Dalla Vecchia, C. 2011b, Mon. Not. R. Astron. Soc., 414, 2458 van de V oort, F., Springel, V ., Mandelker, N., van den Bosch, F. C., & Pakmor, R. 2019, Mon. Not. R. Astron. Soc. Lett., 482, L85 van der Byl, A., Smith, J., Martens, A., et al. 2022, J. Astron. Telesc. Instruments, Syst., 8, 011006 van der Hulst, J. M., Terlouw, J. P., Begeman, K. G., et al. 1992, in Astron. Data Anal. Softw. Syst. I, A.S.P. Conf. Ser., ed. D. M. Worrall, C. Biemesderfer, & J. Barnes, V ol. 25, 131 van Driel, W., Combes, F., Casoli, F., et al. 1995, Astron. J., 109, 942 Vargas, C. J., Heald, G., Walterbos, R. A. M., et al. 2017, Astrophys. J., 839, 118 Verdes-Montenegro, L., Sulentic, J., Lisenfeld, U., et al. 2005, Astron. Astro- phys., 436, 443 Veronese, S., de Blok, W. J. G., & Walter, F. 2023 [arXiv:2301.13526] Wakker, B. P. & Van Woerden, H. 1997, Annu. Rev. Astron. Astrophys., 35, 217 Wakker, B. P., van Woerden, H., & Gibson, B. K. 1999, ASPC, 166, 311 Wakker, B. P., York, D. G., Howk, J. C., et al. 2007, Astrophys. J., 670, L113 Wakker, B. P., York, D. G., Wilhelm, R., et al. 2008, Astrophys. J., 672, 298 Walter, F., Brinks, E., Blok, W. J. G. D., et al. 2008, Astron. J., 136, 2563 Wang, J., Koribalski, B. S., Serra, P., et al. 2016, Mon. Not. R. Astron. Soc., 460, 2143 Westmeier, T., Kitaeff, S., Pallot, D., et al. 2021, Mon. Not. R. Astron. Soc., 1 Wetzel, A. R. & Nagai, D. 2015, Astrophys. J., 808, 40 Wijnholds, S. J., Grobler, T. L., & Smirnov, O. M. 2016, Mon. Not. R. Astron. Soc., 457, 2331 Williams, T. G., Kreckel, K., Belfiore, F., et al. 2022, Mon. Not. R. Astron. Soc., 509, 1303 Wong, O. I., Meurer, G. R., Zheng, Z., et al. 2016, Mon. Not. R. Astron. Soc., 460, 1106 Article number, page 17 of 17",
      "references": [
        "Canvas and cosmos: Visual art techniques applied to astronomy data",
        "The HIPASS catalogue - I. Data presentation",
        "The Circumgalactic Medium"
      ],
      "meta_data": {
        "arxiv_id": "2402.13749v1",
        "doi": "10.1051/0004-6361/202347475",
        "authors": [
          "J. Healy",
          "W. J. G. de Blok",
          "F. M. Maccagni",
          "P. Amram",
          "L. Chemin",
          "F. Combes",
          "B. W. Holwerda",
          "P. Kamphuis",
          "D. J. Pisano",
          "E. Schinnerer",
          "K. Spekkens",
          "L. Verdes-Montenegro",
          "F. Walter",
          "E. A. K. Adams",
          "B. K. Gibson",
          "D. Kleiner",
          "S. Veronese",
          "N. Zabel",
          "J. English",
          "C. Carignan"
        ],
        "published_date": "2024-02-21T12:20:49Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Presents deep MeerKAT MHONGOOSE H I imaging of isolated star-forming galaxy NGC 5068 reaching a 3σ column-density limit of 6.4×10^17 cm⁻² (90″, 20 km s⁻¹), revealing a multi-component H I structure: (i) a regularly rotating inner disk aligned with the optical disk, (ii) a low-column-density warped, more-inclined outer disk (possibly polar), and (iii) northern/northwestern clumpy anomalous gas. Using a two-disk kinematic model, isolates residual “anomalous” H I (≈2% of total H I mass) whose morphology/kinematics favor an extra-galactic origin; evaluates multiple scenarios and argues ongoing fresh accretion along a filament is most likely.",
        "methodology": "MeerKAT spectral-line reduction with CARAcal (flagging, cross-calibration, multi-round continuum self-calibration, continuum subtraction via sky model + 1st-order polynomial). H I imaging with WSClean and iterative multi-resolution CLEAN masks using SoFiA/SoFiA-2. Source finding via SoFiA-2 smooth-and-clip across spatial/spectral kernels to build masks and moment maps. Kinematic characterization via moment maps and PV diagrams. Toy 3D kinematic modeling with 3dBarolo/GIPSY Galmod: inner disk fixed to optical PA/inclination; outer disk constant inclination estimated from geometry; PA warp and rotation curves fit with 3dFit. Residual cube created by masking emission matched by the model to isolate clumpy/anomalous gas; compared spatially to GALEX FUV and MeerKAT continuum.",
        "experimental_setup": "Target: NGC 5068 (D=5.2 Mpc, i≈36°). Observations: 10×5.5 h MeerKAT tracks (≈50 h on-source), narrow-band 107 MHz, native 0.7 km s⁻¹ channels binned to 1.4 km s⁻¹; baselines 29–7800 m; primary/secondary calibrators observed each track. Imaging products: six cubes spanning 7″–90″ resolution (0.34–4 kpc) via robust weighting + tapers; sensitivity range NH I(3σ,20 km s⁻¹) ≈6.8×10^19 to 6.4×10^17 cm⁻². Validation/benchmarks: flux and linewidth comparison to HIPASS and GBT single-dish profiles; consistency of total H I mass across resolutions; PV slices along optical major axis and along outer-disk PA; checks against deep optical imaging (DECaLS) for stellar counterparts; environmental assessment using nearby galaxy catalog distances/velocities and AMIGA isolation criteria; qualitative consistency with published metallicity maps (PHANGS-MUSE, TYPHOON).",
        "limitations": "Kinematic modeling is intentionally simple (two-disk toy model) with fixed/assumed parameters (e.g., constant outer-disk inclination, constant velocity dispersion) and strong degeneracies due to low inclination (near face-on), limiting uniqueness of PA/inclination/rotation solutions and any inflow/outflow inference. Residual/anomalous-gas identification depends on thresholding (model >0.5σ, S/N and moment-2 cuts), so recovered mass/morphology may be selection-dependent. No direct metallicity measurement of the anomalous H I itself; reliance on H II-region metallicity asymmetries as indirect evidence. Cannot unambiguously confirm “polar disk” nature without more sophisticated tilted-ring/3D warp modeling. Interpretation of accretion vs merger/fountain remains scenario-based; no direct detection of connecting filaments beyond H I sensitivity/geometry constraints.",
        "future_research_directions": "Perform full 3D tilted-ring/warp/polar-disk modeling (varying PA, inclination, radial motions) to quantify warp geometry, derive rotation curves robustly, and test for radial inflow linking outer to inner disk. Use higher-sensitivity/optimized imaging and cloud-finding to characterize cloud mass spectrum, phase structure, and kinematic coherence of the anomalous clumps; search for even fainter NH I <10^18 cm⁻² connections. Combine with multiwavelength constraints: targeted optical/UV spectroscopy of outer-disk/northern H II regions to tie metallicity/ionization to putative accretion; deep optical for ultra-low surface-brightness stellar debris to better rule out merger origins; molecular gas (CO) mapping to see whether anomalous gas fuels star formation. Cross-correlate with CGM tracers (e.g., quasar absorption/Lyα emission) to link H I features to cosmic-web inflow; compare systematically across MHONGOOSE sample to identify statistical signatures of fresh accretion in isolated galaxies.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Localized Zeroth-Order Prompt Optimization",
      "full_text": "Elastic Analysis of Augmented Curves and Constrained Surfaces Esfandiar Nava-Yazdani[0000−0003−4895−739X] Zuse Institute Berlin, Berlin, Germany navayazdani@zib.de https://www.zib.de/members/navayazdani Abstract. The square root velocity transformation is crucial for efficiently em- ploying the elastic approach in functional and shape data analysis of curves. We study fundamental geometric properties of curves under this transformation. Moreover, utilizing natural geometric constructions, we employ the approach for intrinsic comparison within several classes of surfaces and augmented curves, which arise in the real world applications such as tubes, ruled surfaces spherical strips, protein molecules and hurricane tracks. Keywords: Elastic shape analysis · Tube · Manifold-valued · Ruled surface · Hurricane track. 1 Introduction Metric comparison of curves is a core task in a wide range of application areas such as morphology, image and shape analysis, computer vision, action recognition and signal processing. Thereby, a Riemannian structure is highly desirable, since it naturally provides powerful tools, beneficial for such applications. In the recent years, the use of Riemannian metrics for the study of sequential data, such as shapes of curves, trajectories given as longitudinal data or time series, has rapidly grown. In elastic analysis of curves, one considers deformations caused from both bending and stretching. A Riemannian metric, which quantifies the amount of those deformations is called elastic (cf. [18,19]). Therein, in contrast to landmark-based approaches (cf. [12,20,22]), one considers whole continuous curves instead of finite num- ber of curve-points. Consequently, the underlying spaces are infinite dimensional and computational cost becomes a significant issue. The square root velocity (SRV) frame- work provides a convenient and numerically efficient approach for analysing curves via elastic metrics and has been widely used in the recent years (cf. [14,11,4,3] and the comprehensive work [24]). In many applications the curves are naturally manifold-valued. For instance, Lie groups such as the Euclidean motion group, or more generally, symmetric spaces includ- ing the Grassmannian and the Hadamard-Cartan manifold of positive definite matrices are widely used in modelling of real world applications. Extensions of SRV framework from euclidean to general manifold-valued data can be found in [13,27,25,9,26]. arXiv:2402.04944v3  [math.DG]  28 Mar 20242 E. Nava-Yazdani Our contributions are the following. We expose for plane curves the behaviour of speed and curvature under the SRV transformation and geometric invariants. More- over, we apply the elastic approach to augmented curves, determining certain classes of surfaces, tubes, ruled surfaces and spherical strips, as well as hurricane tracks consid- ered with their intensities. We recall that with distance and geodesic at hand, signifi- cant ingredients of statistical analysis such as mean and principal geodesic components as well as approximation and modelling concepts such as splines can be computed. This paper is organized as follows. Section 2, presents the Riemannian setting and notations. Section 3 is devoted to applications. Therein, we consider time series, for which in addition to spatial data, auxiliary information give rise to augmented curves and some classes of surfaces generated by them. Thereby, we apply the elastic approach to both euclidean and spherical trajectories. Future prospects and concluding remarks are presented in 4. For the convenience of those readers primary interested in the applications, we mention that, advanced parts and details from differential geometry, presented in 2, can be skipped. Thereby, the essential point is the use of a framework (SRV) for computation of shortest paths on the spaces of curves and their shapes. 2 Riemannian Framework 2.1 Preliminaries For the background material on Riemannian geometry, we refer to [8] and [10]. Let (M, g) be a finite dimensional Riemannian manifold and M the Fr´ echet manifold of smooth immersed curves from D in M, where D denotes either the unit circle S1 or the unit interval I := [0, 1] for closed or open curves respectively. Moreover, we denote the group of orientation preserving diffeomorphisms on D by Diff +. The following reparametrization invariance is crucial for a Riemannian metric G on M: Gc◦φ(h ◦ φ, k◦ φ) = Gc(h, k), for any c ∈ M, h, k∈ TcM and φ ∈ Diff +. The above equivariance ensures that the induced distance function satisfies the following, which is often desirable in applica- tions: d(c0 ◦ φ, c1 ◦ φ) = d(c0, c1), for any two curves c0 and c1 in M. Similarly, denoting the isometry group of M by Isom(M) and the tangent map of F ∈ Isom(M) by T F, the invariance GF◦c(T F◦ h, TF◦ k) = Gc(h, k), ensures that d(F ◦ c0, F◦ c1) = d(c0, c1). With the above invariances, we can divide out the spaces Isom(M) and Diff +, and consider the natural induced distance dS on the quotient space S = M/(Diff + × Isom(M))Elastic Analysis of Augmented Curves and Constrained Surfaces 3 given by dS([c0], [c1]) = inf {d(c0, f◦ c1 ◦ φ) : φ ∈ Diff +, f∈ Isom(M)} = inf {d(f ◦ c0 ◦ φ, c1) : φ ∈ Diff +, f∈ Isom(M)}. In the context of shape analysis of curves, M and S are called the pre-shape and shape space, respectively. Note that the order of quotient operations does not matter, since the left action of Isom(M) and the right action of Diff + commute. M/Diff + is the space of unparametrized curves and its inherited distance reads inf {d(c0, c1 ◦ φ) : φ ∈ Diff +}. We remark that particular essential challenges are due to the fact that some basic concepts and results from finite dimensional differential geometry such as Hopf-Rinow theorem, do not carry over to the infinite dimensional case. Now, let ∇ be the Levi- Civita connection of M and denote the arc length parameter, speed and unit tangent of c by θ, ω and T respectively. Thus, we have ω = |˙c|, dθ = ωdt and T = ˙c ω , where dot stands for derivation with respect to the parameter t. Due to a remarkable result in [16] the geodesic distance induced by the simplest natural choice, the L2-metric GL2 c (h, k) = Z D gc(h, k) dθ, always vanishes. Consequently, some stronger Sobolev metrics have been considered in several works including [17,7,5]. They are given by Gc(h, k) = nX i=0 Z D aigc(∇i T h, ∇i T k) dθ, with a1 non-vanishing and all ai non-negative, distinguish the curves. We consider first order metrics with constant coefficients. We remark that the coefficients ai can be chosen such that the metric is scale invariant, which is a desired property for some applications in shape analysis. A family of certain weighted Sobolev-type metrics, the so-called elastic metrics, based on a decomposition of derivatives of the vector fields into normal and tangent components, has been introduced in [18,19]: Ga,b c (h, k) = Z D agc((∇T h)⊤, (∇T k)⊤) + bgc((∇T h)⊥, (∇T k)⊥) dθ, with 4b ≥ a >0. In this work, we use the square root velocity (SRV) framework, which allows for a convenient and computationally efficient elastic approach. The main tool in this framework is the square root velocity transformation, which for euclidean M reads q : c 7→ ˙cp |˙c| .4 E. Nava-Yazdani It isometrically maps curves modulo translations, with the metric G1,1/4 to M with the flat L2-metric given by G0(v, w) = Z D g(v(t), w(t))dt. This metric is frequently called (cf. [15,2,6]) flat, to emphasize its footpoint indepen- dence. Note that the elastic metric G1,1 corresponds to the first order Sobolev metric with a0 = 0 and a1 = 1. We remark, that for plane curves, the work [23] has extended the SRV transformation to general parameters a, b >0. For further reading on the SRV framework and applications in shape analysis, we refer to [14], [11] (numerical aspects), the survey [6] and particularly, the comprehensive work [24]. 2.2 Plane Curves A natural question that arises is, how essential geometric characteristics of a curve behave under the SRV transformation. In the following, we provide an answer for speed and curvature in the case of plane curves. Let M = R2, ˜c := q(c) and denote the curvature of c by κ. Note that ˜c does not need to be an immersion. Proposition 1. Denoting the speed of˜c by ˜ω, we have ˜ω = r ˙ω2 4ω + ω3κ2. (1) Moreover, ˜c is an immersion if and only ifκ and ˙ω have no common zeros. In this case, ˜κ˜ω = κω + ˙φ, (2) where ˜κ denotes the curvature of˜c and φ := arctan \u00122ω2κ ˙ω \u0013 . Proof. Let N denote the unit normal of c. With the shorthand notations α := √ω and β := α3κ, a straightforward application of the Frenet equations ˙T = ωκN and ˙N = −ωκT , yields ˙˜c = ˙αT + βN, ¨˜c = (¨α − β2 α )T + ( ˙β + ˙αβ α )N. Thus, we have ˜ω = p ˙α2 + β2, immediately implying (1). Obviously, zeros of ˜ω are common zeros of κ and ˙ω. Thus, ˜c is an immersion if and only if κ and ˙ω have no common zeros. In this case, ˜ κ and φ = arctan (β/ ˙α) = arctan \u0010 2ω2κ ˙ω \u0011 are well-defined and ˜κ˜ω3 = ˜ω2β/α + ˙α ˙β − ¨αβ, which immediately implies the curvature formula (2).Elastic Analysis of Augmented Curves and Constrained Surfaces 5 Next, we apply the proposition to study some geometric quantities, which are invariant under the SRV transformation. For closed curves, integrating the curvature formula above over D = S1 (note that in this case, ˜ω >0 almost everywhere), we see that the SRV transformation preserves the total curvature and particularly the turning number. Moreover, κω is preserved if and only if κ = a d dt \u00001 ω \u0001 with a constant a. Clearly, with κ and ω at hand, utilizing Frenet equations, we can compute c up to rigid motions. The following explicit solution is an immediate application of the above proposition. In light of the above proposition, immersed curves, which are mapped to straight lines, can easily be determined as follows. Example 1. Let a, b, Abe constants withab, A >0, ω(t) = A/ sin2(at+b) and κ = a/ω. A straightforward computation, utilizing the curvature formula (2), implies ˜κ = 0. 2.3 Curves in Homogeneous Spaces For the background material on Lie groups and homogeneous spaces, we refer to [10]. The works [13,27] provide extensions of the SRV framework for euclidean curves to the case of general manifolds. The former has high computational cost, while the latter, transported SRV, depends on a reference point and also suffers from distortion or bias caused by holonomy effects. We use the natural extension to homogeneous spaces exposed in [26,9]. For reader’s convenience, we sketch the core ingredients of the approach and refer to the mentioned works for details and some applications. Let M be a homogeneous space, i.e., M = H/K, where K is a closed Lie subgroup of a Lie Group H. Let ∥·∥ denote the induced norm by a left invariant metric on H, L the tangent map of the left translation, and Imm(D, H) the space of immersed curves from D to H. The SRV transformation is given by Q(α) = (α(0), q(α)), where q(α) = Lα−1 ˙αp ∥ ˙α∥ Here, α−1(t) denotes the inverse element of α(t) in H and H the Lie algebra of H. The map Q is a bijection from Imm(D, H) onto H × L2(D, H). Now, M can be equipped with the Riemannian metric given by the pullback of the product metric of H × L2(D, H) using the map Q and horizontal lifting. Let c1 and c2 be immersed curves in M with horizontal lifts α1 and α2 respectively. The induced distance on M reads d(c1, c2) = inf \u001aq d2 H(α1(0), α2(0)x) + ∥q(α1) − Adx−1 (q(α2)∥2 L2 : x ∈ K \u001b . 3 Applications Frequently, besides spatiotemporal data, represented by a curve γ in a manifold M, there are additional or auxiliary information associated with the curve, thus with the same time-correspondence. These can jointly with γ be comprised and represented as a6 E. Nava-Yazdani so-called augmented curve ˜γ in a higher dimensional manifold ˜M. In some applications, the curve ˜γ uniquely determines a submanifold N of M via a natural construction. An important example is provided, when ˜M is a submanifold of the tangent bundle of M, where the auxiliary information is represented as a vector field along γ and the con- struction is given by the Riemannian exponential map. Significant special cases occur, when M is R3 or the unit two-sphere S2 and N a surface. In the next two subsections, we consider certain classes of surfaces in R3, which often arise in applications and are determined by augmented curves in R4. In the last two subsections, we consider certain spherical regions as well as hurricane tracks together with their intensities. In both cases, we utilize the Riemannian distance from subsection 2.3 to S2 × R, which is a homogeneous space (recall that S2 can be identified with SO(3)/SO(2)). For our example applications, we present geodesic paths representing deformations, minimizing the elastic energy within the SRV framework. We remark, that in a Rie- mannian setting, distance and geodesics are essential Building blocks for many major issues in the morphology and shape analysis, such as computation of mean and test statistics as well as principal component or geodesic analysis. Moreover, besides sta- tistical analysis, also some methods for clustering and classification use Riemannian metrics and geodesics. For the code implementing our approach, which particularly includes Riemannian optimization for the computation of geodesic paths, we utilized our publicly available python package https://github.com/morphomatics, introduced in [1]. 3.1 Tubes A tube or canal surface c is a one-parameter family of circles, whose centers constitute a regular curve γ such that the circles are perpendicular to γ. More precisely, denoting the radii of the circles by r, c(s, .) = γ + r(N cos s + B sin s), 0 ≤ s ≤ 2π, where N and B are the normal and binormal of the curve γ = γ(t), t∈ D, resp. Due to the unique correspondence of c to (γ, r), comparison of tubes reduces to comparison of curves in R4. Figure 1 shows some examples of shortest paths of tubes. Real world applications include a variety of fields such as examination of vein, pipes, capsules and plant roots. Clearly, tubes include surfaces of revolution. 3.2 Ruled Surfaces A ruled surface is formed by moving a straight line segment (possibly with varying length) along a base curve. More precisely, let γ be a curve in R3 and v a unit vector field along γ. Then c(s, .) = γ + sv, s∈ I, parametrizes a ruled surface generated by ( γ, v). Figure 2 depicts an example, where each surface consists of straight line segments connecting the blue (for better visibility) curves γ and γ + v. The class of ruled surfaces includes many prominent surfacesElastic Analysis of Augmented Curves and Constrained Surfaces 7 Fig. 1.Two shortest paths of tubes such as cone, cylinder, helicoid (a minimal surface) and M¨ obius strip. They arise in manufacturing (construction by bending a flat sheet), cartography, architecture and biochemistry (secondary and tertiary structure of protein molecules). Fig. 2.Shortest path of ruled surfaces 3.3 Spherical Strips Let exp denote the exponential map of the unit two-sphere S2. We recall that for any non-zero tangent vector to S2 at a point x: expx(v) = cos(|v|)x + sin(|v|) v |v|8 E. Nava-Yazdani and expx(0) = x. Now, let γ be a curve in S2 with binormal B (cross product of γ and its unit tangent), and r a scalar function along γ. Then, the map c given by c(s, .) := expγ s(rB), s∈ I, parametrizes a spherical strip with bandwidth r. Figure 3 depicts an example of the shortest path between two spherical curves comprised with their bandwidth functions visualised as strips. Fig. 3.Shortest path of spherical strips 3.4 Hurricane Tracks Hurricanes belong to the most extreme natural phenomena and can cause major im- pacts regarding environment, economy, etc. Intensity of a hurricane is determined by the maximum sustained wind (maxwind), monotonically classifying the storms into categories (due to Saffir–Simpson wind scale; for instance, maxwind ≥ 137 knots cor- responds to category 5). Due to their major impacts on economy, human life and envi- ronment, as well as extreme variability and complexity, hurricanes have been studies in a large number of works. For our example, we used the HURDAT 2 database provided by the U.S. National Oceanic and Atmospheric Administration publicly available on https://www.nhc.noaa.gov/data/, supplying latitude, longitude, and maxwind on a 6 hours base of Atlantic hurricanes. Fig. 4.2010 Atlantic hurricane tracks (left) and the shortest path between two of them (right) with color-coded maximum sustained wind (in knots)Elastic Analysis of Augmented Curves and Constrained Surfaces 9 We represent the tracks as discrete trajectories in S2. For further details and com- parison with other approaches, we refer to [24,25] and the recent work [21]. The latter, also provides statistical analysis and a classification of hurricane tracks in terms of their intensities. Fig. 4 illustrates this data set with a visualization of the 2010 hurricane tracks and a shortest path, where the intensities, considered as auxiliary information, are color-marked. 4 Conclusion In this paper, we analysed the behaviour of speed and curvature under the square root velocity framework for elastic approach to plane curves. Moreover, we applied an extension of this framework to homogeneous Spaces, to metrically compare augmented curves and special surfaces, generated by those curves, using a natural construction via the Riemannian exponential map. Our approach, allows for computationally efficient determination of geodesic paths in the shape spaces of the respective classes of surfaces. Our example applications include tubes, ruled surfaces, spherical strips and hurricane tracks. Future work includes further real world applications, particularly concerning statistical analysis of longitudinal data such as comparison of group wise trends within a hierarchical model as well as classification and prediction. Acknowledgements This work was supported through the German Research Foun- dation (DFG) via individual funding (project ID 499571814). References 1. Ambellan, F., Hanik, M., von Tycowicz, C.: Morphomatics: Geometric morphomet- rics in non-Euclidean shape spaces (2021). https://doi.org/10.12752/8544, https:// morphomatics.github.io/ 2. Bauer, M., Bruveris, M., Marsland, S., Michor, P.: Constructing reparametrization in- variant metrics on spaces of plane curves. arXiv: Differential Geometry (2012), https: //arxiv.org/pdf/1207.5965.pdf 3. Bauer, M., Bruveris, M., Charon, N., Møller-Andersen, J.: A relaxed approach for curve matching with elastic metrics. ESAIM: Control, Optimisation and Calculus of Variations 25 (03 2018). https://doi.org/10.1051/cocv/2018053 4. Bauer, M., Bruveris, M., Harms, Philipp Michor, P.W.: Soliton solutions for the elastic metric on spaces of curves. Discrete & Continuous Dynamical Systems - A 38, 1161–1185 (2018). https://doi.org/10.3934/dcds.2018049 5. Bauer, M., Bruveris, M., Michor, P.W.: Overview of the geometries of shape spaces and diffeomorphism groups. Journal of Mathematical Imaging and Vision 50(1-2), 60–97 (2014) 6. Bauer, M., Charon, N., Klassen, E., Brigant, A.L.: Intrinsic riemannian metrics on spaces of curves: theory and computation. arXiv preprint (2020), https://arxiv.org/abs/ 2003.05590 7. Bauer, M., Harms, P., Michor, P.W., et al.: Sobolev metrics on the manifold of all rie- mannian metrics. Journal of Differential Geometry 94(2), 187–208 (2013)10 E. Nava-Yazdani 8. do Carmo, M.P.: Riemannian Geometry. Mathematics: Theory and Applications, Birkh¨ auser Boston, Cambridge, MA, USA, 2 edn. (1992) 9. Celledoni, E., Eidnes, S., Schmeding, A.: Shape analysis on homogeneous spaces: a gener- alised srvt framework. In: Computation and Combinatorics in Dynamics, Stochastics and Control: The Abel Symposium, Rosendal, Norway, August 2016. pp. 187–220. Springer (2018) 10. Gallot, S., Hullin, D., Lafontaine, J.: Riemannian Geometry. Universitext, Springer, Berlin, 3 edn. (2004) 11. Huang, W., Gallivan, K.A., Srivastava, A., Absil, P.A.: Riemannian optimization for registration of curves in elastic shape analysis. Journal of Mathematical Imaging and Vision 54(3), 320–343 (2016) 12. Kendall, D., Barden, D. Carne, T., Le, H.: Shape and Shape Theory. John Wiley & Sons (1999) 13. Le Brigant, A.: Computing distances and geodesics between manifold-valued curves in the srv framework. Journal of Geometric Mechanics 9(2) (2017) 14. Liu, W., Srivastava, A., Zhang, J.: Protein structure alignment using elastic shape anal- ysis. In: Proceedings of the First ACM International Conference on Bioinformatics and Computational Biology. pp. 62–70 (2010) 15. Michor, P., Mumford, D., Shah, J., Younes, L.: A metric on shape space with explicit geodesics. Atti Accad. Naz. Lincei Cl. Sci. Fis. Mat. Natur. Rend. Lincei (9) Mat. Appl. 19 (07 2007). https://doi.org/10.4171/RLM/506 16. Michor, P.W., Mumford, D.: Vanishing geodesic distance on spaces of submanifolds and diffeomorphisms. Documenta Mathematica 10, 217–245 (2005) 17. Michor, P.W., Mumford, D.: An overview of the riemannian metrics on spaces of curves using the hamiltonian approach. Applied and Computational Harmonic Analysis 23(1), 74–113 (2007) 18. Mio, W., Srivastava, A., Joshi, S.H.: On shape of plane elastic curves. International Journal of Computer Vision 73, 307–324 (2006) 19. Mio, W., Srivastava, A., Joshi, S.: On shape of plane elastic curves. International Journal of Computer Vision 73, 307–324 (07 2007). https://doi.org/10.1007/s11263-006-9968-0 20. Nava-XYazdani, E., Hege, H.C., Sullivan, T.J., von Tycowicz, C.: Geodesic analysis in kendall’s shape space with epidemiological applications. Journal of Mathematical Imaging and Vision pp. 1–11 (2020). https://doi.org/10.1007/s10851-020-00945-w 21. Nava-Yazdani, E., Ambellan, F., Hanik, M., von Tycowicz, C.: Sasaki metric for spline models of manifold-valued trajectories. Computer Aided Geometric Design 104, 102220 (2023). https://doi.org/10.1016/j.cagd.2023.102220 22. Nava-Yazdani, E., Hege, H.C., von Tycowicz, C.: A hierarchical geodesic model for longi- tudinal analysis on manif olds. Journal of Mathematical Imaging and Vision 64(4), 395 – 407 (2022). https://doi.org/10.1007/s10851-022-01079-x 23. Needham, T., Kurtek, S.: Simplifying transforms for general elastic metrics on the space of plane curves. SIAM Journal on Imaging Sciences 13(1), 445–473 (2020). https://doi.org/10.1137/19M1265132 24. Srivastava, A., Klassen, E.P.: Functional and shape data analysis, vol. 1. Springer (2016) 25. Su, Z., Klassen, E., Bauer, M.: The square root velocity framework for curves in a ho- mogeneous space. 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW) pp. 680–689 (2017) 26. Su, Z., Klassen, E., Bauer, M.: Comparing curves in homogeneous spaces. Differential Geometry and its Applications 60, 9–32 (2018) 27. Zhang, Z., Su, J., Klassen, E., Le, H., Srivastava, A.: Rate-invariant analysis of covariance trajectories. Journal of Mathematical Imaging and Vision 60, 1306–1323 (2018)",
      "references": [
        "Morphomatics: Geometric morphomet- rics in non-Euclidean shape spaces",
        "Constructing reparametrization in- variant metrics on spaces of plane curves.",
        "A relaxed approach for curve matching with elastic metrics.",
        "Soliton solutions for the elastic metric on spaces of curves.",
        "Overview of the geometries of shape spaces and diffeomorphism groups.",
        "Intrinsic riemannian metrics on spaces of curves: theory and computation.",
        "Sobolev metrics on the manifold of all rie- mannian metrics.",
        "Riemannian Geometry.",
        "Shape analysis on homogeneous spaces: a gener- alised srvt framework.",
        "Riemannian optimization for registration of curves in elastic shape analysis.",
        "Shape and Shape Theory.",
        "Computing distances and geodesics between manifold-valued curves in the srv framework.",
        "Protein structure alignment using elastic shape anal- ysis.",
        "A metric on shape space with explicit geodesics.",
        "Vanishing geodesic distance on spaces of submanifolds and diffeomorphisms.",
        "An overview of the riemannian metrics on spaces of curves using the hamiltonian approach.",
        "On shape of plane elastic curves.",
        "Geodesic analysis in kendall’s shape space with epidemiological applications.",
        "Sasaki metric for spline models of manifold-valued trajectories.",
        "A hierarchical geodesic model for longi- tudinal analysis on manif olds.",
        "Simplifying transforms for general elastic metrics on the space of plane curves.",
        "Functional and shape data analysis",
        "The square root velocity framework for curves in a ho- mogeneous space.",
        "Comparing curves in homogeneous spaces.",
        "Rate-invariant analysis of covariance trajectories."
      ],
      "meta_data": {
        "arxiv_id": "2402.04944v3",
        "authors": [
          "Esfandiar Nava-Yazdani"
        ],
        "published_date": "2024-02-07T15:25:20Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Studies geometric effects of the Square Root Velocity (SRV) transform in elastic shape analysis, specifically deriving how speed and curvature of planar curves change under SRV and identifying invariants (e.g., total curvature/turning number for closed curves). Extends and applies an SRV-based elastic framework on homogeneous spaces to intrinsically compare augmented curves that encode auxiliary attributes, enabling intrinsic comparison of constrained surface classes determined by such curves (tubes/canal surfaces, ruled surfaces, spherical strips) and manifold-valued trajectories such as hurricane tracks with intensity.",
        "methodology": "Uses reparameterization- and isometry-invariant Riemannian metrics on spaces of immersed curves. Employs first-order elastic Sobolev-type metrics Ga,b and the SRV transform q(c)=\\dot c/\\sqrt{|\\dot c|} (Euclidean case) to convert elastic geodesic computations into L2 computations. For planar curves, applies Frenet-frame calculus to derive explicit formulas for transformed speed and curvature and characterize when SRV output is an immersion; deduces preserved quantities. For manifold-valued data, uses the generalized SRV transform on Lie groups/homogeneous spaces via left translation to the Lie algebra, horizontal lifting, and optimization over isotropy subgroup K to obtain distances invariant to group actions. Applications model auxiliary information by augmenting curves (e.g., centerline+radius, base curve+direction field, spherical curve+bandwidth, track+intensity in S2×R) and compute geodesic deformations using Riemannian optimization (implemented with the Morphomatics Python package).",
        "experimental_setup": "Primarily demonstration-based rather than a standardized benchmark evaluation. Computes and visualizes shortest paths/geodesics in the relevant (pre-)shape spaces for: (i) tubes represented by curves in R^4 (centerline in R^3 plus radius), (ii) ruled surfaces represented by (γ,v) with γ⊂R^3 and a unit vector field v along γ, (iii) spherical strips represented by (γ,r) on S^2 using the spherical exponential map along binormal directions, and (iv) hurricane tracks represented as discrete trajectories on S^2 augmented with maximum sustained wind (intensity) in R, i.e., curves in S^2×R. Uses the HURDAT2 Atlantic hurricane database (NOAA; 6-hourly latitude/longitude and maxwind) and illustrates 2010 tracks plus an example shortest path with color-coded intensity. Validation is qualitative via geometric plausibility of computed geodesic interpolations and adherence to invariances (reparameterization/isometries) rather than quantitative error metrics.",
        "limitations": "Focuses on theory and illustrative examples; lacks quantitative benchmarking, ablation studies, runtime/accuracy comparisons against alternative elastic/Sobolev or nonelastic methods, and broader statistical evaluations. SRV-based isometry to flat L2 holds for specific elastic parameter choices (notably G_{1,1/4} in Euclidean space); extensions to other (a,b) require different transforms and are not developed here. For planar SRV geometry results, immersion of the transformed curve fails when curvature and speed derivative share zeros, and results rely on Frenet-frame regularity assumptions. Manifold/homogeneous-space SRV distances require choosing horizontal lifts and optimizing over the isotropy subgroup; practical computations can be sensitive to discretization and may still face distortion issues in more general manifold settings (e.g., holonomy in other SRV variants is acknowledged). Surface classes considered are constrained (tubes, ruled surfaces, spherical strips) and rely on unique parameterizations by augmented curves; more general surfaces are not covered.",
        "future_research_directions": "Extend to additional real-world domains and richer surface/trajectory models beyond the constrained classes studied (e.g., more general submanifolds, varying topology, or surfaces not uniquely determined by a single augmented curve). Develop full statistical pipelines on these shape spaces: Fréchet means, hypothesis tests, principal geodesic analysis, and uncertainty quantification for longitudinal/functional data. Pursue hierarchical/geodesic models for group-wise trend comparison, and integrate classification and prediction tasks (e.g., hurricane track/intensity forecasting) using elastic-geodesic features. Provide quantitative benchmarks (accuracy, robustness, runtime) and comparisons to alternative manifold-trajectory and surface-matching approaches; study numerical stability and discretization effects. Investigate SRV-like transforms for broader elastic metric parameters (general a,b), scale invariance, and mitigate biases in manifold settings (e.g., holonomy-related effects) for augmented curves on homogeneous spaces.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "System Prompt Optimization with Meta-Learning",
      "full_text": "From explained variance of correlated components to PCA without orthogonality constraints Marie Chavent∗ Guy Chavent† Abstract Block Principal Component Analysis (Block PCA) of a data matrix A, where loadings Z are determined by maximization of ∥AZ∥2 over unit norm orthogonal loadings, is difficult to use for the design of sparse PCA by ℓ1 regularization, due to the difficulty of taking care of both the orthog- onality constraint on loadings and the non differentiable ℓ1 penalty. Our objective in this paper is to relax the orthogonality constraint on loadings by introducing new objective functions expvar(Y ) which measure the part of the variance of the data matrix A explained by correlated components Y = AZ. So we propose first a comprehensive study of mathematical and numerical properties of expvar( Y ) for two existing definitions Zou et al. [2006], Shen and Huang [2008] and four new definitions. Then we show that only two of these explained variance are fit to use as objective function in block PCA formulations for A rid of orthogonality constraints. Keywords: PCA, sparsity, dimension reduction, explained variance, orthogo- nality constraints, block optimization. ∗Universit´ e de Bordeaux, CNRS, INRIA, Bordeaux INP, IMB, UMR 5251, Talence, France e-mail : marie.chavent@u-bordeaux.fr (corresponding author) †Retired, collaborator to the SERENA Project Team, INRIA-Paris 2 rue Simone Iff, 75589 Paris, France arXiv:2402.04692v1  [stat.ML]  7 Feb 20241 Introduction Many linear data analysis methods construct new variables that “best” summa- rize the columns of a n × p data matrix A where n observations are described by p numerical variables. The m < pnew variables are the columns of Y = AZ where Z is the p × m loading matrix. These new variables are for instance principal components in PCA (Principal Component Analysis), canonical com- ponents in CA (Canonical Analysis) or PLS components in PLS (Partial Least Squares) regression. When the components are orthogonal (which is the case for PCA), it is usual to assess the quality of the components by measuring the part of the variance of A explained by Y = AZ with : expvar(AZ) = ∥AZ∥2 F , where the subscript F denotes the Frobenius norm. But this definition fails when the components are correlated. Correlated components appear when sparsity is introduced into the loading matrix Z to select the important original variables. The loading vectors and the components are no longer necessarily orthogonal. Two definitions have already been pro- posed to measure the explained variance of correlated components : theadjusted variance of Zou et al. [2006] and the total variance of Shen and Huang [2008]. Because there is no single definition, we introduce a set of three conditions to be satisfied by any explained variance definition and we propose a comprehensive study of mathematical and numerical properties of these two existing definitions together with four new definitions. We prove first that the total variance of Shen and Huang [2008] called sub- space explained variance hereafter, satisfies only two of three conditions. This lead us to propose other definitions by projection or by normalization on a set X = [x1, ..., xm] of orthonormal vectors which “points in the same direction” as the components Y = [ y1, ..., ym]. Such orthonormal vectors X can be as- sociated to Y by three different rules: QR or UP (polar) decomposition of Y , or maximization of the explained variance. This leads to five definitions ( two normalized and three projected explained variance) : - the QR and the UP normalized explained variances (QRnormVar and UPnormVar), - the QR projected explained variance (QRprojVar) which is the adjusted variance of Zou et al. [2006], - the UP projected explained variance (UPprojVar), - the optimal projected explained variance (optprojVar). We prove that the five above definitions satisfy also the two first compatibility conditions, but that only the three projected variances, satisfy the third one, and hence are proper explained variances. 1Then we investigate both theoretically and numerically the size of the six explained variances, the existence of order between them, and their ability to rank in the same order - or not - different sets of components Y , which is for what they have been introduced at the origin. Finally, we study the ability of the three proper definitions of explained variance to provide a substitute to the classical Block PCA formulation : maximizing ∥AZ∥2 F under the orthonormality constraint ZT Z = Im, (1) by replacing (1) by a Explained Variance block PCA formulation : max ∥zj∥=1,j=1...m expvar(AZ) , (2) which is rid of orthogonality constraints on loadings Z = [z1 . . . zm], and hence particularly suited as starting point for the design of sparse PCA algorithms.. As a conclusion of this study, we single out the optimal projected explained variance of Y = AZ : expvar(Y ) = max XT X=Im X j=1...m ⟨yj , xj⟩2 , (3) which possesses, after introduction of weights, the desired properties in the sense that it is easy to compute and differentiate, and admits as unique maximizer the SVD solution of PCA Z∗ = [v1 . . . vm] made of the m first right singular vectors of A. This will be the starting point for the design of an efficient group-sparse block PCA algorithm in the companion paper [Chavent and Chavent, 2023]. The paper is organized as follows: In Section 2 we motivate the need for an explained variance definition, and establish necessary conditions to be satisfied by any definition of the variance explained by a set of non-necessarily orthogonal components. Section 3 establishes the properties of the subspace explained variance (total variance) of Shen and Huang [2008]. Section 4 is devoted to the definition of three projected (including the adjusted variance of Zou et al. [2006] and the optimal variance (3)) and two normalized explained variances, together with the study of their mathematical properties. Sections 5 and 5.3 present numerical comparisons of the size of the variance explained by the six definitions, and of their ability to rank in the same order (or not) non necessarily orthogonal components. Finally, Section 6 compares the ability of the three projected variance definitions to discriminate the singular value solution of PCA as their unique maximizer, which makes them fit to use as Block PCA objective function. It should be noted that we have implemented the six explained variance definitions in R [R Core Team, 2021] in the package sparsePCA available at https://github.com/chavent/sparsePCA. 22 Defining variance explained by components when loadings are non orthogonal We set us from now on in the context of Principal Component Analysis (PCA), where one seeks a small number m of uncorrelated components yj = Azj, j= 1 . . . mby combining the p columns of a data matrix A, each containing n samples of a centered variable, with m unit norm loading vectors zj, j= 1 . . . m, in such a way that the components yj retain the largest possible part of the total variance of A. We denote by expvar(Y ) = the part of the variance of A explained by Y = AZ (4) and by ∥A∥2 F = X i,j=1...m a2 i,j the (total) variance of A, where ∥ • ∥F is the Frobenius norm of A . At this point, expvar(Y ) is a still loosely defined quantity outside of the solution of PCA given by : Z = Vm, Y = Um Σm = AVm , (5) where Um and Vm contain them first left and right singular vectors and Σm is the diagonal matrix of them first singular values of the singular value decomposition (SVD) of A : A = UΣV T with UT U = Ir , V T V = Ir , Σ = diag(σ1, . . . , σr) = r × r matrix with σ1 ≥ σ2 ≥ ··· ≥σr > 0 , where r is the rank of A, and the columns u1 . . . ur of U and v1 . . . vr of V are the left and right singular vectors of A. The principal components yj = Azj = σjuj are orthogonal, and hence un- correlated, so the sum of their variance ∥yj∥2 represents the part of the total variance ∥A∥2 F of A explained by these m principal components. So we see that the variance explained by the principal components Y is : expvar(Y ) = ∥Y ∥2 F = X j=1...m ∥yj∥2 = X j=1...m σ2 j ≤ X j=1...r σ2 j = ∥A∥2 F . (6) However, when sparsity constraints are introduced into loading vectors like in sparse PCA for instance, non orthogonal loadings and components are gener- ated and one has to face the problem of defining expvar( Y ) for possibly non orthogonal components. Alas, formula (6) for expvar( Y ) is strictly limited to the case of orthogonal components and loadings, as we see now. Consider first the case of non orthogonal components : take for example for Z an orthonormal basis of span {Vm} but different from Vm. Then components Y = AZ are not anymore orthogonal, and hence correlated, so the sum of their variances (the total variance of Y ) is too optimistic, and one expects that expvar(Y ) < ∥Y ∥2 F , (7) 3which shows that ∥Y ∥2 F is not a satisfying definition of expvar( Y ) in this case. Then take for example a data matrix A with three singular values 3, 2, 1 and hence a total variance of 14. Then chose for Z two linearly independant unit vectors close to the first right singular vector v1. Then : ∥AZ∥2 F = ∥Y ∥2 F = ∥y1∥2+∥y2∥2 = ∥Az1∥2 | {z } ≃σ2 1=9 + ∥Az2∥2 | {z } ≃σ2 1=9 ≃ 18 > 9 + 4 + 1| {z } σ2 1+σ2 2+σ2 3 = 14 = ∥A∥2 F , which violates property expvar(Y ) ≤ ∥A∥2 F implied by (4), so once again ∥Y ∥2 F is not suited as a definition of expvar( Y ). A last motivation for the search of definitions of expvar( Y ) : rather than solving the classical Block PCA formulation (1), why not solve the explained variance Block PCA formulation (2) by maximizing expvar( Y ) under the sole unit norm constraint on the loadings Z ? Sparse block PCA formulations based on such explained variance objective function eliminate the difficulty caused by the orthogonality constraints on the loadings, and, by construction, rule directly the balance between sparsity and explained variance. So we address first in this paper the problem of defining the part of the variance of A explained by components Y = AZ, under the sole condition that : ∥zj∥ = 1, j= 1 . . . m ,rank(Z) = rank(Y ) = m ≤ r . (8) In absence of a sound definition for the explained variance of correlated compo- nents, we define a set of hopefully reasonable necessary conditions to be satisfied by any such definition : • Condition 1: when Y, Zhappen to be the SVD solution of PCA given by (5), expvar(Y ) has to provide the exact value given by (6) : expvar(Y ) = X j=1...m σ2 j ≤ X j=1...r σ2 j = ∥A∥2 F for m = 1 . . . r .(9) • Condition 2: when Y, Zsatisfy only (8), the components are not orthog- onal anymore, and one expects that, because of the correlation between the components, the part of the variance of A explained by Y is smaller than that of the PCA solution : expvar(Y ) ≤ X j=1...m σ2 j . (10) • Condition 3: expvar(Y ) has to take into account the possible correlation of the components as expected in (7) : expvar(Y ) ≤ X j=1...m ∥yj∥2 = ∥Y ∥2 F , with equality only if the components are orthogonal. 4Any explained variance definition expvar( AZ) which satisfies these conditions achieves its PCA maximum valueP j=1...r σ2 j (Condition 2) for Z = Vm (Condi- tion 1), and hence provides a block formulation for PCA without orthogonality constraints on loadings, which can be used to derive sparse PCA algorithms. This point os view will be developped in Section 6 below. We propose now definitions for the explained variance of the components Y = AZ associated to any matrix Z of m ≤ r linearly independant - but not necessarily orthogonal - unit norm loading vectors zj, j = 1, ..., m. These definitions will include those introduces by Shen and Huang [2008] (see section 3 below) and Zou et al. [2006] (see section 4.2). 3 Subspace explained variance We start here from a reformulation of the explained variance (6) of the principal components Y = AZ, based on the subspace spanned by Z = Vm. Let P Vm denotes the orthogonal projection on this subspace. Then P Z = PVm = VmV T m , so that : expvar(Y ) = ∥Y ∥2 F = ∥A PZ∥2 F = ∥A PVm∥2 F = X j=1...m σ2 j . (11) When Z satisfies (8) only, we proceed by analogy with (11), and define the subspace explained variance of Y = AZ by : expvarsubsp (Y ) def = ∥APZ∥2 F = tr \b Y T Y (ZT Z)−1) \t , (12) where P Z = Z(ZT Z)−1ZT denotes the projection matrix on the subspace spanned by Z. This shows that subspace explained variance is the Rayleigh quotient associated to A, Z. Note that with this definition, where the explained variance depends only of the subspace spanned by [z1 . . . zm], the normalization of the loadings zj is not necessary. Of course, we will still continue to represent loadings by unit norm vectors - but this is here only a convenience. The subspace explained variance coincides with the total explained variance introduced by Shen and Huang [2008, section 2.3 p. 1021], which they proved was increasing with the number of loadings, and bounded by the variance ∥A∥2 F of the data. The next lemma gives a complete picture of its properties : Lemma 3.1. (Subspace explained Variance) Let Z satisfy (8). Then the sub- space explained variance defined by (12) satisfies : expvarsubsp (Y ) =σ2 1+··· +σ2 m = Max ⇔ span{Z} = span{Vm} . (13) and hence satisfies conditions 1 and 2. However, it does not satisfy condition 3 : - when the components Y = AZ happen to be orthogonal : expvarsubsp (Y ) ≥ ∥Y ∥2 F with equality iff Z perm = Vm , (14) 5where “ perm = ” denotes the equality of matrices up to a column permutation and multiplicatipn by ±1, - when the loadings Z happen to be orthogonal : expvarsubsp (Y ) = ∥Y ∥2 F with non necessarily orthogonal components . (15) The proof is in Section 8.2 of the Appendix. This lemma shows that expvarsubsp (Y ) verifies only conditions 1 and 2, and overestimates the explained variance in two cases : - when components Y are orthogonal without pointing in the direction of the left singular vectors, inequality (14) implies expvar subsp (Y ) > ∥Y ∥2 F , which contradicts condition 3, - when loadings Z are orthogonal without pointing in the direction of the right singular vectors, then the components are correlated and (15) contra- dicts condition 3, which requires in that case that expvarsubsp (Y ) < ∥Y ∥2 F . We explore in the next section other directions in the hope of being able to comply with all conditions 1, 2 and 3. 4 Projected and normalized explained variances We start now from definition (6) of the explained variance in the case of PCA. A natural generalization would be : expvar(Y ) ? = X j=1...m ∥yj∥2 = ∥Y ∥2 F = ∥AZ∥2 F , which, as we have seen in the Section 2, is not anymore an an acceptable defi- nition when components are correlated. However, this definition continues to make perfect sense for the variance explained by components as long as they are orthogonal, without pointing nec- essarily in the direction of left singular vectors. Hence a natural way to eliminate the redundancy caused by the orthogonality default of the components Y and to satisfy Condition 3 is to : 1. Step 1: choose a rule to associate to the components Y an orthonormal basis X of span{Y } that, loosely speaking, “points in the direction of the components Y ”, and, when the components Y happen to be orthogonal, points in the directions of Y itself. So the rule for the choice of the basis X associated to Y has to satisfy : XT X = Im , span{X} = span{Y } , < yj, xj > ≥ 0 ∀j = 1 . . . m , ⟨yj, yk⟩ = 0 ∀j ̸= k =⇒ xj = yj/∥yj∥ ∀j = 1 . . . m . (16) 6Examples of such rules are : QR decomposition : Y ⇝ X = Q solution of Y = Q R , QT Q = Im where R is an upper triangular matrix , Polar decomposition : Y ⇝ X = U solution of Y = U P , UT U = Im where PT = P ∈ I Rm×m , P ≥ 0 . (17) 2. Step 2: associate to Y orthogonal adjusted components Y ′ along the X axes, and define the variance explained by the components Y by : expvar(Y ) def = ∥Y ′∥2 F . Two ways for obtaining the adjusted components Y ′ are considered hereafter: by projection or by normalization as illustrated in Figure 1. y1 y2 y′2 x1 x2 y′1 E2 y′1 y′2 Figure 1: Illustration of projected and normalized explained variances : The ellipse E2 represents the image by A of all unit norm loadings. Let the X = [x1, x2] be the orthonormal vectors associated to the correlated components Y = [ y1, y2]. Then the extremities of the adjusted components Y ′ = [ y′ 1, y′ 2] obtained by projection are the two blue dots on the x1, x2 axes, and those obtained by normalization are the two green dots located at the intersection of the axes with E2. 4.1 Normalized explained variances We consider first in this section the case where the adjusted components Y ′ in step 2 are obtained by “normalization” in the directions of the chosen orthonor- mal basis X of span{Y }. More precisely, the idea is to choose the abscissa of 7y′ j on the xj axis by requiring that y′ j is the image by A of some unit norm ad- justed loading z′ j. This is illustrated in Figure 1 : one associates to components Y = [y1, y2] the adjusted components Y ′ = [y′ 1, y′ 2] whose extremities are the points on the x1, x2 axes located on the ellipse image by A of the unit ball of the loading space. In order to determine the adjusted loadings z′ j, one computes first (non necessarily unit norm) loadings T = [t1, . . . , tm] by performing on the loadings zj the same linear combinations M : Y = XM , (18) that transformed Y into X, which leads to define T by : Z = T M . (19) Multiplying then by A left gives Y = AZ = AT Mand, comparison with (18) shows that X = AT and so xj = Atj, j= 1 . . . m. The adjusted components y′ j are then defined by : y′ j = Az′ j with z′ j = tj/∥tj∥ so that ∥z′ j∥ = 1 , y′ j = xj/∥tj∥ , j = 1 . . . m , (20) and the normalized explained variance of Y estimated with X is defined by : expvarX norm(Y ) = ∥Y ′∥2 F = X j=1...m 1/∥tj∥2 , (21) where T = [t1, . . . , tm] is given by (19) and (18). Before specifying the rule of step 1 which associates an orthonormal basis X to the components Y , we give some properties of expvarX norm(Y ) which hold independently of the chosen basis X. Lemma 4.1. (Normalized explained variance) For any unit norm loadings Z and any basis X chosen according to the rule (16), the normalized explained variance of Y = AZ defined by(21) satisfies conditions 1 and 2 and : expvarX norm(Y ) ≤ expvarsubsp (Y ) ≤ σ2 1 + ··· + σ2 m . (22) Lemma 4.1 follows from Lemma 3.1 applied to the orthogonal components Y ′ : expvarX norm(Y ) def = ∥Y ′∥2 F ≤ expvarsubsp (Y ′) ≤ σ2 1 + ··· + σ2 m , (23) which proves (22). We can now specify the rules for the choice of X to define two normalized explained variance satisfying conditions 1 and 2. QR normalized variance. Let X be defined by the QR-decomposition Y = XR of the components Y = AZ as recalled in (17) . Then (21) leads to another definition of variance : expvarQR norm(Y ) = X j=1...m 1/∥tj∥2 , Z = T R , Rupper triangular . (24) The normalized variance expvar QR norm(AZ) does not satisfy condition 3, as the counter example of Figure 2, left, shows. 8x1 y1 y2 y′1 y2 y1 x1 x2x2 = E2 y′2 y′1 y′2 Figure 2: Counterexamples for property 3. Left :Let X denote the basis asso- ciated to components Y by the QR-decomposition, and Y ′ be the correspond- ing normalized adjusted components. One sees that expvar QR norm(Y ) = ∥Y ′∥2 ≥ ∥Y ∥2, which violates property 3. Right :Let X denote the basis associated to components Y by the polar decomposition, and Y ′ be the corresponding nor- malized adjusted components. One sees that expvar QR norm(Y ) = ∥Y ′∥2 ≥ ∥Y ∥2, which violates property 3. UP normalized variance. Let X be defined by the UP-decomposition (polar decomposition) Y = XP of the components Y = AZ as recalled in (17). Then (21) defines another variance : expvarUP norm(Y )= X j=1...m 1/∥tj∥2 , Z = T P , P = (Y T Y )1/2 . (25) The normalized variance expvarUP norm(AZ) does not either satisfy condition 3, as the counter example of Figure 2, right, shows. So we explore in the next section another road in order to comply with all conditions 1,2 and 3. 4.2 Projected explained variances We consider in this section the case where the adjusted components Y ′ in step 2 are obtained by projection of the components Y on the chosen orthonormal basis X of span{Y }. The adjusted component y′ j is hence defined by : y′ j = ⟨yj , xj⟩xj , j = 1 . . . m . The so called projected explained varianceof Y estimated with X is then defined by : expvarX proj(Y ) def = ∥Y ′∥2 F = X j=1...m ⟨yj , xj⟩2 (26) 9Before specifying the rule of step 1 which associates an orthonormal basis X to the possibly correlated components Y , we give some properties of expvarX proj(Y ) which hold independently of the chosen basis X : Lemma 4.2.(Projected explained variances) For any unit norm loadings Z and any basis X chosen according to the rule (16), the projected explained variance of Y = AZdefined by (26) satisfies conditions 1, 2 and 3 and : expvarX proj(Y ) ≤ expvarsubsp (Y ) ≤ σ2 1 + ··· + σ2 m . (27) The proof is in Section 8.3 of the Appendix. We can now specify the rules for the selection of the orthonormal basis X, which give each, according to Lemma 4.2, a projected explained variance satisfying conditions 1 to 3. QR projected explained variance. We choose here to associate to the components Y , the orthonormal basis X = Q obtained by QR-decomposition of Y as recalled in (17).The vector x1 of the basis X = Q is chosen in the direction of the component with the larger norm, and the remaining components are projected on the orthogonal subspace to x1. Then x2 is determined by the same process applied in the orthogonal subspace, and so on. This reordering ensures that the basis X = Q associated to Y will point in the direction of Y primarily for the components of larger variance. The (order dependent) resulting QR projected explained variance is given by : expvarQR proj(Y ) = X j=1...m ⟨yj , xj⟩2 = X j=1...m r2 j,j. (28) It coincides with the adjusted variance introduced in Zou et al. [2006]. UP Projected explained variance. We choose now to associate to the components Y , the orthonormal basis X = U obtained by UP-decomposition (polar decomposition) of Y as recalled in (17). The basis X = U does its best to point in the same direction as the components Y , in that it maximizes the scalar product ⟨Y, X⟩F = P j=1...m⟨xj, yj⟩. The (order independent) resulting UP projected explained variance is given by : expvarUP proj(Y ) = X j=1...m ⟨yj , xj⟩2 = X j=1...m p2 j,j . (29) Optimal projected explained variance. The idea here is to associate to the components Y , the basis X which gives the largest projected explained variance defined in (26). This choice satisfies obviously condition (16) and the so called optimal projected explained variance is defined by : expvaropt proj(Y ) = max XT X=Im X j=1...m ⟨yj , xj⟩2 , (30) 10The numerical computation of the optimal projected explained variance requires the maximization of the convex function X ⇝ P j=1...m⟨yj , xj⟩2 under the constraint XT X = Im. This can be done using the algorithm of Journ´ ee et al. [2010], which gives here : Xk+1 = polar \u0000 2 Y diag(XT k Y ) \u0001 , X 0 = U = polar(Y ) . (31) where 2Y diag(XT k Y ) is the gradient atXk of the functionX ⇝ P j=1...m⟨yj , xj⟩2. 5 Comparison of the explained variances In this section, the six variances explained by non orthogonal components (see Table 1), are compared theoretically and numerically. Name Notation Definition Short name subspace variance expvar subsp [12] subspVar QR normalized variance expvar QR norm [24] QRnormVar UP normalized variance expvar UP norm [25] UPnormVar QR projected variance expvar QR proj [28] QRprojVar UP projected variance expvar UP proj [29] UPprojVar optimal projected variance expvar opt proj [30] optprojVar Table 1: Summary of for the 6 variance definitions. 5.1 What we know We give first theoretical results on the relative magnitudes of the 6 explained variances : - The subspace explained variance is larger than any of the five other vari- ances (Lemmas 4.2 and 4.1). It is even larger than expected when com- ponents or loadings are orthogonal without being left and right singular vectors (see (14) and (15)). - The optimal projected variance is greater by definition than any other pro- jected variance, in particular greater than the QR and the UP projected variance. - There is no natural order between the QR and UP projected variances : when the components Y are of equal norm, the basis X which maximizes expvarX proj(Y ) is polar(Y ), which implies in particular that : expvaropt proj(Y ) = expvarUP proj(Y ) ≥ expvarQR proj(Y ) . (32) But the converse of the last inequality can hold when the norms of the components are very different : for m = 2, one checks that ∥y2∥/∥y1∥ small enough implies that expvarUP proj(Y ) ≤ expvarQR proj(Y ). 11- There is no natural order between the QR and UP normalized variances : for components Y such that the basis X associated by QR-decomposition coincides with the m-first left singular vectors Um of A, one has, according to Lemma 4.1 : expvarQR norm(Y ) = X j=1...m σ2 j ≥ expvarUP norm(Y ) , with a strict inequality as soon as Y and Umdiag{σj} don’t coincide. The same reasoning with the polar decomposition in place of the QR decomposition shows that the converse inequality can happen. - There is no natural order between the variances defined by projection and normalization, as illustrated in Figure 3. Figure 3: The two sets of components Y = [ y1, y2] and ˜Y = [˜y1, ˜y2] have been chosen such that their polar decomposition produces the same basis X = [x1, x2], and one sees that : expvarUP proj( ˜Y ) ≤ expvarUP norm( ˜Y ) = expvarUP norm(Y ) ≤ expvarUP proj(Y ). 5.2 What we see We compare now numerically the six explained variances. The comparison is made on non orthogonal components Y obtained by applying sparse PCA to simulated data matrices A. The matrices A are obtained using the simulation scheme of Chavent and Chavent [2023] based on m = 4 underlying loadings vectors of size p = 20 and m = 4 underlying first eigenvalues which are chosen to be either close or different. More precisely, 100 matrices A of size n×20 were drawn randomly using either the “close eigenvalues” or the “different eigenval- ues” scheme. Three sparse PCA algorithms [see Chavent and Chavent, 2023] 12were applied to each matrix A for a grid of 101 values of sparsity parameters λ ∈ [0, 1]. Finaly, 30300 = 100 ∗ 3 ∗ 101 loadings matrices Z and components matrices Y = AZ where obtained for the “close eigenvalues” and for the “dif- ferent eigenvalues” scheme. The variance explained by these components where performed using the six variance definitions to finally obtain the dimensionless proportion of explained variance (pev) defined by : pev = expvar(Y ) ∥A∥2 F ≤ σ2 1 + ··· + σ2 m ∥A∥2 F , where the right inequality follows from (10) in Condition 2, which is satisfied by all definitions. Figure 4 gives the mean pev for the 300 non orthogonal components (for the “close eigenvalues” and the “different eigenvalues” case) as a function of the sparsity parameter λ and for each definition of explained variance. 0.0 0.1 0.2 0.3 0.4 0.5 0.80 0.85 0.90 0.95 Close eigenvalues lambda mean pev subspVar optprojVar/UPprojVar QRprojVar QRnormVar UPnormVar 0.0 0.1 0.2 0.3 0.4 0.5 0.70 0.75 0.80 0.85 0.90 0.95 Different eigenvalues lambda mean pev subspVar optprojVar UPprojVar QRprojVar QRnormVar UPnormVar Figure 4: Comparison of the mean pev (proportion of explained variance) over two sets of components (top and bottom) as function of sparsity parameter λ for the six variance definitions. For the “close eigenvalues” case (top), the six definitions give relatively close results and the results produced by optprojVar and UPprojVar are so close that 13they cannot be distinguished on the figure (remember that they would coincide were the norms equal, see (32)). One sees also that the results seem to be in a certain order for all λ : subspVar ≥ optprojVar ≥ UPprojVar ≥ QRprojVar ≥ QRnormVar ≥ UPnormVar (the two first inequalities are not a surprise, as they hold theoretically). For the “different eigenvalues” case (bottom), a zoom on the curves shows that subspVar and optprojVar are again larger than all other variances. But the previous apparent order between UPprojVar, QRprojVar and QRnormVar is not longer observed. The UPnormvar remains the smallest but its behavior seems disturbed as the sparsity paramter λ increases. Table 2 shows that all definitions but UPnormVar (in the “different eigen- values” case) exhibit quite similar dispersions over the 300 realizations (for λ = 0.3). Close eigenvalues Different eigenvalues subspVar 0.63 1.63 optprojVar 0.66 1.38 UPprojVar 0.66 1.06 QRprojVar 0.72 1.25 QRnormVar 1.06 1.33 UPnormVar 1.21 7.74 Table 2: Standard deviations ×100 of the six pev (proportion of explained vari- ance) obtained for λ = 0.3 with the three algorithms over the two sets of com- ponents (close eigenvalues and different eigenvalues). 5.3 Ranking properties The proportions of explained variance are meant to be used for the ranking of algorithms, so it is important to figure out wether or not definitions i and j of explained variance will rank in the same order the components Y and Y ′ obtained from possibly different algorithms and/or sparsity parameter λ and/or realization of the data matrix A. The components obtained with the 3 algorithms, the 50 smallest values λ and 100 realizations of A (“different eigenvalues ” case) gave 15000 × 14999/2 couples ( Y, Y′) to be tested. Among these couples, we may consider as ϵ-distinguishable from the point of view of our explained variances those for which |pevi(Y ) − pevi(Y ′)| ≥ϵ for all i = 1 . . .6 for some ϵ ≥ 0. Table 3 shows the percentage of cases where pevi and pevj rank identically components Y and Y ′ among all ϵ-distinguishable couples. The good news is that all three projected variances optprojVar, UPpojVar and QRprojVar, as well as the normalized variance QRnormVar, produce the 14optprojVar UPprojVar QRprojVar QRnormVar UPnormVar subspVar 79.98 71.05 70.71 69.75 56.19 optprojVar 88.93 89.87 89.19 73.71 UPprojVar 96.22 95.27 84.62 QRprojVar 98.34 83.15 QRnormVar 82.75 UPnormVar optprojVar UPprojVar QRprojVar QRnormVar UPnormVar subspVar 86.13 83.78 84.70 84.57 68.68 optprojVar 96.40 98.57 98.32 81.14 UPprojVar 97.81 97.72 84.74 QRprojVar 99.66 82.55 QRnormVar 82.66 UPnormVar optprojVar UPprojVar QRprojVar QRnormVar UPnormVar subspVar 89.57 89.57 89.57 89.57 68.80 optprojVar 100.00 100.00 100.00 79.23 UPprojVar 100.00 100.00 79.23 QRprojVar 100.00 79.23 QRnormVar 79.23 UPnormVar Table 3: The entry of each table on line i and column j gives the percentage of ϵ-distinguishable couples Y, Y′ which are ranked identically by pev i and pevj. Top : ϵ = 0, middle : ϵ = 10−3, bottom : ϵ = 10−2. same ranking as soon as one considers that differences in proportion of explained variance under 10 −2 are not significative. For the same ϵ, the two other def- initions subspVar and UPnormVar still produce quite different rankings. The numerical results of this section show that all investigated definitions rank the explained variance of components in essentially the same order, and hence can all be used to compare the variance explained by components obtained by different algorithms or with different parameters. Of course, this is only an experimental result based on our simulated data sets, that needs to be confirmed by further numerical tests. 6 Explained variance block PCA formulations With the objective in mind to get rid of the orthogonality constraints in the usual block PCA formulation (1), we discuss in this section the possibility of using the above defined explained variance measures as objective function in new block PCA formulations where the loadings are subject to unit norm constraints 15only : max ∥zj∥ = 1, j= 1. . . m expvar(AZ) = σ2 1 + ··· + σ2 m , (33) As seen in Lemmas 3.1, 4.1 and 4.2, property (33) holds for the subspace variance and the five normalized and projected variance definitions. However, from these six definitions, only the three projected explained variances satisfy (Lemma 4.2) all necessary conditions 1, 2 and 3 of section 2 required for a proper definition of explained variance. So we shall limit our search for an explained variance block formulation to these three definitions. A projected explained variance expvar proj(AZ) will provide a block PCA formulation (33) if and only if its sole maximizers are the loadings Z made of the m first right singular vectors Vm in any order, up to a ±1 multiplication, or in short : Z perm = Vm are the sole maximizers of expvar( AZ) . (34) We denote also by Am def = UmΣmV T m with Σ m def = diag {σ1 . . . σm} (35) the restriction of the matrix A to the subspace of the m first right singular vectors V1 . . . Vm. It is a bijection from the right to the left singular subspaces associated to the m ≤ r first singular values. In the singular basis Um, Vm, Am reduces to Σm. The next lemma characterizes the maximizers of any projected variance : Lemma 6.1. Maximizers of projected variances 1. For a given set of components Y , the basis X which gives the largest projected explained variance expvarX proj(Y ) is necessarily a solution of : Y diag(XT Y ) = XP with PT = P and P ≥ 0 (36) 2. The projected explained variance expvarX proj(AZ) achieves it PCA maxi- mum value σ2 1 +··· +σ2 m if and only if Z and X satisfy the three following conditions : span{Z} = span{Vm} (37) the loadings Z are (AT mAm)−1– orthogonal (38) the basis X associated to Y = AZ is X = [n1 . . . nm] . (39) where n1 . . . nm denote unit normals at y1 . . . ym to the ellipsoid Em of span{Um} image by Am of the unit sphere of span{Vm} (see Figure 5) 16y1 n1 y′2 n2n1n2 y2 y′1 R2=σ21+σ22 Figure 5: Illustration of condition (39) for m = 2. The ellipse E2 is the image by A of all unit norm loadings z of the right singular space V2, with half axes σ1 and σ2. Let y1, y2 be two (possibly correlated) components such that the normals n1, n2 to E2 at y1, y2 are orthogonal, the tangents are orthogonal too and hence meet on the Cartan Circle of radius R = (σ2 1 + σ2 2)1/2. Then for the choice X = (n1, n2) one has expvarX projY = ∥y′ 1∥2 + ∥y′ 2∥2 = R2 = σ2 1 + σ2 2. The proof is in Appendix 8.4 Condition (38) reduces, in the singular basis Vm, to Σ −2 m –orthogonality of the loadings. It ensures that the normals nj are orthogonal, and hence can be chosen as the basis X associated to Y in (39). Of course, the SVD solutions Z perm = Vm of PCA satisfy always (37) (38) (39). Once a rule Y ⇝ X has been specified, points 1 and 2 of the lemma will make it possible to determine wether or not (34) is satisfied. 6.1 Maximizers of expvarQR proj(Y ) The projected explained variance expvar QR proj(AZ) attains its maximum σ2 1 + ··· + σ2 m if and only if : Z perm = Vm , (40) When the maximum norm selection procedure is applied at each step and the components renumbered accordingly, the unique maximizer is Z = Vm. Proof: The “if” part of (40) is trivial, we prove the “only if” part : let Y be such that expvarQR projY = σ2 1 +··· +σ2 m, and X be given by its QR decomposition 17Y = XR with R upper triangular. By hypothesis, X maximizes expvarX Y , and point 1 of Lemmas 6.1 implies that Y diag(XT Y ) = XP = XRdiag(XT Y ), with P = Rdiag(XT Y ) symmetric positive. This implies that R is a diagonal matrix, so that all xj point in the direction of yj. But property 2 of Lemma 6.1 shows that the xj’s are also normal to the ellipsoid Em at yj, which can happen only if yj = Azj coincides with its principal axes and hence each zj is one of the m first right eigenvectors vj. Proposition 6.2. The QR projected explained variance expvarQR proj(AZ) (the adjusted variance of Zou et al. [2006]) associated to unit norm loadings Z pro- vides a block PCA formulation : max ∥zj∥ = 1, j= 1. . . m expvarQR proj(AZ) = σ2 1 + ··· + σ2 m , (41) which admits the SVD solution (40) as “unique” maximizer. Numerical implementation of this formulation requires the computation of the gradient of the Z ⇝ expvarQR proj(AZ) function, which is defined through the QR decomposition of Y = AZ. This can be done by the adjoint state method Chavent [2010], which is feasible but may be cumbersome. The block PCA formulation (41) can be used as starting point for the design of sparse PCA algorithms, keeping in mind that enforcing sparsity by subtracting theℓ1 norm of loadings leads to a difficult, though tractable, non smooth optimization problem. 6.2 Maximizers of expvarUP proj(Y ) The projected explained variance expvar UP proj(AZ) attains its maximum σ2 1 + ··· + σ2 m if and only if : Z perm = Vm (SVD solution) or Z = Z# (42) where the “parasitic” solution Z# is such that the components Y # = AZ# satisfy ⟨y# 1 , x1⟩ = ··· = ⟨y# m, xm⟩, with the hyperplanes tangent to Em at y# j delimiting an m-dimensional orthant of span {Vm}. For m = 2, the parasitic solution is illustrated on figure 6, where one sees that the components y# j (in red) corresponding to the choice of different principal axes for the intersection of the tangents coincide up to a multiplication by ±1; the SVD components are in blue. The proof is in Appendix 8.5. So expvarUP proj(AZ) cannot be used for the construction of a block PCA for- mulation like (33), as the optimization algorithm might converge to the parasitic solution Z# ! 18x1 x2 E2 u1 y′1# y′2# u2 R2=σ21+σ22 y#2 y∗1 y#1 y∗2 −y#2 −y#1 Figure 6: Illustration, for m = 2, of the parasitic maximizer Y # of expvarUP proj. The polar decomposition associates to components Y # = y# 1 , y# 2 orthonormal vectors X = x1, x2 such that ∥y′ 1 #∥ = ∥y′ 2 #∥. The red points on the Cartan circle correspond to the parasitic maximizers Y # = ±y# 1 , ±y# 2 , the blue points to the SVD solution Y ∗ = ±y∗ 1, ±y∗ 2 = ±σ1u1, ±σ2u2 6.3 Maximizers of expvaropt proj(Y ) According to point 2 of Lemma 6.1, the optimal projected explained variance expvaropt proj(AZ) attains its maximum σ2 1 + ··· + σ2 m if and only if : span{Z} = span{Vm} and the loadings Z are (AT mAm)−1– orthogonal . (43) This situation is similar to the maximization (1) of∥AZ∥2 under orthonormality constraints ZT Z = Im, were the maximum is attained for all orthonormal Z which span Vm. But the difference is that the orthogonality condition (43)-right is not a constraint for the maximization of expvar opt proj(AZ), it just happens to be satisfied by the maximizer ! 196.4 Weighted optimal projected explained variance In order to select the SVD solutionZ = Vm among the maximizers of expvaropt proj(AZ), we introduce weights µj such that : µ1 ≥ µ2 ≥ . . . µm > 0 , (44) and define a weighted optimal projected variance by : expvaropt proj,µ(AZ) = max XT X=Im X j=1...m µ2 j⟨Azj , xj⟩2 , which coincides with expvar opt proj(AZ) when µj = 1 for all j. This leads to the weighted optimal projected explained variance block PCA formulation : max ∥zj∥ = 1 j = 1. . . m expvaropt proj,µ(AZ) = max ∥zj∥ = 1 j = 1. . . m max XT X=Im X j=1...m µ2 j⟨Azj , xj⟩2 = X j=1...m µ2 jσ2 j . (45) The nice properties of this formulation are recalled in the next proposition : Proposition 6.3. Let the singular values of A satisfy : σ1 > σ2 > ··· > σm > 0 , and the weights µj satisfy (44). Then the PCA loadings Z = Vm and nor- malized components X = Um defined in (5) are one solution of the block PCA formulation (45) when the weights µj are constant, and its unique solution (up to a multiplication by ±1 of each column of course) when the weights µj are strictly decreasing, in which case the maximizers Z∗ and X∗ are independent of the weights µj. The (unweighted) optimal projected variance explained by Y ∗ = AZ∗ is : expvaropt proj (Y ∗) = σ2 1 + ··· + σ2 m ≤ ∥A∥2 F . Proof: Exchanging the order of maximization in the center term of (45) solving analytically the maximization with respect to Z gives : max ∥zj∥ = 1 j = 1. . . m expvaropt proj,µ(AZ) = max XT X=Im X j=1...m µ2 j∥AT xj∥2 . (46) The last term in resp. (46) is the maximization of a weighted Rayleigh quotient for AT , which is known to be equivalent to a PCA problem forAT , and hence for A (see for example Absil et al. [2008], recalled as Theorem 8.1 in the Appendix for the case of constant weights, and Brockett [1991] for the case of decreasing weights). Of course, formulation (45) is of little interest for PCA itself, as there exists plenty of other efficient solution methods. But it will provide the starting point for the design of a sparse PCA algorithm, to be developped in the companion paper Chavent and Chavent [2023]. 207 Conclusion We have investigated the problem of defining the part of the variance of a data matrix explained by correlated components, such as those which arise when sparse loadings are searched for. We have established three compatibility conditions to be satisfied by any such explained variance definition in order to be compatible with the Principal Component Analysis (Condition 1), and to ensure a loss in explained variance when the components are correlated (Conditions 2 and 3). We have proved that the two existing and the four new definitions : - Subspace (total variance of Shen and Huang [2008]), - QR normalized, - UP or Polar normalized, - QR projected (adjusted variance of Zou et al. [2006]), - UP or Polar projected, - Optimal projected all satisfy the two first compatibility conditions, but that only the three pro- jected explained variance satisfy also the third one and provide proper explained variance definitions. Numerical experiments have shown that the choice of a specific definition for the ranking of correlated components by explained variance is not critical. But we have shown that only the QR and the (weighted) optimal projected explained variance definitions admit the SVD solution as unique maximizer, and hence provide new explained variance block PCA formulations rid of orthogonality constraints on loadings. Their use for the construction of a group sparse PCA algorithm is the subject of a second paper Chavent and Chavent [2023]. 8 Appendix 8.1 Generalized Rayleigh quotient This is a classical result, see for example Absil et al. [2008] and Brockett [1991] : Theorem 8.1. Let the loadings Z satisfy : Z = [z1 . . . zm] ∈ I Rp×m , rank(Z) = m ≤ rank(A) def = r . Then the generalized Rayleigh quotient tr{(ZT AT AZ)(ZT Z)−1} satisfies : tr{(ZT AT AZ)(ZT Z)−1} ≤σ2 1 + ··· + σ2 m ≤ ∥A∥2 F , and the left inequality becomes an equality if and only if : span{Z} = span{v1 . . . vm} , where v1, . . . vm are the m first right singular vectors of A. 218.2 Proof of Lemma 3.1 Definition (12) of the subspace explained variance and the properties of the Rayleigh quotient tr {ZT AT AZ(ZT Z)−1)} recalled in Theorem 8.1 show that (9) and (10), and hence Properties 1 and 2, hold as well as (13) and (15). It remains to prove (14) which shows that Condition 3 does not hold. So let Y = AZ be orthogonal components : ⟨yj, yk⟩ = 0 , j, k= 1 . . . m, j̸= k corresponding to unit norm loadings : ∥zj∥ = 1 j = 1 . . . m , and define X, Tby : xj = yj/∥yj∥ , t j = zj/∥yj∥ , j = 1 . . . m , so that : XT X = Im . Then on one side one has : ∥Y ∥2 F = X j=1...m ∥yj∥2 = X j=1...m 1/∥tj∥2 = tr{diag−1(TT T)} , (47) and on the other side, as Y and X span the same subspace : expvarsubsp (Y ) = expvarsubsp (X) = tr{(XT X)(TT T)−1} = tr{(TT T)−1} (48) Formula (14) will be proved if we show that : tr{diag−1(TT T)} ≤tr{(TT T)−1} . (49) We use for that an idea taken from Miller [1969], and perform a QR-decomposition of T. By construction, the diagonal elements of R satisfy : 0 < ri,i ≤ ∥ti∥ . Then : TT T = RT QT Q R= RT R , (TT T)−1 = R−1(RT )−1 = R−1(R−1)T , where R−1 satisfies : R−1 = upper triangular matrix , [R−1]i,i = 1/ri,i . Hence the diagonal element of ( TT T)−1 are given by : : \u0002 (TT T)−1\u0003 i,i = \u0002 R−1(R−1)T \u0003 i,i = [ R−1]2 i,i + X j>i [R−1]2 i,j ≥ [R−1]2 i,i = 1/r2 i,i ≥ 1/∥ti∥2 . (50) 22which gives (49) by summation over i = 1 . . . m, and (14) is proved. We suppose now that the orthogonal components yj, j= 1 . . . msatisfy ∥Y ∥2 F = expvarsubsp (Y ). Then (47) (48) imply that equality holds in (49) and hence all inequality in (50) are equalities : 1. first inequality : [ R−1]2 i,j = 0 for all j > i ⇒ R−1 and hence R are diagonal 2. second inequality : 1 /r2 i,i = 1/∥ti∥2 ⇒ R is diagonal But R diagonal implies that the tj - and hence also the loadings zj - are orthog- onal, which together with the hypothesis of orthogonal components yj, implies that (yj/∥yj∥, zj) are pairs of singular vectors of A, which proves that Z = Vm and ends the proof of (14). 8.3 Proof of Lemma 4.2 By construction, expvarX proj(AZ) satisfies clearly conditions 1 and 3 of Section 4. We prove now that it satisfies moreover (27), and hence also condition 2. Let EX be the ellipsoid of span(X) = span(Y ) image by A of the unit sphere of span(Z). By construction one has : yj ∈ EX , j = 1 . . . m , and the modified components Y ′ defined by projection satisfy, c.f. (26) : ∥y′ j∥ = |⟨yj, xj⟩| ≤νj def = max y ∈ EX ⟨y, xj⟩ , j = 1 . . . m , (51) so that : expvarX projY def = ∥Y ′∥2 F ≤ ν2 1 + ··· + ν2 m . (52) We can now “box” the ellipsoid EX in the parallelotope PX of span{X} defined by : PX = \b y ∈ span{X} | −νj ≤ ⟨y, xj⟩ ≤+νj , j = 1 . . . m \t , (see figure 7). By construction, one can draw from each of the 2 m vertices of PX m orthogonal hyperplanes tangent to the ellipsoid EX, which implies that they are all on the orthoptic or Cartan sphere of the ellipsoid, whose radius is known to be the sum of the squares of the half principal axes σX j , j= 1 . . . mof EX (see for example the textbook Tauvel [2000]). Hence : ν2 1 + ··· + ν2 m = (σX 1 )2 + ··· + (σX m)2 . (53) Let then yX 1 . . . yX m be vectors whose extremity are points of EX located on its principal axes, so that : ∥yX j ∥ = σX j , j = 1 . . . m ,⟨yX i , yX j ⟩ = 0 , i, j= 1 . . . m, i̸= j . Property (14) of Lemma 3.1 applied to the orthogonal components Y = Y X gives: : (σX 1 )2 + ··· + (σX m)2 = ∥Y X∥2 ≤ expvarsubsp Y X ≤ σ2 1 + ··· + σ2 m . (54) Combining inequalities (52) (53) (54) proves the inlem 4-2equality (27). 23Figure 7: Illustration of the upper bound to ∥Y ′∥2 F in span {Y } when Y ′ is defined by projection. 8.4 Proof of Lemma 6.1 (notations of Section 8.3) We prove first point 1 of the Lemma. Maximization of the convex function X ⇝ expvarprojY X under the constraint XT X = Im by algorithm (31) and passing to the limit proves (36). We prove now the “only if” part of point 2. Let X be given such that expvarX projY = σ2 1 + ··· + σ2 m. Then necessarily : • equality holds in (54), and property (13) of subspace variance implies that the loadings Z span the subspace Vm of the m first right singular vectors, which proves (37). • equality holds in (51), which implies that for j ̸= k the normals to EX at yj and yk are orthogonal (see Figure 7). The restriction Am of A to span{Vm} is an isomorphism from span {Vm} to span{Um}, hence : EX = {y ∈ span{Um} | ∥A−1 m y∥2 = 1} . A normal n(y) to EX at y is then : n(y) = 1 2∇y \u0000 ∥A−1 m y∥2 − 1 \u0001 = (A−1 m )T A−1 m y = (A−1 m )T z , 24and the orthogonality of n(yj) and n(yk) shows that : ⟨n(yj), n(yk)⟩ = ⟨(A−1 m )T z1, (A−1 m )T z2⟩ = ⟨z1, (AT mAm)−1z2⟩ = 0 , (55) which proves (38). When vectors and matrices are written on the singular bases Um and Vm, one has ( AT mAm)−1 = diag{ 1 σ2 1 . . .1 σ2m }. We finally prove the “if” part of point 2. So let (37) (38) (39) hold. Property (37) implies that the half axes of EX are σ1 . . . σm, and (38) that the normal nj to EX at yj, j= 1 . . . mare orthogonal. So one can box EX with a parallelotope PX with axes parallel to the normals nj, and define X as the orthonormal basis along its axes. Then the same reasonning as above for the proof of (27) shows that expvarprojY = σ2 1 + ··· + σ2 m, which ends the proof of the lemma. 8.5 Proof of Property (42) Let the loadings Z be such that the UP-projected explained variance of the components Y = AZ satisfies expvarUP proj(Y ) = σ2 1 +··· +σ2 m. Then by definition of expvarUP proj one has : expvarUP proj(Y ) = expvarX proj(Y ) with Y = UP, UT U = Im , P = PT , P≥ 0 (56) But expvarX proj(Y ) = σ2 1 + ··· + σ2 m, and point 1 of Lemma 6.1 implies the existence of P′ such that : Y diag(XT Y ) = XP ′ with P′T = P′ and P′ ≥ 0 (57) Comparison of the two last properties shows that : Y = UP = XP ′diag(XT Y )−1 , (58) and uniqueness of the polar decomposition implies that P′diag(XT Y )−1 = P and hence is symmetrical, which can happen in only two cases : 1. either diag( XT Y ) = λIm for some λ, which gives the parasitic solution Z = Z#, 2. or P′ - and hence also P itself - is diagonal, which implies that the com- ponents Y = UP are orthogonal. But point 2 of Lemma 6.1 implies that they are also ( AT mAm)−1– orthogonal, which is possible only if the com- ponents Y are proportional to the left singular basis Um, which gives the SVD solution Z perm = Vm. References Pierre-Antoine Absil, Robert Mahony, and Rodolphe Sepulchre. Optimization Algorithms on Matrix Manifolds , volume 78. 12 2008. ISBN 978-0-691-13298- 3. doi: 10.1515/9781400830244. 25RW Brockett. Dynamical systems that sort lists, diagonalize matrices, and solve linear programming problems. Linear algebra and its applications, 146:79–91, 1991. Guy Chavent. Nonlinear Least Squares for Inverse Problems: Theoretical Foun- dations and Step-by-Step Guide for Applications . 01 2010. ISBN 978-90-481- 2784-9. doi: 10.1007/978-90-481-2785-6. Marie Chavent and Guy Chavent. A group sparse explained variance block pca. Submitted, 2023. Michel Journ´ ee, Yurii Nesterov, Peter Richt´ arik, and Rodolphe Sepulchre. Gen- eralized power method for sparse principal component analysis. Journal of Machine Learning Research, 11(Feb):517–553, 2010. G Miller. Closed-form inversion of the gram matrix arising in certain least- squares problems. IEEE Transactions on Circuit Theory, 16(2):237–240, 1969. R Core Team. R: A Language and Environment for Statistical Computing . R Foundation for Statistical Computing, Vienna, Austria, 2021. URL https: //www.R-project.org/. Haipeng Shen and Jianhua Z Huang. Sparse principal component analysis via regularized low rank matrix approximation. Journal of multivariate analysis , 99(6):1015–1034, 2008. Patrice Tauvel. Cours de g´ eom´ etrie: agr´ egation de math´ ematiques. Dunod, Paris, 2000. Hui Zou, Trevor Hastie, and Robert Tibshirani. Sparse principal component analysis. Journal of computational and graphical statistics , 15(2):265–286, 2006. 26",
      "references": [
        "Optimization Algorithms on Matrix Manifolds",
        "Dynamical systems that sort lists, diagonalize matrices, and solve linear programming problems.",
        "Nonlinear Least Squares for Inverse Problems: Theoretical Foun- dations and Step-by-Step Guide for Applications",
        "A group sparse explained variance block pca.",
        "Gen- eralized power method for sparse principal component analysis.",
        "Closed-form inversion of the gram matrix arising in certain least- squares problems.",
        "R: A Language and Environment for Statistical Computing",
        "Sparse principal component analysis via regularized low rank matrix approximation.",
        "Cours de g´ eom´ etrie: agr´ egation de math´ ematiques.",
        "Sparse principal component analysis."
      ],
      "meta_data": {
        "arxiv_id": "2402.04692v1",
        "authors": [
          "Marie Chavent",
          "Guy Chavent"
        ],
        "published_date": "2024-02-07T09:32:32Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "[Unavailable]",
        "methodology": "[Unavailable]",
        "experimental_setup": "[Unavailable]",
        "limitations": "[Unavailable]",
        "future_research_directions": "[Unavailable]",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "AMPO: Automatic Multi-Branched Prompt Optimization",
      "full_text": "3DPrinter-ControlledSyringePumpsforDual,Active,RegulableandSimultaneousDispensingofReagents.ManufacturingofImmunochromatographicteststrips. GabrielSiano* a,b ,LeandroPeretti c,d ,JuanManuelMárquez b ,NazarenaPujato b, LeonardoGiovanini a,e ,ClaudioBerli c (a)InstitutodeInvestigaciónenSeñales,SistemaseInteligenciaComputacional(sinc(i),UNL-CONICET),Argentina(b)FacultaddeBioquímicayCienciasBiológicas,UniversidadNacionaldelLitoral,Argentina(c)InstitutodeDesarrolloTecnológicoparalaIndustriaQuímica(INTEC,UNL-CONICET),Argentina(d)FacultaddeCienciasMédicas,UniversidadNacionaldelLitoral,Argentina.(e)FacultaddeIngenieríayCienciasHídricas,UniversidadNacionaldelLitoral,Argentina *Correspondingauthor:gsiano@sinc.unl.edu.ar(G.Siano),Tel:+5403424575233(117)Postaladdress:CiudadUniversitariaUNL,RutaNacionalNº168,km472.4,FICH,4toPiso(3000)SantaFe–Argentina Abstract: Lateralflowimmunoassays(LFIA)arewidelyusedworldwideforthedetectionofdifferentanalytesbecausetheycombinemultipleadvantagessuchaslowproductioncost,simplicity,andportability,whichallowsbiomarkersdetectionwithoutrequiringinfrastructureorhighlytrainedpersonnel.HereweproposetoprovidesolutionstothemanufacturingprocessofLFIAatlaboratory-scale,particularlytothecontrolledandactivedispensingofthereagentsintheformtheTestLines(TL)andtheControlLines(CL).Toaccomplishthistask,weadapteda3DprintertoalsocontrolSyringePumps(SP),sincetheproposedadaptationofa3Dprinteriseasy,freeandmanylaboratoriesalreadyhaveitintheirinfrastructure.Inturn,thestandardfunctionofthe3DprintercanbeeasilyrestoredbydisconnectingtheSPsandreconnectingtheextruder.Additionally,theunifiedcontrolofthe3Dprinterenablesdual,active,regulableandsimultaneousdispensing,fourfeaturesthataretypicallyfoundonlyincertainhigh-costcommercialequipment.Withtheproposedsetup,thechallengeofdispensingsimultaneouslyatleast2lines(CLandTL)withSPscontrolledbya3Dprinterwasaddressed,includingregulationinthewidthofdispensedlineswithinexperimentallimits.Also,theconstructionofaLFIAforthedetectionofleptospirosisisshownasapracticalexampleofautomatizedreagentdispensing. Keywords:3DPrinter,Immunochromatography,Activedispensing,Syringepumps,LateralFlowImmunoassays(LFIA),Linewidths 11.Introduction Lateralflowimmunoassays(LFIA)arewidelyusedworldwideforthedetectionofdifferentanalytesbecausetheycombinemultipleadvantages,suchaslowproductioncost,simplicity,andportability,whichallowsbiomarkersdetectionwithoutrequiringinfrastructureorhighlytrainedpersonnel.Thiskindofimmunochromatographictechniquehasbeenusedforthedetectionofvariouspathologicalagentssuchasviruses,bacteria,fungi 1 ,toxins 2 ,antibiotics 3 ,pesticides 4 ,herbicides 5 ,amongothers.ThemanufactureofLFIAinvolve:i)thesynthesisandcharacterizationofcolloidalgoldparticleswithsizescloseto40nm,ii)theirsubsequentsensitizationwithbiomolecules(antigensorspecificantibodies),iii)thedispensingofthegold-biomoleculescomplexesonspecificsupports,iv)thedispensingofreagentsintheformoflinesonnitrocellulosemembranes(NCM),whichwillgiverisetothebiorecognitionreactionsinareascalledTestLine(TL)andControlLine(CL),andfinallyv)theassemblyofthedifferentparts(samplepad,absorbentpad,housing)toobtaintheteststrip.Inlaboratoryprototypes,TLandCLareusuallymademanually,inordertoevaluatedifferentvariablessuchasconcentrationandvolumeofreagents.However,asubsequentstepinvolvesautomatingreagentdispensingtoachievegoodreproducibilityofthesystem.Althoughthereiscommercialequipmentdedicatedtoresolvingthisaspect,itisalsopossibletodevelopcheaperalternativesthatmaybewithinthereachofmorelaboratories.Ontheotherhand,commercialequipment,especiallythosebasedonSyringePumps(SP),usuallyhaveasignificantdeadvolume,whichisnotrelevantwhenalargenumberofIFLsareproduced.However,inthedevelopmentstagesofanewprototypeitisimportanttominimizereagentlosses.Inaddition,whenaprototypepresentsgoodresults,theautomationofthedispensingofthereagentsontheNCMallowsincreasingtheproductionofIFLs.ThereproducibleproductionofasignificantnumberofLFIAisakeysteptovalidatethedevicesinfrontofahighnumberofrealsamplesandtoreliablydeterminetheirdiagnosticparameters,suchassensitivityandspecificity.HereweproposetoprovidesolutionstothemanufacturingprocessofLFIAforlaboratory-scale,particularlytothecontrolleddispensingofthereagentsintheformoflines.Toaccomplishthistask,weadapteda3DprintertoalsocontrolSPs,sincetheadaptationofa3Dprinteriseconomicalandmanylaboratoriesalreadyhaveitintheirinfrastructure.AnFFF(FusedFilament Fabrication) 3Dprinter for conventional thermoplastics(PLA,PETG,ABS,etc)canbemadeorcanbeboughtwithalowbudget.Thiskindof3Dprintersexistmassivelyandalotofthemareopensource(hardware,firmwareandsoftware).Themakercommunityhasachievedahighlevel ofdocumentationand/orexemplification.Allthesecharacteristicsmake3Dprintersinstrumentswithahighlevelofaccessibility.Atthesametime,3Dprintersareusuallyself-replicant(the3Dprintercanprintpartsofitself)andtheadditionofdesignedpiecesonorneartheactuator(thatis,thepiece/carriagethatholdsthehotend)canbeachievedwithincertainweightlimits.Recently,astudy 6 wasreportedinwhichatechnicalpenwasattachedtothecarriageofa3Dprinter.Thissetupisrestrictedtopassivedispensingonly,andduetotherigidityoftheDT,minorimperfectionscancausetheNCMtobecomescratched.Giventhelimitationtopassivedispensing,multiple-passexperimentsontheNCMwerereported.Inthecurrentstudy,however,thedispensedquantityisunderactiveandconfigurablecontrol,andoptionally,itisalsopossibletosupporttwo(evenmore)technicalpensorotherDTsforpassivedispensing. 2Beyondmerelyutilizingthe3Dpositioningcapabilitiestomoveadispensingtoolforactiveorpassivedispensing,someactionscanbecarriedoutwithsimpleG-code,suchascontrollingthetemperature,levelandspeedoftheprintingbed,andalsogeneralhandlingofsteppersmotors(axes,extruders,etc).Thisimpliesthattheinstrumentcanbemodifiedandcaneasilydrivesteppermotors,whichrelatesittothecontrolofsteppers-basedSP.ThiskindofSPalsoexistsin3D-printableandopensourcemodels 7–9 ,astheonesusedinthepresentwork,andcanbeusedtoperformdifferentanalyticaltechniques,suchaslab-in-syringe 10,11 .Alternatively,a3Dprintingkitwastransformedintoaprogrammablesyringepumpset 12 ,controlledbythesamefirmwareasinthepresentwork(Marlin 13 ),butusingtherespectiveconnectionstotheX,Y,andZaxes.BeingabletoimplementG-codeavoidstheneedforprogrammingthepumpsfromscratch,whichisanadvantage.Similarly,withotherfirmwares 14 ,ithasbeenpossibletocontrolmultipleSPssimultaneously.However,inbothcases,the3Dmovementaxeswereused,withoutpreservingthestandardfunctionofa3Dprinter,whichwasindeedoneoftheobjectivesofthepresentstudy. CommercialequipmentforLFIAmanufacturing,whichmayormaynothaveactivedispensing,isusuallyexpensive,limitedtoworkinginasingledimension,andusuallyhasnootherutility.Ontheotherhand,somedevicesorsetupshavebeenreportedintheliterature.Oneexampleisanopensource3Dprintableantibodydispenser 15 ,whichworkswithacontinuousNCMandrequiresabsorptionbythemembrane.Thatis,thereagentisdispensedpassivelybythewickingforcecausedbytheporesoftheNCM 15 and/orbygravity.Itshouldbeemphasizedthatactivedispensingallowsforprecisecontroloftheliquidquantity.Therearenogravity-relatedsideeffects,asliquidmovementiscontrolledthroughpressure/vacuum.Incontrast,passivedispensingmayleadtoresultsthatcannotbeoptimizedormayvarywhenchangingtheabsorptionpropertiesoftheNCM.Inanotherrelatedresearch 16 ,activedispensingwithSPsresultedinmoreconsistentlines,comparedtoflowdrivenbybothhydrostaticpressureandwickingforce.InthatworktwocommercialSPswereused,onetopumpliquidandtheothermodifiedasalinearactuatortomovetheDispensingTip(DT).However,thismethodologyislimitedtoworkinginasingledimension,andifmorethanonelineisrequired,theymustbeobtainedsequentiallyandnotsimultaneously.Sincethesetestsrequireatleast2lines(TestandControllines)ofdifferentcomposition,andtakingintoaccountthat,inadditiontothechallengeofhandlingthetypicallylimitedavailablevolumes,theliquidreplacementstepintheDTwouldbeadded,simultaneitybecomesessential.Inturn,aunifiedcontrolforbothSPswasnotreported,sincetheoneusedtomovetheDThadindependentcontrol,sobothSPsmustbemanuallycoordinatedbytheoperator.Sincethisaddsvariabilitytothesystem,itwasavoidedinourwork.BothSPsandNCMmovementswerecontrolledbythe3Dprinter.Recently,anothersyringe-basedArduino-operatedantibodydispenserhasbeenreported 17 .Theproposedsetupisconfigureddirectlythroughhardware(potentiometers)butdoesnotuseG-code.Itisrestrictedtodispensingasinglelineandimperfectionswerereportedattributedtoinstabilityinthesteppermotor-drivenconveyorbeltused. Inthisstudy,certainaspectsoftheproposeddispensingprocedurewereexamined,particularlywithregardtolinewidths.Firstly,thechallengeofdispensingnotsequentiallybutsimultaneouslyatleast2lines(controlandtest)withSPscontrolledbya3Dprinterwasaddressed.SinceitmaybedesirabletoobtaintwodifferentlinewidthsforTLandCL(consideringsignalintensity,costs,amongotherfactors),andthisrequiresindependence 3betweenthelinesinadditiontosimultaneity,activeandpassivemethodsofachievingthisarediscussed.Additionally,theconstructionofaLFIAforthedetectionofleptospirosisispresentedasapracticalapplication.Leptospirosisisagloballydistributedzoonoticdisease,prevalentinbothurbanandruralareas,andiscausedbyspirochetesbelongingtothegenusLeptospiraspp 18 .Recently,alaboratoryprototypeofanLFIAwasdevelopedforthedetectionofleptospirosisusingantigensderivedfromLeptospirastrainscirculatingwithinArgentina 19 .Inthiswork,LFIAdevicesforthedetectionofhumanleptospirosiswereobtainedandassessedusingpositiveandnegativecontrolsamples. 42. MaterialandMethods 2.1InstrumentsandmaterialsAmodifiedHellbotMagnaI3Dprinterwasutilized.ThemodificationsinvolvedtheadditionofwiresandanA4988stepperdriverforanextrastepper/extruder,afirmwarechange,andtheattachment ofa3D-printedDispensingTipsHoldertotheprinter'scarriage.ThefirmwarewasupdatedtoMarlin2 13  andtheMixingExtrudermode 20  wasenabled.Pronterfacewasutilizedtocontrolthe3DprinterandsendruntimeG-code.ItshouldbenotedthatonceasetuphasbeenoptimizedandadebuggedG-codeisavailable,itisalsopossibletosaveitonamemorycardanddispensedirectlywithouttheneedforaPC.FortheassemblyoftheSyringePumps,PLAwasusedfortheprintedpartsalongwithalltheessentialcomponentsrequiredfortheassemblyof2SPs 7,9 .An8mmlead/2mmpitchscrew(veryusualinZ-axisof3Dprinters)wasutilizedasleadscrew.Smithsmedical22GJelco®IVCatheterRadiopaquewereusedasDispensingTips.SPsandDTswerecoupledwith3-WayValves(3WVs),PTFEtubing(0.8mmid)and21Gneedles.Printex®3WVswereutilized,with1maleand2femaleLuerconnectors.Whennecessary,male-maleLueradapterswereobtainedbyconnectingtwomaleconnectorsfromSecondaryMedicationSets(Euromix®)usingtheincludedtubing.ToloaddispensingliquidsintotheDTs,standard1mLsyringeswereemployed,whilestandard10mLsyringeswereusedforaccumulatingdisplacementfluid.Bovineserumalbumin(BSA,Sigma-Aldrich),fooddyesandDDIwaterwereemployedtoobtaincolouredsolutions. 2.2EdgeDetectionandImageAnalysis:InordertodeterminethewidthanduniformityofthelinesdispensedthroughimageanalysisofdifferentNCM,Canny 21 edgedetectionwasemployed,implementedinPythonusingOpenCV 22 .Twoedgesweredetectedforeachline.Theseedgeswereanalyzedalongtheentirepathofthelines,andaveragelinewidthsinmmwereobtainedevery2.5mmoflinearmovement. 2.3LeptospiraAntigenAntigenwaspreparedasreportedintheliterature 23 .Briefly,acultureofL. interrogansHardjoserovarwascarriedoutuntilexponentialphaseandsubsequentlylysedbysonication.Phenyl-methyl-sulfonylfluoride(PMSF,1mM)wasaddedtoinhibitproteasesandtheconcentrationwasdeterminedbythemethodofBicinchoninicAcid(BCA,PierceReagents) 24 .Thelysatewasstoredat-20°Cuntiluse. 2.4Preparationofgold-labeledanti-humanIgG(a-IgGhum /GNPs)First,GNPs(～40nm)wereprepared 25 .Briefly,100uloftrisodiumcitratesolutionwereadded(finalconcentration0.015%w/v)to50mLofAuCl3-·HCl·4H2Osolution(0.01%w/v)understirringat100°C.After30minutes,thereactionisstoppedandallowedtoreachroomtemperature.GNPssizewasdeterminedbyUV/visspectrophotometer.Then,GNPswereemployedtoobtaintheconjugatewithGoatanti-humanIgG(a-IgGhum ,Sigma) 25 .PassivesensitizationwasperformedatpH=9(boratebuffer20mM)bymixingfor30min10mLofGNPswith0.2mg/mLa-IgGhum understirring.Subsequently,thefreesitesofnanoparticles 5wereblockedwithBSA1%finalconcentration.Excessofproteinswereremovedbycentrifugationandredispersioncyclesin20mMboratebufferpH=9,supplementedwith5%sucrose,0.2%Tween-20,0.2MNaCland0.1%sodiumazide,andtheconjugatewasstoredat4°Cuntiluse. 2.5AssemblyofthestriptestsTLandCLweredepositedontheNCM(FF120HP,GELifeSciences)aspreviouslydescribed,usingasolutionof0.5µg/µlofleptospiralantigenand1µg/µlofarabbitanti-goatIgG(a-IgGgoat ,Sigma-Aldrich),respectively.NCMweredriedat50°Cfor30min.Theconjugatepad(Standard17,GELifeSciences)wasembeddedwiththea-IgGhum /GNPssolutionpreviouslypreparedanddriedat50°Cfor30min.Membraneswereassembledonabackingcardaccordingtostandarddescriptions,eachpadoverlapped2mmwitheachother,leavingtheNCMbelowbothconjugateandabsorbentpads(CF3,Whatman,GELifeSciences),andthenthesamplepad(CF3).Finally,membraneswerecutbyapapercutterevery4mmandplacedintheirhousing. 2.6SerumsamplesTwoserapreviouslyclassifiedbyreferencetechniquesintopositivecontrolandnegativecontrolwereused,providedbytheLaboratorioNacionaldeReferenciadeLeptospirosis(LNRL),InstitutoNacionaldeEnfermedadesRespiratorias(INER),SantaFe,Argentina.TheuseofthesesampleswasapprovedbytheBioethicsCommitteeoftheFacultaddeBioquímicayCienciasBiológicasoftheUniversidadNacionaldelLitoral(CE2019-37,Acta05/19). 2.7EvaluationoftheLFIASampleswerediluted1/10inphosphatebufferedsaline(PBS)pH=7,6and100µlwereappliedonthesamplezoneofthehousing.At15minutes,thevisualizationof2lines(TLandCL)indicatesapositiveresult,andthevisualizationofonlydeCLindicatesanegativeresult. 63.ResultsandDiscussion 3.1.ExperimentalsetupandgeneralconsiderationsTheprimaryobjectiveofthisstudywastoperformadual,activeandsimultaneousdispensingoftwolineswithdifferentwidths,whichwasachievedwiththeexperimentalsetupdepictedinfigure1.Controloverthelinewidthsissought,butobtainingamathematicalexpressionthatdescribesthemexactlydependsonparametersbeyondthoseofa3Dprinter.NCMproperties(wettability,absorption,hydrophilicity,poresize,flatness),aswellasfluidproperties(vaporpressure,concentrationofreactivesubstances,viscosity),areundoubtedlysignificant.Therefore,theaimofthisstudyisnarroweddowntoempiricalcontrolwithincertainexperimentallimits,whilemaximizingthehighestdegreeofindependenceamongwidths.Certainly,commercialinstrumentsalsonecessitateapreviousempiricalstagetoevaluatethequalityofthedispensedlines. Figure1:Experimentalsetup.A)DiagramoftheexperimentalsetupB)SymbolsofcomponentsusedintheexperimentalsetupC)PhotographoftheexperimentalsetupD)SideviewoftiltedDTsdispensingontoanNCME)Diagramofthedispensingend. Infigure1A,anoutlineoftheexperimentalsetupisdepicted.Firstly,itisworthnotingthatthe3Dprinterhascontrolovernotonly2SPsbutalsoalltheparameterstypicallycontrolledbya3Dprinter,suchastemperatures,positions,velocities,etc.Tokeepthe3Dpositioningfunctions,theconnectorsoftheX,Y,andZaxesarenotusedtocommandSPs.Inotherwords,oneobjectiveofthisstudywasnottointerferewiththenormaloperationofthe3Dprinter.Onlyinthecaseofdispensing,theplasticextruderwillbedisconnected,andoneoftheSPswillbeconnectedinitsplace.Inturn,anindependentwiringsystemisschematizedforasecondSP.Sinceitiscommonforprinterstohaveasingleextruder,itwillbe 7 necessarytoinstallanadditionalpowerdriver,alongwithanappropriatecableforanotherSP.Afterthat,eachSPwillbeabletoworkwithaspecificflowrate,achievingthehighestdegreeofindependencebetweenlinewidths.Oncethishasbeenaccomplished,apartfromasinglefirmwareupdatetoenableasecondextruder(whichwillbediscussedlater),nothingmorewillberequiredotherthanconnectingtheoriginalextrudertoresumethenormalplasticprintingfunctions.Inanycase,thepowerdriversandthewiringtoeachSPmustbeadequatetotransmitthenecessaryenergytotheSPs,andanychangeinconfigurations,bothforthe3DprinterandtheSPs,mustbeabletobedonewithG-codeinruntime.InthedevelopmentofLFIAprototypesatlaboratoryscale,itiscommontohavealimitedvolumeavailablefordispensing,sometimesintheorderofmicroliters.Thisimpliesthatanydeadvolumemustbeminimized.Italsomeansthattherewillbelittlechanceofpre-testingandthatthepreviousexperiments(calibrationofspeeds,distances,etc)mustbecarriedoutwithliquidswhosecompositionmustbesimilartothatoftheliquidofinterest.IndiagramAoffigure1,thepreviouslymentionedsmallvolumesarerepresentedbysyringesandcatheters(DTs)containinggreenandyellowliquids.Byutilizing3WVsatthedispensingends,itispossibletomanuallyloadthesesmallvolumesdirectlyintothecatheters.Thesevalvesarebasicnursing/medicalsupplies,theyallowforsplittingoffluidsandtheycanbecoupledwithother3WVs(1maleand2femaleends).Unlikepassivedispensingsystems,thedescribedsystemincorporatesSPsforactiveloadingorunloading,makingitpossibletoalsoperformloadingthroughsuction. Beyondthewayinwhichsmallvolumescanbeloaded,awaytotransferthevolumedisplacementfromtheSPsisstillrequired.Apossibleapproachwouldbetofilleachsyringe,aswellasthetubingleadinguptotheDTs,withtheliquidofinterest,althoughthiswouldentailasignificantdeadvolume.Thealternativeimplementedhereinvolvesloadingreagentswith3WVs,fillingthedeadvolumecompletelywithanauxiliaryliquidandusingittodisplacethetargetliquidintheDTs.Thisliquid,referredtoastheauxiliarydisplacementfluidhere,isrepresentedinlightblueinfigure1andhasbeenultrapurewaterinthepresentreport.Ifthedilutioneffectattheinterfacewiththedisplacementfluidisnotcompatiblewiththerequiredlines,asignificantlylargervolumewillbeneededtooccupytheentiredeadvolumefromtheSPstotheDTs(afterfinishing,thisvolumecouldberecovered).However,ifitishandledwithcareandlittletimeisallowedfordiffusion,thiseffectwillbegreatlyminimized. More3WVscanbeobservedimmediatelyattheexitoftheSPs,whichareusedtoconnectanauxiliaryliquidreservoir(asyringewithoutitsplunger,orientedupwards).ThismakesiteasytopurgethesystemuptotheDTsandeliminatepotentialairbubbles.Additionally,anincompressibletubingshouldbeusedbetweenthese3WVsattheSPs'outputandthe3WVsattheDTs'input,ensuringthatthevolumedrivenbytheSPsmatchesthevolumedispensedatthetipsoftheDTs(thereshouldbenoflexibletubingthatmightabsorbenergy).TheconnectionscanbemadewithvariouscomponentsthatadheretotheLuerstandard(3WVs,needles,adaptors,etc). Inthedescribedconditions,itispossibletodispenseliquidusingonlyG-code.However,theprinteristypicallyconfiguredtoperformNustepsmicrostepsandmove1mmofplasticfilament(ofaspecificdiameter)throughanextruder.Whendispensingliquid,itisadvisablefortheunittobevolumetric 12 ,suchasμ L.Thus,itisnecessarytoreconfigurethenumberofmicrostepsrequiredinthesteppermotorofanSPtomove1μ L.Thecommandtobeusedis 8\"M92ENusteps,\"wheretheletterEindicatesthattheμ step/unitchangeisfortheExtruder(notforX,Y,orZ).Tocalculatethisvalue,itisnecessarytoperformagravimetriccalibrationofthesyringetobeusedineachSP.Furthermore,itispossibletodeterminetheinternaldiameterofasyringebasedonconstructionstandards(BD,BectonDickinson),whichenablesthecalculationofthenumberofμ stepsperunitofdisplacedvolumeaccordingtothefollowingequation: Eq1 𝑁µ𝑠𝑡𝑒𝑝 [µ𝑠𝑡𝑒𝑝𝑠/µ𝐿] = 𝑆𝑡𝑒𝑝𝑠𝑃𝑒𝑟𝑅𝑒𝑣[𝑠𝑡𝑒𝑝𝑠/𝑟𝑒𝑣] × µ𝑆𝑡𝑒𝑝𝑝𝑖𝑛𝑔[µ𝑠𝑡𝑒𝑝𝑠/𝑠𝑡𝑒𝑝]π × (𝐼𝐷[𝑚𝑚]/2) 2  × 𝑙𝑒𝑎𝑑[𝑚𝑚/𝑟𝑒𝑣]whereStepsPerRevrepresentsthenumberofstepsperrevolutionintheSPsteppermotor(200),μ Steppingisthenumberofmicrostepsperstepconfiguredinthestepperdriver(16),IDdenotestheinnerdiameterofthesyringeusedintheSP(BD10mL,14.5mm),andleadisthelinearmovementperrevolutionprovidedbythethreadedrod(leadscrew)connectedtothestepper(8mm/rev).Theparametersenclosedinparentheseswereemployedinthecurrentresearch,resultinginafinalvalueof2.4223μ step/ μ L. Asmentioned,inordertocontrolasecondSP,anewpowerdriverandwiringarerequired.Furthermore,sinceitiscommontohaveonlyoneextruder,itwillalsobenecessarytoactivateasecondextruderinthefirmwaretoenablemultipleextrusion.Inthepresentreport,Marlinwasutilized 13 ,andamongseveraloptions,theMixerExtruder 20 wasselected.Thistypeofextrudercanmeltupto6differentplasticssimultaneouslyandutilizesasinglethermistor.Thisisnotatrivialdetail,asotherconfigurationsmayrequiremultiplethermistors,andregardlessofthechoicemade,theremustbeagenuinecoherencebetweenthefirmwareparametersandtheactualhardwareconnected.Sinceacommonextruder-basedprinteralreadyhasathermistorinstalled,thereisnoneedforadditionalconnectionsinpractice.Furthermore,thereareadditionaladvantages.ThemixingisactivatedatruntimeusingG-code.Otherwise,asingleextruderconfigurationisimplemented.Inthisway,whentheoriginalplasticextruderisreconnected,theprinterresumesitsoriginalfunctions.Also,ifinsteadofamixingextruderSPsareconnected,themixingprocessisnotcarriedoutinpractice,thusachievingthehighestdegreeofindependenceforlinewidths,aseachSPcanbeutilizedwithaspecificflowrate.Oncethefirmwarehasbeenupdated,thecommand\"M165AaaBbb\"willallowconfiguringafraction\"aa/(aa+bb)\"ofthetotalvolumethatwillbedispensedbySPA,andafraction\"bb/(aa+bb)\"thatwillbedispensedbySPB(iftherewereindeedamixingprocedure,thosewouldbethefractionsofthemixture).Forexample,if20μ Laretobedispensed,M165A50B50wouldyield2lines,eachcontaining10μ L,whereasM165A75B25wouldresultin2lines,onewithAand15μ L,andtheotherwithBand5μ L. InadditiontoM92andM165,thereexistothersignificantcommands.Forinstance,\"M302P1\"enablescoldextrusion,typicallyprohibitedtopreventdamagetotheextruder.Sinceliquidsaredispensed,thereisnoneedforaminimumtemperatureforextruderprotection(whichwillnotevenbeutilized). Therearegeneralexperimentalconsiderationsthatmustbetakenintoaccount:- FixationoftheNCMtotheprinterbed:TheNCMmustremainflatandsecurelyfixedonthebed,sothisglasswascoatedwithamagneticadhesive,andmagnetictapeswereemployedforthefixationoftheNCM. 9- Hardnessofdispensingtips:theNTCiseasilyscratchedbymetalneedlesorrigidplastictips,suchasthoseusedformicropipettes.Inthisstudy,thebestresultswereachievedwithcatheters,whicharemadeofinertmaterialsandhavestandardizeddiameters.Also,inthepresenceofminorimperfectionsinthesurface,catheterscanflexiblyadaptifnecessary.- MountingoftheDTstotheprintingcarriage:Acouplingpiecewasdesigned,printedandattachedtotheeffectorofthe3Dprinter,whichcanbeobservedinfigure1D(photograph,grayplastic)andE(diagram,inorange).Asdepicted,thispieceholdstwo3WVs(forreagentloading).Furthermore,itenablestheadjustmentoftheangleoftheDTsrelativetotheNCMandtheline-to-linespacing.- VerticalspacingbetweenNCMandDTs:Differentspacingtestswereconducted.Insomecases,therewasactualspacingbetweenNCMandDTs,whileinothers,DTsmadecontactwiththeNCM.ThebestresultswereachievedbyslightlyloweringtheDTstomakecontactwiththeNCM,andthenloweringanadditional0.5mm,causingthecatheterstoflexslightly.- Bedleveling:SincethespacingandanglesbetweenDTsandNCMmustremainconsistentthroughouttheentiretrajectory,bedlevelingisofutmostimportance.Thishasbeendocumentedinsimilar3Dprintingapplications,suchasthedirectfabricationofmicrostructuredsurfacesforplanarchromatography 26 .- CouplingofSPswithDTs:AscanbeobservedindiagramAoffigure1,SPsandDTswereinterconnectedutilizing3WVs,21GneedlesandPolytetrafluoroethylene(PTFE)tubingof0.8mmid.Thistypeoftubingisdeemednon-compressible,soitisassumedthatanyvolumedisplacedfromthesyringeshouldexitthesystemwithoutobservabledelayatthedistalend.- TheutilizationofdyesandBSAinpreliminarytestsolutions:Asmentionedearlier,itisadvisabletocarryoutpreliminarytestsusingliquidsthatcloselyresemblethefinaldispensingcomposition.Therefore,inthecaseofpreliminarytestsinvolvingcoloredsolutions,afinalconcentrationof1%BSAwasalsoemployed.- AdditionalFeaturesoftheproposedsetup:Therearefeaturesthatwerenotutilizedinthisstudy.Forexample,the3Dprinterbedalsoallowsfortemperaturecontrol.Sincethisstudywasdesignedforthedispensingofbiologicalfluidsthataretypicallysensitiveeventolowtemperatures(below40°C),thisfeaturewasnotutilizedhere.However,forothertypesofdispensing,suchasEnzyme-LinkedImmunoSorbentAssay(ELISA)platereactions,thisorotherfunctionscouldprovetobeuseful.Furthermore,it'sworthhighlightingthatinthesuggestedconfiguration,the3Dprintermaintainsitsabilitytoexecute3Dmovements,whichmeansitsapplicationextendsbeyondLFIAlinesandcanbeadaptedforothermicrofluidicpaper-baseddevices,whetherfeaturing2Dor3Dgeometries 27 .And,ingeneraltermsofdispensing(notonlyonpaperorNCM),thereisregulablevolumetriccontrol.- Regardingseveraloftherequiredcomponents,theproposedmethodutilizesstandardizedandreadilyavailablenursing/medicalsupplies,suchas3WVs,catheters,syringes,needles.Inturn,thesecomponentshaveLuerinputs/outputs,whichalsostandardizestheconnectionbetweenallofthem.Additionally,inclassicsecondarymedicationsets,thereisusuallyanextensionofthetubingthatendsinamaleLuerconnector.Twoofthese,coupledwiththesametubing,makeupamale-maleLueradapter.Withtheseadapters,andthe3WVs,itispossibletomakeanycombinationoffluidconnections.Theuseofthesecomponentsenhancesthepotentialforreplicatingtheproposedsetupinvariouslaboratories.Thisalsoimplied 10theprintingofopen-sourcesyringepumpsbasedonNEMAmotors,classic3Dprintingrods,amongothers,aswellastheadaptationofacommercial3Dprinterwithopen-sourcefirmware,whichiscurrentlypresentintheinfrastructureofmanylaboratories. 3.2.Singledispensing:DispensingSpeedandDispensingRateIntheexperimentsdescribedbelow,sequentialdispensingwascarriedoutwithasingleSP,thustheycanbeperformedwithoutfirmwaremodifications.IfthepowersuppliedbytheextruderdriverissufficientfortheSP,andiftheextruderwiringcanhandlethisenergy,itisonlynecessarytoconnecttheSPanduseruntimecode.Asimple\"M92Exx\"commandwillsufficetoreconfigurethenumberofμ Steps/ μ L,aparameterthat,intheseexperiments,wassetto2.4223.TheexperimentswereconductedinthesameNCM,butinonecasewithdifferentDS(DispensingSpeed,lineardisplacementspeedoftheNCM,mm/min)andthesamedispensedvolume,andintheothercase,theywereperformedwiththesameDSandvariabledispensedvolumes.ThisresultedindifferentDR(DispensingRate,μ L/mm).Aconsistentdistanceof200mmwastraversed,encompassingaportionovertheNCM.Asinpreviousreports 16 ,beforeimageanalysis,theinitialpartofeverydispensing(40mm)wasexcludedduetoinitialexcesses(stainsfromthefirstcontactoftheNCMwiththeDT)andsubsequentnarrowingbeforereachingaSteadyStateZone.Subsequently,70mmofNCMwereanalyzed,obtainingtheaveragewidthofeachlineevery2.5mm(28bins).Infigure2,resultsobtainedfromtheanalysisoftheseNCMregionscanbeobserved. Figure2: LinewidthsforsingledispensingwithvariableDSorDR.Left:variableDS(alwayswith15μ L).Right:variableDR(alwaysat1500mm/min).Up:Photographs,detectededges(green)andCIforlinewidthsinmm.Down:Widthvaluesfromdetectededgesevery2.5mm.Allsolutionshad1%BSAandbluedye. Ontheleftsideoffigure2,itcanbeobservedthat,despitesomedifferences,inthesteadystate,especiallyfromapproximately60mm,thelinescloselyresembleeachother,andthewidthsobtainedwereintherangeofclassiclinewidthsforimmunoassays 28 .Before60mm,thewidthofline1(1500mm/min)isnoticeablygreater.ToensurethattheDTisfilledbeforestartingtodispenseonanNCM,alllineshaveaninitialprimingoutsidethemembrane.Atthebeginningofthedispenseofline1,therewasahigheraccumulationofliquid(additionalmanualpriming).Thisexcessjustifiesthegreaterwidth,especiallythroughtheeffectreferredtohereasthe\"brush\"or\"drag\"effect.Inotherwords,beyondtheinternalcontent 11 thatisflowingandwillbedispensedbytheDT,ifthereisexternalcontent,itwillbedraggeduntilitiseventuallydepleted,ultimatelyreachingasteadystate(onlyinternalflow).Excludingline1forthereasonsmentionedandwiththeexceptionofline4(4500mm/min),inwhichwideninginthemiddlesection(approximately70-80mm)wasdetected,theaveragewidthoftheremaininglinesisnotdifferentaccordingtotheANOVAtest.TheDTused(22Gcatheter)hasanouterdiameterof0.9mm.Itshouldbenotedthatthisvaluewasonlyexceededonafewoccasions,includingwhenthe\"brush\"effectispresent,anditisveryclosetotheaveragewidthofthelines.Thisimpliesthatiftherearenoflowexcessesordefects,inadditiontotheinternalwidthofaDT,itsexternalwidthwillbeoneofthedeterminingfactorsintheobtainedlinewidth.Itispossibletoobtainwiderlineswithhigherflowsandthe\"brush\"effect,butitisreasonabletoassumethatbeyondacertainflowrate,the\"brush\"effectmaynotbesufficienttoachieveauniformlinewidth,leadingtodefectssuchassignificantaccumulationsonthelines. ThesimilarityinthedispensedlinewidthsatdifferentDS(DispensingSpeed,lineardisplacementspeedoftheNCM,mm/min)arisesfromaparticularaspectoftheproposedsetup.Inthiswork,increasingDSdoesnotnecessarilyleadtoadecreaseinthedispensedlinewidth,ashasbeenreportedinsimilarstudies 16 andasonemightintuitivelyexpect.Sincethe3Dprinterhascontroloverboththebed(Y-axis,dispensingdirection)andtheSP,itispossibletouseG-codeforsimultaneousextrusionandmovement(G1,asG0isreservedfornon-extrusionmovements).Forexample,theG-code“G1F3000Y200E20”willexecutea200mmmovementatabedspeed(DS)of3000mm/minwhileproportionallydispensing20μ L.Thisimplies,ononehand,thattheparameterDR(DispensingRate,μ L/mm)willtakethevalueof0.1μ L/mm(20μ L/200mm),andontheother,it'scrucialtonotethatthestepperspeedoftheSPisnotsetbytheuserbutisinsteadrecalculatedbythefirmwareoftheprintertotraverse200mmanddispenseproportionally20μ L.Inotherwords,ifspeedlimitsimposedbythefirmwarearenotexceeded,theflowrateofaSPwilldependonthetotaldispensedvolume,thedistancetotravel,andtheDS,asexpressedbythefollowingequation: Eq.2𝑆𝑃𝑓𝑙𝑜𝑤𝑅𝑎𝑡𝑒 = 𝑡𝑜𝑡𝑎𝑙𝑉𝑜𝑙𝑢𝑚𝑒𝐷𝑖𝑠𝑝𝑒𝑛𝑠𝑒𝑑[µ𝐿]×𝐷𝑆[𝑚𝑚·𝑚𝑖𝑛 −1 ]𝑡𝑜𝑡𝑎𝑙𝐷𝑖𝑠𝑡𝑎𝑛𝑐𝑒[𝑚𝑚] =  𝐷𝑅 × 𝐷𝑆 WithincertainlimitsofDS(inthiscase,between1500and5500mm/min),thevalueusedmaybemoreempirical,forinstance,toachievegooddispensinginashorttime,butitmaynotsignificantlyaffectthelinewidths.Also,thevalueofDRwillprovidedirectinformationregardingthevolumeofliquidrequiredtoachieveacertaintotallinelength.Furthermore,becauseofitsrelationwithconcentrationandmass,theparameterofvolumeperunitlengthappearstobeabetterdescriptorinrelationtotheobtainedwidth(and,consequently,theintensityinthesignalofthetest)thantheDSandtheflowrateatwhichitwasobtained. Theanalysisofthecurvesontherightsideoffigure2indicatesthatthecurveswithtotalvolumesof3to9μ Lhaveequivalentvariances(Bartletttest),buttheirwidthsarenotstatisticallysimilar(ANOVAtest).Ontheotherhand,thecurvesfor12and15μ Lalsohaveequivalentvariances,andtheirwidthsarenotstatisticallysimilar(t-test).Allofthesedifferencesarevisuallyapparentandwereexpected.Itshouldbenotedthatline1(DS=1500mm/min,15μ Ltotal)intheleft-handcurveshasthesameparametersasline5intheright-handcurves(15μ Ltotal,DS=1500mm/min).Ifastatisticalanalysisisperformed, 12bothcurvesresulthomoscedasticandshownosignificantdifferencesinlinewidthaccordingtothet-test. 3.3.SimultaneousdispensingAsmentioned,thedualandsimultaneousdispensingexperimentswereconductedbyindependentlymanagingtwoSPs,forwhichitwasnecessarytomodifythefirmwareoncetoenabletheMixingExtrudermode.Twoexperimentswereconductedandtwovideoswererecorded.Animageanalysiswascarriedoutonasnapshotfromthesecondvideo,followingasimilarprocedureastheonepreviouslydone.ThesameDTsasinpreviousexperimentswereused,withaDSof3000mm/minanda7mmseparationbetweenDTs.Solutionsof1%BSAwithblueandorangedyesweredispensed.TheNCMusedwassmall,asoneextragoalwastoobservethebehavioroftheSPsoutsideoftheNCM.Thisinformationisvaluableforassessingwhether,undertheconfiguredconditions,thereisanexcessorlackofliquid,orifthereareissuesrelatedtotheSPsthemselvesratherthantheNCMs.Thetotaltraveldistancewas180mm.Asbefore,thebeginningoftheNCMwasnotconsideredforlinedetectionandsubsequentstatisticalanalysis(onlyinthesteady-statezone).Inbothcases,24μ Lofliquidsweredispensed.Resultscanbeseeninfigure3. Figure3:Simultaneousdispensing:24μ Lofcoloredliquidsweredispensedsimultaneouslyby2SPsovera180mmtraveldistance.Firstdispensing,linesAandC:12μ Leach.Seconddispensing,linesBandD:19.2μ Land4.8μ L,respectively.Confidenceintervalswereobtainedbystatisticallyanalyzingonlythesteady-statezone. Inthefirstsimultaneousdispensing(linesAandC),eachSPdispensed50%ofthevolume(i.e.“M165A50B50”beforedispensing).Figure3showsthatbothlinewidthsweresimilar,whichisconsistentwiththeequalityofvolumesandDTs.EachlinewasdispensedwithanapproximateDRof66.7pL/mm(12μ L/180mm).IfresultsofexperimentswithsingledispensingandvariableDR(figure2,rightside)areevaluated,itcanbeobservedthatthelineswithwidthsof0.74mmand0.95mmhadDRsof60and75pL/mm,respectively.Therefore,theobtainedwidths(0.82and0.81mm)areconsistentwithaDRof66.7pL/mm.Additionally,theexperimentswereconductedatdifferentDS(1500and3000mm/min),whichalsohadnosignificanteffects.Subsequently,theDTswererepositioned,andlinesBandDwereobtained,representing80%and20%ofthetotal24μ L(i.e.\"M165A80B20\"beforedispensing).Thedispensed 13 volumeswere19.2μ Land4.8μ L,respectively,over180mm,resultinginDRsof26.67pL/mmand106.67pL/mm,respectively.AsexpectedforlineD,thewidthobtainedfor15%ofthevolumewassmaller.ItsDRof26.67pL/mmanditswidthof0.53mmwereconsistentwiththevaluesobtainedinthefigure2(rightside),forthedispensedvolumesof3and6μ Lover200mm(0.487mmwith15pL/mmand0.562mmwith30pL/mm,respectively).RegardinglineB,theresultingDRof106.67pL/mmishigherthanallpreviouslyusedvalues.Whileitalsoyieldedthewidestline(0.96mm),onemighthaveexpectedawiderwidthforthisDR.However,basedonthepreviousexperiments,itisknownthatwiththisDT,themaximumachievablewidthswerecloseto0.96mm.Thisconstraintimpliesanupperlimitonthewidth,andtherefore,seedingwithaDRof106.67pL/mmisexcessiveundertheseconditions.ThisimpliesthattheliquiddoesnotgetabsorbeduniformlyintotheNCM,asevidencedbytheobservedartifactsandconfirmedbyawiderconfidenceintervalaroundthemeanwidthof0.96mm.RegardingthedispensedlinesoutsideoftheNCM,inthefirstcase(50%and50%),continuouslineswithaconstantwidthwereobserved,withoutexcessesorinterruptions.Ontheotherhand,inthesecondcase(80%and20%),forthethinline,therewerediscontinuitiesnotcausedbySPpulsesbutratherduetoaverylowDR.Forthethickline,amessierexitfromtheNCMwasapparentcomparedtotherest,withacontinuouslinewithoutpulses,appearingnotsomuchwiderbutrathertaller/thicker,inlinewiththeexcessinDR. 3.4.StudycaseAsanillustrativeapplication,thedispenserwasemployedtodepositthereagentsfordevelopingaLFIAcapableofdetectingspecificIgGantibodiesforhumanleptospirosis.Similarconditionstothoseusedinsection3.3wereemployed.Fora150mmsectionofNCM,10μ Lofbothreagents(TLandCL)weredispensedwhilekeepingthesameDT,DS,andDR.Figure4presentsasimplifiedschematicofthemembraneassemblyusedinalateralflowtestforhumanleptospirosisdetection(A).Theschematicincludesthefollowingkeycomponents:- NitrocelluloseMembrane:AthinstripofnitrocellulosemembranewheretheTestandControllinesweredeposited.- TestLine:Theregiononthemembranewheretheantigenwasimmobilized,correspondingtoLeptospirainterroganshomogenateataconcentrationof0.5mg/mL.- ControlLine:Anareaadjacenttothetestlinethatcontainsanti-IgGgoatantibodyataconcentrationof1mg/mL.Thiszonecapturesthelabeledconjugateandservesasavalidationofthetest'sintegrity.- ConjugatePad:Areddish-coloredareawheregoldparticlesconjugatedwithanti-humanIgGgoatantibodiesaredeposited.- SamplePad:Theareawherethesampleisapplied,consistingof1/10dilutedpositiveandnegativecontrolseruminthesamplebuffer.- AbsorbentPad:Theareawhereassayfluidsareretained. 14Figure4:DevelopmentofanLFIAforthedetectionofhumanleptospirosis.A:SchematicrepresentationandaphotographoftheLFIAstructure.B:Resultsobtainedfromassessingthetestsusingpositiveandnegativecontrolsamples. Ontheright,aphotographoftheactualassemblyisshownbeforebeingcross-cutwithaguillotinetoobtain4mm-widereactivestrips.Itshouldbenotedthatoncethebiomolecule-seededlineshavedried,theyarenolongervisible.Thesestripsaresubsequentlyplacedinthehousing(B).Followingtheapplicationof100uLofpositiveandnegativesamplesinthedesignatedarea,itstartsmigratingacrossthemembranes,carryingtheconjugateandreactingwiththespecificantibodiespresentinthepositivesample.Inthiscase,bothlinesarevisible.Conversely,intheabsenceofspecificantibodiesinthesample,onlythecontrollineisobserved. TheproposedconfigurationallowedthedevelopmentofanLFIAforLeptospirosisdetectionanditalsooffersadditionaladvantages.Firstly,intheinfrastructureofacontemporarylaboratory,itiscustomary(oradvisable)tohavea3Dprinter.Thecostofthisequipmentislow,muchlowerthanseveralotherlaboratoryinstruments,includingcommercialSPs.Furthermore,thecomponentsusedinthisworkarealsoinexpensive.Whethertheyare3DprintingsuppliesforproducingSPs(plasticfilaments,powerdrivers,NEMAsteppers,linearbearings)ornursing/medicalstandards,theyarereadilyavailableonaglobalscale.Secondly,everythingiscontrolledbythe3Dprinter,eliminatingboththeneedtorenderitunusableforplasticprintingandtherequirementformanualactuatorcoordination,evenfor 15 simultaneousdispensingwithdifferentlinewidths.Furthermore,whileitwasdemonstratedasanexampleforLFIA,theproposedsetupisnotlimitedtothat,asitpossessesadditionalcapabilities(temperaturecontrol,3Dpositioning,generaladjustablevolumetriccontrol,amongothers).Thirdly,whereasothersetupsnecessitatefillingtheentiredeadvolumewithreagentsor,conversely,areconstrainedtosmallloads(onlywithintheDT),theoneproposedherecanbeadaptedtoanysituation. 164.Conclusions Thesuccessfuldispensingofreagentsforimmunochromatographicteststripswasachievedthroughtheintegrationof3Dprinters,syringepumpsandnursing/medicalbasicsupplies.Withthesetupreported,itwaspossibletoactivelydispensebothindividualandsimultaneouslines.Theunifiedcontrolofthe3Dprinterenablesdual,active,regulableandsimultaneousdispensing,fourfeaturesthataretypicallyfoundonlyincertainhigh-costcommercialequipment.Inallcasesthewidthoftheselinescouldbeconfiguredwithincertainempiricallimits.Asapracticalexample,agenuineLFIAforleptospirosiswasalsoobtained.Also,thesetupcanbeadaptedtothevolumeactuallyavailablefordispensing.Alltheresourcesusedarelowcostandgloballyaccessible,whichfacilitatesthereproducibilityofthesetup.Thisincludesthenursingsupplies,theSPsbuilt,the3Dprinteritselfandalsothefirmwareused(opensourceandfreelyavailable).Thestandardfunctionoftheprintercanberestoredafterreconnectingtheextruder,andinaddition,itcanbeusedtoperformothertasksnotexemplifiedhere(othertypesofdispensing,suchasfornon-lineargeometriespaper-baseddevicesorinELISAplates,temperaturecontrol,amongothers). AcknowledgmentsWearegratefultoAgenciaNacionaldePromociónCientíficayTecnológica(ANPCyT):PICT2017-2040,ConsejoNacionaldeInvestigacionesCientíficasyTécnicas(CONICET):PIBAA28720210100763COandUniversidadNacionaldelLitoral(UNL):CATTNº:06-01-2021fortheirﬁnancialsupport.AndtoDra.BibianaVanasco(InstitutoNacionaldeEnfermedadesRespiratorias“Dr.EmilioConi”,SantaFe,Argentina)fortheprovisionoftheserasamples. 17References (1) DiNardo,F.;Chiarello,M.;Cavalera,S.;Baggiani,C.;Anfossi,L.TenYearsofLateralFlowImmunoassayTechniqueApplications:Trends,ChallengesandFuturePerspectives.Sensors2021,21(15),5185.https://doi.org/10.3390/s21155185.(2) Tripathi,P.;Upadhyay,N.;Nara,S.RecentAdvancementsinLateralFlowImmunoassays:AJourneyforToxinDetectioninFood.Crit. Rev. FoodSci. Nutr. 2018,58(10),1715–1734.https://doi.org/10.1080/10408398.2016.1276048.(3) Alhammadi,M.;Aliya,S.;Umapathi,R.;Oh,M.-H.;Huh,Y.S.ASimultaneousQualitativeandQuantitativeLateralFlowImmunoassayforOn-SiteandRapidDetectionofStreptomycininPigBloodSerumandUrine.Microchem. J. 2023,195,109427.https://doi.org/10.1016/j.microc.2023.109427.(4) Jara,M.D.L.;Alvarez,L.A.C.;Guimarães,M.C.C.;Antunes,P.W.P.;deOliveira,J.P.LateralFlowAssayAppliedtoPesticidesDetection:RecentTrendsandProgress.Environ. Sci. Pollut. Res. 2022,29(31),46487–46508.https://doi.org/10.1007/s11356-022-20426-4.(5) Feng,R.;Wang,M.;Qian,J.;He,Q.;Zhang,M.;Zhang,J.;Zhao,H.;Wang,B.MonoclonalAntibody-BasedEnzyme-LinkedImmunosorbentAssayandLateralFlowImmunoassayfortheRapidScreeningofParaquatinAdulteratedHerbicides.Microchem. J. 2022,180,107644.https://doi.org/10.1016/j.microc.2022.107644.(6) Kalligosfyri,P.M.;Tragoulias,S.S.;Tsikas,P.;Lamprou,E.;Christopoulos,T.K.;Kalogianni,D.P.DesignandValidationofaThree-DimensionalPrinter-BasedSystemEnablingRapid,Low-CostConstructionoftheBiosensingAreasofLateralFlowDevicesforImmunoassaysandNucleicAcidAssays.Anal. Chem. 2024,96(1),572–580.https://doi.org/10.1021/acs.analchem.3c04915.(7) Wijnen,B.;Hunt,E.J.;Anzalone,G.C.;Pearce,J.M.Open-SourceSyringePumpLibrary.PLOSONE2014,9(9),e107216.https://doi.org/10.1371/journal.pone.0107216.(8) Booeshaghi,A.S.;Beltrame,E.daV.;Bannon,D.;Gehring,J.;Pachter,L.PrinciplesofOpenSourceBioinstrumentationAppliedtothePoseidonSyringePumpSystem.Sci.Rep. 2019,9(1),12385.https://doi.org/10.1038/s41598-019-48815-9.(9) Samokhin,A.S.SyringePumpCreatedUsing3DPrintingTechnologyandArduinoPlatform.J. Anal. Chem. 2020,75(3),416–421.https://doi.org/10.1134/S1061934820030156.(10) Maya,F.;Estela,J.M.;Cerdà,V.CompletelyAutomatedIn-SyringeDispersiveLiquid–LiquidMicroextractionUsingSolventsLighterthanWater.Anal. Bioanal. Chem.2012,402(3),1383–1388.https://doi.org/10.1007/s00216-011-5572-4.(11) Horstkotte,B.;Solich,P.TheAutomationTechniqueLab-In-Syringe:APracticalGuide.Molecules2020,25(7),1612.https://doi.org/10.3390/molecules25071612.(12) Baas,S.;Saggiomo,V.Ender33DPrinterKitTransformedintoOpen,ProgrammableSyringePumpSet.HardwareX2021,10,e00219.https://doi.org/10.1016/j.ohx.2021.e00219.(13) MarlinFirmware2.0.https://marlinfw.org/.(14) Wijnen,B.;Anzalone,G.C.;Haselhuhn,A.S.;Sanders,P.G.;Pearce,J.M.FreeandOpen-SourceControlSoftwarefor3-DMotionandProcessing.2016,4(1),e2.https://doi.org/10.5334/jors.78.(15) Han,W.;Shin,J.H.Low-Cost,Open-Source3DPrintedAntibodyDispenserforDevelopmentandSmall-ScaleProductionofLateralFlowAssayStrips.HardwareX2021,9,e00188.https://doi.org/10.1016/j.ohx.2021.e00188.(16) Nash,M.A.;Hoffman,J.M.;Stevens,D.Y.;Hoffman,A.S.;Stayton,P.S.;Yager,P.Laboratory-ScaleProteinStripingSystemforPatterningBiomoleculesontoPaper-BasedImmunochromatographicTestStrips.Lab. Chip2010,10(17),2279–2282.https://doi.org/10.1039/C004991C.(17) Le,A.P.H.;Nguyen,Q.L.;Pham,B.H.;Cao,T.H.M.;Vo,T.V.;Huynh,K.;Ha,H.T.T. 18SALAD:Syringe-BasedArduino-OperatedLow-CostAntibodyDispenser.HardwareX2023,15,e00455.https://doi.org/10.1016/j.ohx.2023.e00455.(18) Levett,P.N.Sequence-BasedTypingofLeptospira:EpidemiologyintheGenomicEra.PLoSNegl. Trop. Dis. 2007,1(2),e120.https://doi.org/10.1371/journal.pntd.0000120.(19) Chiani,Y.;Jacob,P.;Varni,V.;Landolt,N.;Schmeling,M.F.;Pujato,N.;Caimi,K.;Vanasco,B.IsolationandClinicalSampleTypingofHumanLeptospirosisCasesinArgentina.Infect. Genet. Evol. J. Mol. Epidemiol. Evol. Genet. Infect. Dis. 2016,37,245–251.https://doi.org/10.1016/j.meegid.2015.11.033.(20) Taubert,P.Continuously-VariableMaterialPropertiesinRepRap3DPrinting,UniversityofBath,2012.https://www.reprap.org/mediawiki/images/a/a5/Pia-taubert-material-mixing-report.pdf.(21) Canny,J.AComputationalApproachtoEdgeDetection.IEEETrans. PatternAnal.Mach. Intell. 1986,PAMI-8(6),679–698.https://doi.org/10.1109/TPAMI.1986.4767851.(22) Bradski,G.TheOpenCVLibrary.DrDobbsJ. Softw. Tools2000.(23) Adler,B.;Murphy,A.M.;Locarnini,S.A.;Faine,S.DetectionofSpecificAnti-LeptospiralImmunoglobulinsMandGinHumanSerumbySolid-PhaseEnzyme-LinkedImmunosorbentAssay.J. Clin. Microbiol. 1980,11(5),452–457.(24) Bainor,A.;Chang,L.;McQuade,T.J.;Webb,B.;Gestwicki,J.E.BicinchoninicAcid(BCA)AssayinLowVolume.Anal. Biochem. 2011,410(2),310–312.https://doi.org/10.1016/j.ab.2010.11.015.(25) Hermanson,G.T.Chapter14-MicroparticlesandNanoparticles.InBioconjugateTechniques(ThirdEdition);Hermanson,G.T.,Ed.;AcademicPress:Boston,2013;pp549–587.https://doi.org/10.1016/B978-0-12-382239-0.00014-5.(26) Macdonald,N.P.;Currivan,S.A.;Tedone,L.;Paull,B.DirectProductionofMicrostructuredSurfacesforPlanarChromatographyUsing3DPrinting.Anal. Chem.2017,89(4),2457–2463.https://doi.org/10.1021/acs.analchem.6b04546.(27) Lim,H.;Jafry,A.T.;Lee,J.Fabrication,FlowControl,andApplicationsofMicrofluidicPaper-BasedAnalyticalDevices.Molecules2019,24(16),2869.https://doi.org/10.3390/molecules24162869.(28) Jones,K.TroubleshootingProteinBindinginNitrocelluloseMembranes.Part1:Principles.IVDTechnol. 1999,5,32–41. 19",
      "references": [
        "TenYearsofLateralFlowImmunoassayTechniqueApplications:Trends,ChallengesandFuturePerspectives.",
        "RecentAdvancementsinLateralFlowImmunoassays:AJourneyforToxinDetectioninFood.",
        "ASimultaneousQualitativeandQuantitativeLateralFlowImmunoassayforOn-SiteandRapidDetectionofStreptomycininPigBloodSerumandUrine.",
        "LateralFlowAssayAppliedtoPesticidesDetection:RecentTrendsandProgress.",
        "MonoclonalAntibody-BasedEnzyme-LinkedImmunosorbentAssayandLateralFlowImmunoassayfortheRapidScreeningofParaquatinAdulteratedHerbicides.",
        "DesignandValidationofaThree-DimensionalPrinter-BasedSystemEnablingRapid,Low-CostConstructionoftheBiosensingAreasofLateralFlowDevicesforImmunoassaysandNucleicAcidAssays.",
        "Open-SourceSyringePumpLibrary.",
        "PrinciplesofOpenSourceBioinstrumentationAppliedtothePoseidonSyringePumpSystem.",
        "SyringePumpCreatedUsing3DPrintingTechnologyandArduinoPlatform.",
        "CompletelyAutomatedIn-SyringeDispersiveLiquid–LiquidMicroextractionUsingSolventsLighterthanWater.",
        "TheAutomationTechniqueLab-In-Syringe:APracticalGuide.",
        "Ender33DPrinterKitTransformedintoOpen,ProgrammableSyringePumpSet.",
        "MarlinFirmware2.0.",
        "FreeandOpen-SourceControlSoftwarefor3-DMotionandProcessing.",
        "Low-Cost,Open-Source3DPrintedAntibodyDispenserforDevelopmentandSmall-ScaleProductionofLateralFlowAssayStrips.",
        "Laboratory-ScaleProteinStripingSystemforPatterningBiomoleculesontoPaper-BasedImmunochromatographicTestStrips.",
        "SALAD:Syringe-BasedArduino-OperatedLow-CostAntibodyDispenser.",
        "Sequence-BasedTypingofLeptospira:EpidemiologyintheGenomicEra.",
        "IsolationandClinicalSampleTypingofHumanLeptospirosisCasesinArgentina.",
        "Continuously-VariableMaterialPropertiesinRepRap3DPrinting",
        "AComputationalApproachtoEdgeDetection.",
        "TheOpenCVLibrary.",
        "DetectionofSpecificAnti-LeptospiralImmunoglobulinsMandGinHumanSerumbySolid-PhaseEnzyme-LinkedImmunosorbentAssay.",
        "BicinchoninicAcid(BCA)AssayinLowVolume.",
        "Chapter14-MicroparticlesandNanoparticles.",
        "DirectProductionofMicrostructuredSurfacesforPlanarChromatographyUsing3DPrinting.",
        "Fabrication,FlowControl,andApplicationsofMicrofluidicPaper-BasedAnalyticalDevices.",
        "TroubleshootingProteinBindinginNitrocelluloseMembranes.Part1:Principles."
      ],
      "meta_data": {
        "arxiv_id": "2402.04354v1",
        "authors": [
          "Gabriel Siano",
          "Leandro Peretti",
          "Juan Manuel Marquez",
          "Nazarena Pujato",
          "Leonardo Giovanini",
          "Claudio Berli"
        ],
        "published_date": "2024-02-06T19:37:05Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Addresses low-cost, reproducible laboratory-scale manufacturing of lateral flow immunoassays (LFIA), focusing on automated deposition of test and control lines (TL/CL) on nitrocellulose membranes. Main contribution is an adaptation of a common FFF 3D printer to jointly control two stepper-driven syringe pumps (SPs) for dual, active (pressure-driven), regulable and simultaneous dispensing, while preserving the printer’s normal 3D-printing capability by swapping connections with the extruder. Demonstrates empirical control of line width via volumetric dispensing rate (µL/mm) and shows a practical LFIA case study for human leptospirosis detection produced with the system.",
        "methodology": "Hardware/firmware co-design: (1) Modify a commercial 3D printer (Hellbot Magna I3) by adding a second stepper driver (A4988), wiring for an additional ‘extruder’ channel, and a 3D-printed dispensing-tip holder mounting two catheter tips with adjustable angle/spacing. (2) Use open-source, 3D-printable syringe pumps (lead screw + NEMA stepper) connected via Luer-lock medical components (3-way valves, PTFE tubing, needles) to minimize reagent loss by loading small volumes directly into tips and using an auxiliary displacement fluid to fill dead volume. (3) Update firmware to Marlin 2 enabling Mixer Extruder mode to command two independent pumps via G-code; use M92 to set microsteps per µL (computed via syringe geometry and gravimetric calibration) and M165 to allocate per-pump volumetric fractions during simultaneous dispensing; use M302 to allow ‘cold extrusion’. (4) Define dispensing parameters: dispensing speed (bed travel, mm/min), total volume, travel distance, and dispensing rate DR=µL/mm; relate SP flow rate to DR×DS via firmware-synchronized motion. (5) Quantify line width and uniformity using Python/OpenCV Canny edge detection, measuring width every 2.5 mm along steady-state regions.",
        "experimental_setup": "Instrumentation: Modified Hellbot Magna I3 3D printer; Marlin 2 firmware; Pronterface for runtime G-code. Two 3D-printed syringe pumps using 8 mm lead/2 mm pitch screw (8 mm/rev), NEMA stepper (200 steps/rev), 1/16 microstepping; catheter dispensing tips (22G Jelco, OD ~0.9 mm); PTFE tubing (0.8 mm ID); 3-way valves and Luer adapters; magnetic bed fixation for membranes. Calibration: set 2.4223 microsteps/µL for a 10 mL BD syringe (ID 14.5 mm) using Eq.1 and/or gravimetric calibration. Linewidth experiments: (a) Single-pump sequential dispensing over 200 mm travel; exclude first 40 mm, analyze 70 mm steady-state; vary dispensing speed DS (1500–5500 mm/min) at fixed total volume 15 µL; and vary total volume (3–15 µL) at fixed DS=1500 mm/min, yielding different DR. Stats: ANOVA, t-tests, Bartlett test for variance; compute confidence intervals on width. (b) Dual simultaneous dispensing over 180 mm travel with DT spacing 7 mm and DS=3000 mm/min; total 24 µL split 50/50 (12/12 µL) and 80/20 (19.2/4.8 µL) using M165; assess steady-state linewidths and observe artifacts outside NCM. Case-study LFIA: Prepare Leptospira interrogans Hardjo lysate antigen; synthesize ~40 nm Au nanoparticles and conjugate anti-human IgG; dispense TL (antigen 0.5 µg/µL; manuscript later states 0.5 mg/mL) and CL (rabbit anti-goat IgG 1 µg/µL; later 1 mg/mL) onto FF120HP nitrocellulose, dry 50°C 30 min; embed conjugate pad, assemble pads with 2 mm overlap, cut 4 mm strips, house. Validation: test with two reference-classified human sera (positive/negative controls), diluted 1:10 in PBS; apply 100 µL; read visually at 15 min (TL+CL positive; CL only negative).",
        "limitations": "Linewidth control is empirical and constrained by membrane/fluid properties (wettability, pore size, flatness; viscosity, evaporation) and by dispensing tip geometry—observed upper bound near catheter OD (~0.9–1.0 mm). Very high DR caused non-uniform absorption and artifacts (thicker/taller deposits, wider CI), while very low DR caused discontinuities. Initial ‘brush/drag’ effect and priming can broaden early segments; steady-state requires excluding initial length. Assumes negligible compliance in tubing (uses PTFE) and minimal diffusion at the interface between displacement fluid and reagent; dilution/mixing can occur if not handled quickly. Requires good bed leveling and careful DT–NCM contact; risk of scratching if rigid tips are used. Dual-pump capability depends on firmware modification (Mixer Extruder) and additional driver/wiring; replicability may vary across printer electronics/torque limits. Biological validation is limited (only control sera, qualitative readout, no sensitivity/specificity study or large-sample clinical evaluation).",
        "future_research_directions": "Develop predictive models linking DR, tip geometry, contact force, and membrane properties to linewidth and deposited mass to reduce empirical tuning; include real-time feedback (camera/vision) for closed-loop control of line width and continuity. Extend to >2 simultaneous lines (multi-analyte LFIA) by adding more pump channels or toolheads; explore automated reagent switching/cleaning to reduce cross-contamination and diffusion at displacement interfaces. Quantify deposited biomolecule mass per length and correlate with assay performance (LOD, signal-to-noise), optimizing different widths for TL vs CL. Evaluate broader membrane types and fluids, and exploit bed temperature/humidity control to manage drying/evaporation. Improve mechanical design for consistent DT contact pressure and automated bed leveling. Perform comprehensive analytical/clinical validation with larger sample sets and compare against commercial stripers (repeatability, CV, throughput, dead volume, cost). Adapt platform for other dispensing patterns (2D/3D paper microfluidics), ELISA plate dispensing, and other biofabrication tasks using the printer’s 3D motion capabilities.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Improving Complex Reasoning with Dynamic Prompt Corruption: A Soft Prompt Optimization Approach",
      "full_text": "00(00) (0000) 1-9 Novel analytical solutions to a new formed model of the (2+1)-dimensional BKP equation using a novel expansion technique Rajib Mia † Department of Mathematics, School of Applied Sciences, Kalinga Institute of Industrial Technology (KIIT) Deemed to be University, Bhubaneswar, 751024, Odisha, India Submission Info Communicated by Albert Luo Received 01-09-2021 Accepted 01-09-2021 Available online 01-09-2021 Keywords Nonlinear PDEs BKP equation Exact solution Traveling wave solutions Abstract In this article, we present a comprehensive analytical study to obtain the exact traveling wave solutions to a new formed model of the (2+1)-dimensional BKP equation. We construct exact solutions of the considered model using a recently developed expansion technique. This current proposed technique has been successfully implemented to obtain a few exact solutions of a new formed (2+1)-dimensional BKP equation. In order to understand the physical interpretation of solutions effectively, the 2D and 3D graphs are plotted for each type of the solutions obtained for different particular values of the parameters. Furthermore, it is found that the obtained solutions are periodic and solitary wave solutions. We anticipate that the proposed method is reliable and can be applied for obtaining wave solutions of the other nonlinear evolution equations (NLEEs). ©2022 1 Introduction Nowadays the exact solution of nonlinear evolution equation is a very active research area. In the study of nonlinear physical phenomena, the exact analytical traveling wave solutions of the non–linear partial–differential equations (NPDEs) plays a very significant role. The non-linear physical wave phe- nomena takes place in several engineering and scientific fields. For example, in plasma physics, optical fibers, chemical physics and geochemistry, biology, solid state physics and fluid mechanics, the non- linear evolution equations are broadly used as a model to study the physical phenomena. Various powerful effective techniques have been established to obtain the exact traveling wave solutions of various nonlinear evolution equations. Some of these powerful techniques are the sine- Gordon method [1–3], the Sardar sub-equation technique [4–6], the modified Sardar sub-equation method [7, 8], the Riccati-Bernoulli sub-ODE method [9, 10], the \u0010 G′ G \u0011 -expansion method [11–16], the\u0010 G′ G , 1 G \u0011 -expansion method [17–20], the modified exp-function method [21–24], the simple equation method (SEM) [25,26], the tanh–coth expansion method [27–29], \u0000 1 G′ \u0001 -expansion technique [20,30–32], Lie symmetry approach [33–36] and so on. Recently, Ahmed et al. have found mixed lump, periodic-lump and breather soliton solutions to the (2 + 1)-dimensional KP equation with the help of symbolic computation using the Hirota bilinear †Corresponding author. Email address: rajib.miafma@kiit.ac.in 0000.00.000 arXiv:2401.01594v1  [math-ph]  3 Jan 20242 Author 1, Author 2 / Journal of Applied Nonlinear Dynamics 00(0) (2022) page1–page2 method [37]. They have also observed some new type of characteristics of the periodic–lump solutions and kinky–breather solitons. Kayum et al. have established stable traveling wave solutions to the nonlinear Klein–Gordon equation in particle physics, condensed matter physics, solid state physics, the gas dynamics and nonlinear optics equation utilizing the sine-Gordon expansion technique [38]. They have established the familiar stable wave solutions covering an extensive range which related to free parameters. They have obtain various new solutions for the particular values of the parameters. In the article [39], a generalized ( G′/G)-expansion technique is used for deriving the exact solitary wave solutions generated in the form of trigonometric, hyperbolic and rational functions for the the non-linear and regularized long wave (RLW) as well as for the Riemann–wave (RW) models. Shakeel et al. [23] have obtained various types of analytical exact wave solutions such as complex function and hyperbolic solutions of the strain-wave equation found in micro-structured solids, which is very important in the field of solid physics, utilizing the modified exp-function technique. Tripathy et al. [40] and Khaliq et al. [41] have successfully applied the proposed expansion technique to obtain some novel exact traveling wave solutions of the ion sound Langmuir wave model and the (2 + 1)-dimensional Boussinesq equation respectively. They have demonstrated the physical interpretation of nonlinear processes and the efficiency of the proposed method. In this current work, our main purpose is to obtain the new exact traveling wave solutions to a new formed equation of the ( 2 + 1)-dimensional BKP model. In this work, for the first time we have used\u0010 G′ G′+G+A \u0011 -expansion technique [41–43] to obtain a few analytical solutions to a new formed equation of the ( 2 + 1)-dimensional BKP model. We have plotted the 3D and 2D graphics simulations of the newly obtained exact solutions. By observing the various nature of the waves and from numerical simu- lations one can find more useful information regarding the new reduced form of the ( 2+1)-dimensional generalized BKP equation. This work is arranged as follows: In the Section 2, we have explained the algorithm of the \u0010 G′ G′+G+A \u0011 - expansion technique. In Section 3, we have implemented the proposed method to obtain the exact traveling wave solutions to a new form of the (2+1)-dimensional B-type Kadomtsev-Petviashvili (BKP) equation. Results and discussions are given in Section 4. Lastly, Section 5 is devoted to the conclusions. 2 The algorithm of the \u0010 G′ G′+G+A \u0011 - expansion technique The steps of the \u0010 G′ G′+G+A \u0011 -expansion technique to find the exact traveling wave solutions of the nonlin- ear evolution equations (NLEEs) are discussed in this section. Consider the general non-linear evolution equation of the following type f (u,uy,ux,ut,uxx,uyy,uyx,uxt,uxy,... ) = 0, (1) where the function u ≡ u(x,y,t) is not known, f is a polynomial function involving u and its higher order derivatives together with nonlinear terms. Subscripts in this Eq. (1) indicate the various partial derivatives. The main steps of the \u0010 G′ G′+G+A \u0011 - expansion technique are given below: Step-I: The traveling wave variable is defined as ξ = nx+my− pt by taking three independent variables x,y, and t into one variable ξ. Here n, m and p are constants. Let us assume that the solution of Eq. (1) in ξ as u(x,y,t) = w(ξ). (2) Using the above transformation ξ = nx+my− pt and u(x,y,t) =w(ξ) in the Eq. (1), we get the following ordinary differential equation (ODE): g(w,w′,w′′,w′′′,... ) = 0, (3)Author 1, Author 2 / Journal of Applied Nonlinear Dynamics 00(0) (2022) page1–page2 3 where w′ = dw dξ , w′′ = d2w dξ2 , w′′′ = d3w dξ3 ... . Step-II: We suppose that the exact analytical solutions of Eq. (3) can be written in the following form: w(ξ) = N ∑ k=0 ai \u0012 G′ G′ +G +A \u0013k , (4) where G ≡ G(ξ) satisfies the following auxiliary second order ODE: G′′ +BG′ +CG +AC = 0. (5) Here the prime indicates for ordinary derivative with respect to ξ. In the above equation A,B,C are real constants and ak(k = 0,1,2,..., N) are arbitrary constants. Step-III: By the homogeneous balance principle and balancing the highest order derivative ofw with the highest order nonlinear term in Eq. (3), determine the positive integer N. Furthermore, the coefficients ak (k = 0,1,2,... N) can be found by solving a system of linear algebraic equations which will come from suggested method. Then using the values of A,B and C and Eq. (5), the function G(ξ) will be evaluated. Step-IV: Lastly, on substitution of Eq. (4) in the Eq. (3), we can obtain the polynomial of \u0010 G′ G′+G+A \u0011 . Then by equating the coefficients of like powers of \u0010 G′ G′+G+A \u0011 from both sides, we will get a system of algebraic equations for η, ak (k = 0,1,2,... N), A,B, and C. 3 Implementation of the proposed technique In this current section, we have presented the implementation of the proposed \u0010 G′ G′+G+A \u0011 -expansion method to derive the exact analytical wave solutions to a new reduced form of the ( 2+1)-dimensional BKP model. The reduced new form ( 2+1)-dimensional BKP model could be obtained from the (3+1)- dimensional generalized BKP model by setting the spatial variable z = x [44]. The new formed equation of the (2 +1)-dimensional BKP model is presented below [44,45]: uxxxy +α(uyux)x + (uy +2ux)t −(uyy +2uxx) = 0, (6) where α is a constant. Recently Kara et al. [45] applied the \u0010 G′ G , 1 G \u0011 -expansion method to the new form of the (2+1)−dimensional BKP equation and obtained exact solutions in the form of hyperbolic, trigonometric and rational functions. Utilizing the wave transformation u(x,y,t) = w(ξ) where ξ = nx +my − pt, the Eq. (6) reduces to the following ODE n3mwiv +2αn2mw′w′′ −(2n2 +m2 +2np +mp)w′′ = 0. (7) By assuming w′(ξ) =U(ξ) and η = −(2n2 +m2 +2np +mp), the Eq. (7) reduces to n3mU′′′ +2αn2mUU′ +ηU′ = 0. (8) Integrating Eq. (8) with respect to ξ and assuming the integration constant equal to zero, we obtain n3mU′′ +αn2mU2 +ηU = 0. (9) By the homogeneous balance principle, balancing between the terms U′′ and U2 in Eq. (9), gives N +2 = 2N which implies N = 2. Hence from Eq. (4), the solution of Eq. (9) can be written as: U(ξ) = 2 ∑ i=0 ai \u0012 G′ G′ +G +A \u0013i . (10)4 Author 1, Author 2 / Journal of Applied Nonlinear Dynamics 00(0) (2022) page1–page2 Utilizing Eq. (10) into Eq. (9) and equating the coefficients of similar powers of \u0010 G′ G′+G+A \u0011 of Eq. (9), we obtain a system of linear algebraic equations which is given below:    a1BCmn3 −2a1C2mn3 +2a2C2mn3 +a0η +αa2 0mn2 = 0, a1B2mn3 −6a1BCmn3 +6a2BCmn3 +6a1C2mn3 −12a2C2mn3 +2a1Cmn3 +a1η +2αa0a1mn2 = 0, −3a1B2mn3 +4a2B2mn3 +9a1BCmn3 −24a2BCmn3 +3a1Bmn3 −6a1C2mn3 +24a2C2mn3 −6a1Cmn3 +8a2Cmn3 +a2η +αa2 1mn2 +2αa0a2mn2 = 0, 2a1B2mn3 −10a2B2mn3 −4a1BCmn3 +30a2BCmn3 −4a1Bmn3 +10a2Bmn3 +2a1C2mn3 −20a2C2mn3 +4a1Cmn3 −20a2Cmn3 +2a1mn3 +2αa1a2mn2 = 0, 6a2B2mn3 −12a2BCmn3 −12a2Bmn3 +6a2C2mn3 +12a2Cmn3 +6a2mn3 +αa2 2mn2 = 0. (11) By solving the above system in Eq. (11), we obtain two sets of solutions as given below: SET-1: η = 4Cmn3 −B2mn3, a0 = −6Cn(−B +C +1) α , a1 = 6 \u0000 B2n −3BCn−Bn +2C2n +2Cn \u0001 α , a2 = −6n(B −C −1)2 α SET-2: η = mn3 \u0000 B2 −4C \u0001 , a0 = −B2n +6BCn−6C2n −2Cn α , a1 = 6 \u0000 B2n −3BCn−Bn +2C2n +2Cn \u0001 α , a2 = −6n(B −C −1)2 α For SET-1, we obtain the following exact traveling wave solutions: Case-I: When Λ = B2 −4C > 0 U11(x,y,t) = −6nC(C −B +1) α +6 \u0000 B2n −3BCn−Bn +2C2n +2Cn \u0001 α   C1 \u0010 B + √ Λ \u0011 +C2 \u0010 B − √ Λ \u0011 e √ Λξ C1 \u0010 B + √ Λ−2 \u0011 +C2 \u0010 B − √ Λ−2 \u0011 e √ Λξ   −6n(B −C −1)2 α   C1 \u0010 B + √ Λ \u0011 +C2 \u0010 B − √ Λ \u0011 e √ Λξ C1 \u0010 B + √ Λ−2 \u0011 +C2 \u0010 B − √ Λ−2 \u0011 e √ Λξ   2 . (12) Case-II: When Λ = B2 −4C < 0 U12(x,y,t) = −6Cn(−B +C +1) α +6 \u0000 nB2 −3nBC−nB +2nC2 +2nC \u0001 α × (13)Author 1, Author 2 / Journal of Applied Nonlinear Dynamics 00(0) (2022) page1–page2 5   sin \u0010√ −Λ 2 ξ \u0011\u0000 BC2 +C1 √ −Λ \u0001 +cos \u0010√ −Λ 2 ξ \u0011\u0000 BC1 −C2 √ −Λ \u0001 sin \u0010√ −Λ 2 ξ \u0011\u0000 (B −2)C2 +C1 √ −Λ \u0001 +cos \u0010√ −Λ 2 ξ \u0011\u0000 (B −2)C1 −C2 √ −Λ \u0001   −6n(B −C −1)2 α ×   sin \u0010√ −Λ 2 ξ \u0011\u0000 BC2 +C1 √ −Λ \u0001 +cos \u0010√ −Λ 2 ξ \u0011\u0000 BC1 −C2 √ −Λ \u0001 sin \u0010√ −Λ 2 ξ \u0011\u0000 (B −2)C2 +C1 √ −Λ \u0001 +cos \u0010√ −Λ 2 ξ \u0011\u0000 (B −2)C1 −C2 √ −Λ \u0001   2 . For SET-2, we get the following exact traveling wave solutions: Case-I: When Λ = B2 −4C > 0 U21(x,y,t) = −B2n +6BCn−6C2n −2Cn α + 6 \u0000 B2n −3BCn−Bn +2C2n +2Cn \u0001 α   C1 \u0010 B + √ Λ \u0011 +C2 \u0010 B − √ Λ \u0011 e √ Λξ C1 \u0010 B + √ Λ−2 \u0011 +C2 \u0010 B − √ Λ−2 \u0011 e √ Λξ   −6n(B −C −1)2 α   C1 \u0010 B + √ Λ \u0011 +C2 \u0010 B − √ Λ \u0011 e √ Λξ C1 \u0010 B + √ Λ−2 \u0011 +C2 \u0010 B − √ Λ−2 \u0011 e √ Λξ   2 . Case-II: When Λ = B2 −4C < 0 U22(x,y,t) = −B2n +6BCn−6C2n −2Cn α + 6 \u0000 B2n −3BCn−Bn +2C2n +2Cn \u0001 α × (14)   sin \u0010√ −Λ 2 ξ \u0011\u0000 BC2 +C1 √ −Λ \u0001 +cos \u0010√ −Λ 2 ξ \u0011\u0000 BC1 −C2 √ −Λ \u0001 sin \u0010√ −Λ 2 ξ \u0011\u0000 (B −2)C2 +C1 √ −Λ \u0001 +cos \u0010√ −Λ 2 ξ \u0011\u0000 (B −2)C1 −C2 √ −Λ \u0001   −6n(B −C −1)2 α   sin \u0010√ −Λ 2 ξ \u0011\u0000 BC2 +C1 √ −Λ \u0001 +cos \u0010√ −Λ 2 ξ \u0011\u0000 BC1 −C2 √ −Λ \u0001 sin \u0010√ −Λ 2 ξ \u0011\u0000 (B −2)C2 +C1 √ −Λ \u0001 +cos \u0010√ −Λ 2 ξ \u0011\u0000 (B −2)C1 −C2 √ −Λ \u0001   2 . 4 Results and discussion In this section, we outline a variety of traveling wave solutions representations of the obtained results for the different values of the associated parameters. Integrating U11, U12, U21 and U22 given in Eqs. (12)-(14) with respect to ξ, we get the required solutions and have shown the 2D and 3D plots of four different solutions in different cases (see Figs. 1-4). The kink shape soliton solution for SET-1(Case-1) is presented in Fig. 1 for the associated parameter values “ t = 1;α = 1;B = 1;C = 0.1;C1 = 1,C2 = 1, n = 1;m = 1; p = −0.8”. In this figure, (I) represents the 3D plot of Eq. (12) and curve (II) represents the corresponding 2D plot of Eq. (12). Fig. 2 represents the 3D and 2D plots of the singular periodic shape wave solution at the parameter values “t = 1,α = 1,B = 1,C = 1.1,C1 = 1,C2 = 1, n = 1,m = 1, p = −2.13333” for SET-1(Case-2) of Eq. (13). One soliton solution of Eq. (14) for SET-2(Case-1) at the parameter values “t = 1,α = 1,B = 1,C = 0.15,C1 = 1,C2 = 1, n = 1,m = 1, p = −1.13333” is depicted in Fig. 3 in which (I) represents the 3D plot and curve (II) represents the corresponding 2D plot of Eq. (14). In Fig. 4, we have shown the 3D and 2D graphical representations of the singular periodic wave solution of Eq. (14) for SET-2(Case-2) for the parameter values “t = 1,α = 1,B = 1,C = 1.1,C1 = 0,C2 = 1, n = 1,m = 1, p = 0.133333”.6 Author 1, Author 2 / Journal of Applied Nonlinear Dynamics 00(0) (2022) page1–page2 Fig. 1 The king shape soliton solution for t = 1;B = 1;α = 1;C = 0.1;C1 = 1,C2 = 1: (I) represents the 3D plot and (II) represents the 2D plot of Eq. (12). Fig. 2 The singular periodic wave solution for t = 1;α = 1;B = 1;C = 1.1;C1 = 1,C2 = 1: (I) represents the 3D plot and (II) represents the 2D plot of Eq. (13). 5 Conclusions The relatively new \u0010 G′ G′+G+A \u0011 -expansion technique has been successfully utilized to derive the new exact traveling wave solutions to a new reduced form of the (2+1)-dimensional BKP equation. We have used the symbolic mathematical computation program (Mathematica), to show the graphical representation of the solutions. The exact traveling wave solutions are presented in the form of 2-D and 3-D figures. It is noticed that all the solutions obtained are in general form and involving a few parameters. The reported solutions are represented by equations (12), (13), (14) and (14). By assigning the particular values to the parameters, one kink shape soliton solution, two singular periodic shape wave solutions and one soliton solution are presented in Figs. 1 – 4. Exploring the new exact analytical solutions of the new reduced form of the ( 2 + 1)-dimensional generalized BKP equation, we anticipate that the aforementioned technique could be applied to many other NPDES model arising in mathematical physics and engineering problems to obtain new exact wave solutions.Author 1, Author 2 / Journal of Applied Nonlinear Dynamics 00(0) (2022) page1–page2 7 Fig. 3 One soliton wave solution for t = 1,B = 1,α = 1,C = 0.15,C1 = 1,C2 = 1: (I) represents the 3D plot and (II) represents the 2D plot of Eq. (14) . Fig. 4 The singular periodic wave solution for t = 1,B = 1,C = 1.1,α = 1,C1 = 0,C2 = 1: (I) represents the 3D plot and (II) represents the 2D plot of Eq. (14) . Acknowledgments The author would like to thank the anonymous referees and editor for all the apt suggestions and comments which improved both the quality and the clarity of the paper. References [1] K. K. Ali, R. Yilmazer, H. Bulut (2019), Analytical solutions to the coupled boussinesq–burgers equations via sine-gordon expansion method, in: International Conference on Computational Mathematics and Engineering Sciences, Springer, 1111, 233–240. [2] M. A. Akbar, L. Akinyemi, S.-W. Yao, A. Jhangeer, H. Rezazadeh, M. M. Khater, H. Ahmad, M. Inc (2021), Soliton solutions to the boussinesq equation through sine-gordon method and kudryashov method, Results in Physics, 25, 104228. [3] M. R. A. Fahim, P. R. Kundu, M. E. Islam, M. A. Akbar, M. Osman (2022), Wave profile analysis of a couple of (3+ 1)-dimensional nonlinear evolution equations by sine-gordon expansion approach, Journal of Ocean Engineering and Science, 7 (3), 272–279.8 Author 1, Author 2 / Journal of Applied Nonlinear Dynamics 00(0) (2022) page1–page2 [4] H. Rezazadeh, M. Inc, D. Baleanu (2020), New solitary wave solutions for variants of (3+ 1)-dimensional wazwaz-benjamin-bona-mahony equations, Frontiers in Physics, 8, 332. [5] M. Cinar, A. Secer, M. Ozisik, M. Bayram (2022), Derivation of optical solitons of dimensionless fokas-lenells equation with perturbation term using sardar sub-equation method, Optical and Quantum Electronics, 54 (7), 1–13. [6] M. I. Asjad, N. Munawar, T. Muhammad, A. A. Hamoud, H. Emadifar, F. K. Hamasalh, H. Azizi, M. Khademi, et al. (2022), Traveling wave solutions to the boussinesq equation via sardar sub-equation technique, AIMS Mathematics, 7 (6), 11134–11149. [7] H. ur Rehman, A. U. Awan, A. Habib, F. Gamaoun, E. M. T. El Din, A. M. Galal (2022), Solitary wave solutions for a strain wave equation in a microstructured solid, Results in Physics , 39, 105755. [8] N. Ozdemir, H. Esen, A. Secer, M. Bayram (2022), Novel soliton solutions of sasa–satsuma model with local derivative via an analytical technique, Journal of Laser Applications , 34 (2), 022019. [9] X.-F. Yang, Z.-C. Deng, Y. Wei (2015), A riccati-bernoulli sub-ode method for nonlinear partial differential equations and its application, Advances in Difference equations, 2015 (1), 1–17. [10] S. Hassan, M. A. Abdelrahman (2019), A riccati–bernoulli sub-ode method for some nonlinear evolution equations, International Journal of Nonlinear Sciences and Numerical Simulation , 20 (3-4), 303–313. [11] M. Wang, X. Li, J. Zhang (2008), The (G ′/G)-expansion method and travelling wave solutions of nonlinear evolution equations in mathematical physics, Physics Letters A , 372 (4), 417–423. [12] E. M. Zayed, K. A. Gepreel (2009), Some applications of the (G ′/G)-expansion method to non-linear partial differential equations, Applied Mathematics and Computation , 212 (1), 1–13. [13] M. A. Akbar, N. H. M. Ali, E. Zayed (2012), Abundant exact traveling wave solutions of generalized bretherton equation via improved (G′/G)-expansion method, Communications in Theoretical Physics, 57 (2), 173. [14] H. Kim, R. Sakthivel (2012), New exact traveling wave solutions of some nonlinear higher-dimensional physical models, Reports on Mathematical Physics, 70 (1), 39–50. [15] A.-A. Mamun, N. H. M. Shahen, S. N. Ananna, M. Asaduzzaman, et al. (2021), Solitary and periodic wave solutions to the family of new 3d fractional wbbm equations in mathematical physics, Heliyon, 7 (7), e07483. [16] A. Aniqa, J. Ahmad (2022), Soliton solution of fractional sharma-tasso-olever equation via an efficient (G′/G)- expansion method, Ain Shams Engineering Journal , 13 (1), 101528. [17] L.-x. Li, E.-q. Li, M.-l. Wang (2010), The (G ′/G, 1/G)-expansion method and its application to travelling wave solutions of the zakharov equations, Applied Mathematics-A Journal of Chinese Universities , 25 (4), 454–462. [18] E. M. Zayed, S. H. Ibrahim, M. Abdelaziz (2012), Traveling wave solutions of the nonlinear (3+ 1)-dimensional kadomtsev-petviashvili equation using the two variables (G ′/G, 1/G)-expansion method, Journal of Applied Mathematics, 2012, 1–8. [19] M. M. Miah, A. R. Seadawy, H. S. Ali, M. A. Akbar (2020), Abundant closed form wave solutions to some nonlinear evolution equations in mathematical physics, Journal of Ocean Engineering and Science , 5 (3), 269–278. [20] A. Yokus, H. Durur, H. Ahmad, P. Thounthong, Y.-F. Zhang (2020), Construction of exact traveling wave solutions of the bogoyavlenskii equation by (G′/G, 1/G)-expansion and (1/G′)-expansion techniques, Results in Physics, 19, 103409. [21] H. Naher, F. A. Abdullah, M. A. Akbar (2012), New traveling wave solutions of the higher dimensional nonlinear partial differential equation by the exp-function method, Journal of Applied Mathematics , 2012. [22] J. Biazar, Z. Ayati (2012), Exp and modified exp function methods for nonlinear drinfeld–sokolov system, Journal of King Saud University-Science , 24 (4), 315–318. [23] M. Shakeel, N. A. Shah, J. D. Chung, et al. (2022), Application of modified exp-function method for strain wave equation for finding analytical solutions, Ain Shams Engineering Journal , 14 (3), 101883. [24] T. Yi, L. Jun (2021), A modified exp-function method for fractional partial differential equations., Thermal Science, 25 (2), 1237-1241. [25] E. M. Zayed (2011), A note on the modified simple equation method applied to sharma–tasso–olver equation, Applied Mathematics and Computation , 218 (7), 3962–3964. [26] M. Mirzazadeh (2014), Modified simple equation method and its applications to nonlinear partial differential equations, Information Sciences Letters, 3 (1), 1–9. [27] E. Parkes (2010), Observations on the tanh–coth expansion method for finding solutions to nonlinear evolution equations, Applied Mathematics and Computation , 217 (4), 1749–1754. [28] J. Manafian, M. Lakestani, A. Bekir (2016), Comparison between the generalized tanh–coth and the (G ′/G)- expansion methods for solving npdes and nodes, Pramana, 87 (6), 1–14. [29] M. Ali, M. Alquran, O. B. Salman (2022), A variety of new periodic solutions to the damped (2+ 1)- dimensional schrodinger equation via the novel modified rational sine–cosine functions and the extended tanh–coth expansion methods, Results in Physics , 37, 105462.Author 1, Author 2 / Journal of Applied Nonlinear Dynamics 00(0) (2022) page1–page2 9 [30] A. Yoku¸ s, H. Durur (2019), Complex hyperbolic traveling wave solutions of kuramoto-sivashinsky equation using (1/G′)-expansion method for nonlinear dynamic theory, Balıkesir ¨Universitesi Fen Bilimleri Enstit ¨us¨u Dergisi, 21 (2), 590–599. [31] M. Gholami Baladezaei, M. Ghachpazan, A. H. Borzabadi (2020), Extraction of approximate solution for a class of nonlinear optimal control problems using (1/G ′)-expansion technique, Control and Optimization in Applied Mathematics, 5 (2), 65–82. [32] H. Durur, A. Yoku¸ s (2021), Exact solutions of the benney–luke equation via (1/G′)-expansion method, Bilecik ¸ Seyh Edebali¨Universitesi Fen Bilimleri Dergisi , 8 (1), 56–64. [33] C. M. Khalique, A. Biswas (2009), A lie symmetry approach to nonlinear schr¨odinger’s equation with non-kerr law nonlinearity, Communications in Nonlinear Science and Numerical Simulation , 14 (12), 4033–4040. [34] M. Kumar, A. K. Tiwari (2018), Soliton solutions of blmp equation by lie symmetry approach, Computers & Mathematics with Applications, 75 (4), 1434–1442. [35] C. M. Khalique, S. A. Abdallah (2020), Coupled burgers equations governing polydispersive sedimentation; a lie symmetry approach, Results in Physics , 16, 102967. [36] S. Kumar, V. Jadaun, W.-X. Ma (2021), Application of the lie symmetry approach to an extended jimbo– miwa equation in (3+ 1) dimensions, The European Physical Journal Plus , 136 (8), 1–30. [37] I. Ahmed, A. R. Seadawy, D. Lu (2019), Mixed lump-solitons, periodic lump and breather soliton solutions for (2+ 1)-dimensional extended kadomtsev–petviashvili dynamical equation, International Journal of Modern Physics B, 33 (05), 1950019. [38] M. A. Kayum, S. Ara, M. Osman, M. A. Akbar, K. A. Gepreel (2021), Onset of the broad-ranging general stable soliton solutions of nonlinear equations in physics and gas dynamics, Results in Physics , 20, 103762. [39] R. Roy, H. K. Barman, M. N. Islam, M. A. Akbar (2021), Bright-dark wave envelopes of nonlinear regularized- long-wave and riemann wave models in plasma physics, Results in Physics , 30, 104832. [40] A. Tripathy, S. Sahoo (2020), Exact solutions for the ion sound langmuir wave model by using two novel analytical methods, Results in Physics , 19, 103494. [41] S. Khaliq, A. Ullah, S. Ahmad, A. Akg ¨ul, A. Yusuf, T. A. Sulaiman (2022), Some novel analytical solutions of a new extented (2+ 1)-dimensional boussinesq equation using a novel method, Journal of Ocean Engineering and Science, in press. [42] B. Hong, W. Chen, S. Zhang, J. Xub (2019), The G′/(G′ +G+A)-expansion method for two types of nonlinear schr¨odinger equations, J Math Phys , 31 (5), 1155–1156. [43] Mia, Rajib, Miah, M Mamun, Osman, MS (2023), A new implementation of a novel analytical method for finding the analytical solutions of the (2+ 1)-dimensional KP-BBM equation, Heliyon, 9 (5), e15690. [44] L. Kaur, A.-M. Wazwaz (2018), Lump, breather and solitary wave solutions to new reduced form of the generalized bkp equation, International Journal of Numerical Methods for Heat & Fluid Flow , 29 (2), 569– 579. [45] Kara, S ¨umeyra and ¨Unsal, ¨Omer (2022), Analytical solutions to new forms of two nonlinear partial differential equations via two variable expansion method, Partial Differential Equations in Applied Mathematics , 5, 100210.",
      "references": [
        "Analytical solutions to the coupled boussinesq–burgers equations via sine-gordon expansion method",
        "Soliton solutions to the boussinesq equation through sine-gordon method and kudryashov method",
        "Wave profile analysis of a couple of (3+ 1)-dimensional nonlinear evolution equations by sine-gordon expansion approach",
        "New solitary wave solutions for variants of (3+ 1)-dimensional wazwaz-benjamin-bona-mahony equations",
        "Derivation of optical solitons of dimensionless fokas-lenells equation with perturbation term using sardar sub-equation method",
        "Traveling wave solutions to the boussinesq equation via sardar sub-equation technique",
        "Solitary wave solutions for a strain wave equation in a microstructured solid",
        "Novel soliton solutions of sasa–satsuma model with local derivative via an analytical technique",
        "A riccati-bernoulli sub-ode method for nonlinear partial differential equations and its application",
        "A riccati–bernoulli sub-ode method for some nonlinear evolution equations",
        "The (G ′/G)-expansion method and travelling wave solutions of nonlinear evolution equations in mathematical physics",
        "Some applications of the (G ′/G)-expansion method to non-linear partial differential equations",
        "Abundant exact traveling wave solutions of generalized bretherton equation via improved (G′/G)-expansion method",
        "New exact traveling wave solutions of some nonlinear higher-dimensional physical models",
        "Solitary and periodic wave solutions to the family of new 3d fractional wbbm equations in mathematical physics",
        "Soliton solution of fractional sharma-tasso-olever equation via an efficient (G′/G)- expansion method",
        "The (G ′/G, 1/G)-expansion method and its application to travelling wave solutions of the zakharov equations",
        "Traveling wave solutions of the nonlinear (3+ 1)-dimensional kadomtsev-petviashvili equation using the two variables (G ′/G, 1/G)-expansion method",
        "Abundant closed form wave solutions to some nonlinear evolution equations in mathematical physics",
        "Construction of exact traveling wave solutions of the bogoyavlenskii equation by (G′/G, 1/G)-expansion and (1/G′)-expansion techniques",
        "New traveling wave solutions of the higher dimensional nonlinear partial differential equation by the exp-function method",
        "Exp and modified exp function methods for nonlinear drinfeld–sokolov system",
        "Application of modified exp-function method for strain wave equation for finding analytical solutions",
        "A modified exp-function method for fractional partial differential equations.",
        "A note on the modified simple equation method applied to sharma–tasso–olver equation",
        "Modified simple equation method and its applications to nonlinear partial differential equations",
        "Observations on the tanh–coth expansion method for finding solutions to nonlinear evolution equations",
        "Comparison between the generalized tanh–coth and the (G ′/G)- expansion methods for solving npdes and nodes",
        "A variety of new periodic solutions to the damped (2+ 1)- dimensional schrodinger equation via the novel modified rational sine–cosine functions and the extended tanh–coth expansion methods",
        "Complex hyperbolic traveling wave solutions of kuramoto-sivashinsky equation using (1/G′)-expansion method for nonlinear dynamic theory",
        "Extraction of approximate solution for a class of nonlinear optimal control problems using (1/G ′)-expansion technique",
        "Exact solutions of the benney–luke equation via (1/G′)-expansion method",
        "A lie symmetry approach to nonlinear schr¨odinger’s equation with non-kerr law nonlinearity",
        "Soliton solutions of blmp equation by lie symmetry approach",
        "Coupled burgers equations governing polydispersive sedimentation; a lie symmetry approach",
        "Application of the lie symmetry approach to an extended jimbo– miwa equation in (3+ 1) dimensions",
        "Mixed lump-solitons, periodic lump and breather soliton solutions for (2+ 1)-dimensional extended kadomtsev–petviashvili dynamical equation",
        "Onset of the broad-ranging general stable soliton solutions of nonlinear equations in physics and gas dynamics",
        "Bright-dark wave envelopes of nonlinear regularized- long-wave and riemann wave models in plasma physics",
        "Exact solutions for the ion sound langmuir wave model by using two novel analytical methods",
        "Some novel analytical solutions of a new extented (2+ 1)-dimensional boussinesq equation using a novel method",
        "The G′/(G′ +G+A)-expansion method for two types of nonlinear schr¨odinger equations",
        "A new implementation of a novel analytical method for finding the analytical solutions of the (2+ 1)-dimensional KP-BBM equation",
        "Lump, breather and solitary wave solutions to new reduced form of the generalized bkp equation",
        "Analytical solutions to new forms of two nonlinear partial differential equations via two variable expansion method"
      ],
      "meta_data": {
        "arxiv_id": "2401.01594v1",
        "authors": [
          "Rajib Mia"
        ],
        "published_date": "2024-01-03T07:55:22Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Derives new exact traveling-wave solutions for a newly reduced (2+1)-dimensional B-type Kadomtsev–Petviashvili (BKP) equation u_{xxxy}+α(u_y u_x)_x+(u_y+2u_x)_t−(u_{yy}+2u_{xx})=0 obtained from a (3+1)D generalized BKP by setting z=x. Main contribution is the first application of the G′/(G′+G+A)-expansion technique to this BKP form, producing families of closed-form solitary (kink/soliton) and periodic/singular periodic solutions parameterized by auxiliary-ODE constants and wave parameters, with illustrative 2D/3D plots.",
        "methodology": "Applies a traveling-wave reduction ξ=nx+my−pt, u(x,y,t)=w(ξ), reducing the PDE to an ODE. Sets U(ξ)=w′(ξ) and integrates once to obtain n^3 m U″ + α n^2 m U^2 + η U = 0 (η derived from wave parameters). Assumes an ansatz U(ξ)=Σ_{k=0}^N a_k (G′/(G′+G+A))^k with N determined by homogeneous balance (N=2). The auxiliary function G(ξ) satisfies G″ + B G′ + C G + A C = 0. Substitutes the ansatz into the ODE, equates coefficients of powers of G′/(G′+G+A), solves the resulting algebraic system for a_k and η (two parameter sets). Uses Λ=B^2−4C to express solutions in exponential/hyperbolic form (Λ>0) or trigonometric form (Λ<0). Integrates U(ξ) to recover w(ξ) and hence u(x,y,t).",
        "experimental_setup": "No datasets or numerical benchmarks. Validation is symbolic-algebraic: derivations performed and algebraic systems solved (reported as using Mathematica). Qualitative verification via plotting representative solutions (2D/3D surfaces/curves) for selected parameter values for four cases (SET-1/SET-2 with Λ≷0), demonstrating kink/soliton and singular periodic behaviors. No error metrics or comparisons beyond citing prior work that obtained related functional forms via other expansion methods.",
        "limitations": "Solutions are restricted to traveling-wave forms; non-traveling structures (lumps, breathers, multi-soliton interactions) are not addressed. An integration constant is set to zero after integrating the reduced ODE, potentially excluding broader solution families. The auxiliary-ODE/ansatz imposes a specific functional class, so completeness is not guaranteed. Parameter regimes yielding singular periodic solutions may correspond to nonphysical blow-ups; stability and physical admissibility are not analyzed. Limited novelty assessment versus existing methods (e.g., G′/G, (G′/G,1/G))—no systematic comparison of solution families or redundancy. Graphical “validation” is qualitative; no direct substitution checks, conservation laws, or integrability properties are reported.",
        "future_research_directions": "Extend the approach to include nonzero integration constants and investigate whether additional solution families emerge. Explore more general reductions (e.g., oblique/complex wave variables, similarity reductions) and seek non-traveling solutions (lumps, rogue waves, breathers) possibly via Hirota bilinear or Darboux methods. Analyze stability and physical relevance of the derived waves (linear stability, perturbation analysis, numerical time evolution). Compare systematically with other analytic techniques to determine novelty/independence of solutions and characterize parameter regions (Λ, α, n,m,p) leading to regular vs singular waves. Apply the G′/(G′+G+A)-expansion method to other (2+1)D and higher-dimensional NLEEs and to BKP variants with variable coefficients, forcing, or fractional derivatives.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Self-Refine: Iterative Refinement with Self-Feedback",
      "full_text": "arXiv:2308.05355v1  [cs.CV]  10 Aug 2023 TCSloT: T ext Guided 3D Context and Slope A ware Triple Network for Dental Implant Position Prediction 1st Xinquan Y ang College of Computer Science and Software Engineering. Shenzhen University Shenzhen, China xinquanyang99@gmail.com 2nd Jinheng Xie Show Lab National University of Singapore Singapore sierkinhane@gmail.com 3rd Xuechen Li College of Computer Science and Software Engineering. Shenzhen University Shenzhen, China timlee@szu.edu.cn 4th Xuguang Li Department of Stomatology Shenzhen University General Hospital Shenzhen, China lixuguang@szu.edu.cn 5th Linlin Shen College of Computer Science and Software Engineering. Shenzhen University Shenzhen, China llshen@szu.edu.cn 6th Y ongqiang Deng Department of Stomatology Shenzhen University General Hospital Shenzhen, China qiangyongdeng@sina.com Abstract—In implant prosthesis treatment, the surgical guide of implant is used to ensure accurate implantation. However , such design heavily relies on the manual location of the impl ant position. When deep neural network has been proposed to assist the dentist in locating the implant position, most of them take a single slice as input, which do not fully explore 3D contextual information and ignoring the inﬂuence of implan t slope. In this paper , we design a T ext Guided 3D Context and Slope A ware T riple Network (TCSloT) which enables the perception of contextual information from multiple adjace nt slices and awareness of variation of implant slopes. A T extu re V ariation Perception (TVP) module is correspondingly elab orated to process the multiple slices and capture the texture varia tion among slices and a Slope-A ware Loss (SAL) is proposed to dynamically assign varying weights for the regression head . Additionally, we design a conditional text guidance (CTG) m odule to integrate the text condition (i.e., left, middle and righ t) from the CLIP for assisting the implant position prediction. Ext ensive experiments on a dental implant dataset through ﬁve-fold cr oss- validation demonstrated that the proposed TCSloT achieves superior performance than existing methods. Index T erms—Dental Implant, T ransformer , Multi-slice Learn- ing, Multi-module Detection, T ext Guided Detection I. I N T RO D U CT IO N Dental implant is a common surgical procedure in oral and maxillofacial surgery [14], in which the surgical guide plays an important role in precise bone drilling and implant placement [2] [15]. Generally, the design of the surgical gu ide relies on the patient’s panoramic radiographic image, or co ne beam computed tomography (CBCT), image to estimate the implant position. However, traditional surgical guide des ign requires extensive manual work for preoperative planning, (Corresponding author: Linlin Shen.) which signiﬁcantly increases the cost of template fabrica- tion [7]. Besides, the design of the surgical guide heavily depends on the subjective experiences of dentists, is thus q uite inefﬁcient and tedious. In contrast, Artiﬁcial intelligen ce (AI) based systems have demonstrated a strong ability to quickly locate the implant position, which potentially improves th e efﬁciency of surgical guide design. Since 3D neural networks require high computation costs, recent works [5] [16] tend to directly predict implant posit ion on a single 2D slice extracted from the 3D CBCT data. However, though such a manner is relatively efﬁcient, they discard the 3D contextual information, e.g., the relations hip among adjacent slices, potentially leading to worse implan t precision. Consider only one 2D slice like Fig. 1(b), the tex ture of the sparse teeth region and the region to be implanted, share high similarity with the real implant region, which ma y result in the inaccurate prediction of implant position. Wh en multiple slices like Figs. 1(a), (b), and (c) are involved, t he distances of the neighboring teeth at the sparse teeth regio n change faster than the real implant region, and the texture o f the implant will emerge in the lower slice. These contextual information can help to distinguish the sparse teeth and the region with implant, avoiding inaccurate detection. Besid es, as the width of each patient’s alveolar bone is different, th e implant should be tilted with a suitable slope during operat ion (See Fig. 2). The larger the slope of the implant, the greater the displacement of implant positions among involved 2D slices . Hence, contextual information among slices and implant slo pe are required for an accurate operation of dental implant. T o mitigate above problems, TSIPR [19] introduced a two- stream network that uses an implant region detector (IRD)Fig. 1. Comparison of different 2D slices at tooth crown. to locate the most probable implant region to ﬁlter out the error detections generated by the regression network. Impl ant- Former [20] proposed to ﬁt the centerline of implant using a series of prediction results of the 2D slices during inferen ce, which remedies the 3D context loss by associating the result s of independent 2D slices. TCEIP [21] introduced the directi on embedding form CLIP [10] to guide the prediction model where to implant, thus avoids the false alarms. However, bot h the contextual information among adjacent slices and impla nt slope are not studied in these works. In this paper, we design a T ext Guided 3D Context and Slope A ware Triple Network (TCSloT), to integrate the per- ception of contextual information from multiple adjacent s lices and awareness of variation of implant slopes. Speciﬁcally, we construct a triplet of (t− k)th, tth, and (t+k)th slices as input. A T exture V ariation Perception (TVP) module is proposed to process the triplet and capture the texture variation among slices. Besides, a Slope-A ware Loss (SAL) is proposed to dynamically assign different weights for the regression he ad, which takes into account the inﬂuence of different slopes for the offset of implant position. The dynamic weights are calculated based on the implant slope. Additionally, we des ign a Conditional T ext Guidance (CTG) module, which consists of a conditional text interaction and the knowledge alignme nt module [21], to integrate the text condition from the CLIP fo r assisting the implant position prediction. The proposed CT G module enables the TCSloT to perform well in the case of multiple missing teeth. W e conducted extensive experiments on a dental implant dataset [20] and ﬁve-fold cross-validation was performed t o validate our network performance. Main contributions of th is paper are summarized as follows: • T o the best of our knowledge, the proposed TCSloT is the ﬁrst multi-slices based implant position regression network that simultaneously perceives contextual infor- mation and implant slope. • A T exture V ariation Perception (TVP) module and a Slope-A ware Loss (SAL) are devised to integrate the per- ception of contextual information from multiple adjacent slices and information of implant slopes, respectively. • W e design a Conditional T ext Guidance (CTG) module to integrate the text condition into the TCSloT to assist Fig. 2. The distribution of implant slope in the dental impla nt dataset. the implant position prediction. • Extensive experiments show that the proposed TCSloT achieves superior performance than the previous methods. II. M E T H O D S Using tooth crown image to predict the implant position has shown its effectiveness [20]. In this work, we follow this paradigm to train the proposed network. An overview of TCSloT is presented in Fig. 3, which mainly consists of four parts: i) Encoder and Decoder, ii) T exture V ariation Perception Module (TVP), iii) Conditional T ext Guidance (CTG) Module, and iv) Regression Head. After obtaining the predicted coordinates of the implant at the tooth crown, we project them from the tooth crown to tooth root area by the space transformation algorithm [20] to obtain the actua l implant position. Next, we will introduce these modules in detail. A. Encoder and Decoder The encoder of TCSloT can be replaced by any available network. T o make a fair comparison with other detectors, the widely used ResNet [4] is employed. Speciﬁcally, we use three ResNet-50 models with shared weights as the encoder, which enables the prediction network to perceive the textur e variation from different slices. Given a 2D slice of tooth cr own St p ∈ RH× W × C of patient p, we extract the upper k-th slice, the current slice and the lower k-th slice to form a tripletFig. 3. The overview of the proposed TCSloT , which consists o f four parts, i.e., Encoder and Decoder, T exture V ariation P erception Module (TVP), ConditionalT ext Guidance (CTG) Module, and Regression Hea d. (St− k p , St p, St+k p ) as input. k is set as 7 in this work, which is determined by the ablation experiment. A set of feature maps , i.e., {M1 e, M2 e, M3 e}, can be accordingly extracted from the output of ResNet. Each feature map has a spatial and channel dimension R H 4 × W 4 × ˆC . T o ensure ﬁne-grained heatmap regression, we design a de- coder to recover high-resolution features. The decoder con sists of three deconvolution layers. It consecutively upsamples the output feature map MT e of TVP , as high-resolution feature representations, in which the recovered features Md can be extracted. B. T exture V ariation P erception Module (TVP) Given the output features of encoder for different slices, we consider two important characteristics to perceive the texture variations. One is the contextual information amon g slices, which can guide the detection model to discriminate the implant, completed implant or sparse teeth region. As shown in Fig. 1, when multiple slices are concerned, the distances between the neighboring teeth at the sparse teeth region change faster than the implant region, and the textur e of the region with implant appear in the lower slice. The basic semantic information of the current slice is the other important characteristics to help the prediction model to l ocate the implant position. W e thus design a T exture V ariation Perception (TVP) mod- ule to fuse the features of different slices using the cross- attention and residual connection. The architecture of TVP is given in Fig. 3. W e implement the cross-attention module by two independent single-head self-attention modules, wh ich takes the tth slice as the query and the upper (t− k)th or lower (t + k)th slice as the key and value: M21 e = softmax(M2 eM1 e T √ db1 )M1 e, (1) M23 e = softmax(M2 eM3 e T √ db2 )M3 e, (2) where √ db1 and √ db2 are the dimension of M1 e and M3 e, respectively. By this means, features of multiple slices ca n be fused to capture the tendency of texture variation. W e add th e original feature of the current slice to the outputs of the cr oss- attention module {M21 e , M23 e } by the residual connection. It will provide the basic information for the fusion feature to improve the predicting robustness. In the end, we concatena te these two fusion features to generate the output MT e : MT e = concat[M21 e + M2 e, M23 e + M2 e]. (3) C. Conditional T ext Guidance Module (CTG) Clinically, the patient may have multiple missing teeth to be implanted. When there is missing teeth on both sides of alveolar bone, the doctor usually choose one side for treatm ent ﬁrst, such that the patient can use the other side for chewing . Moreover, the width of the anterior alveolar bone is much narrower than that of the bilateral alveolar bone, which nee ds to be considered separately when selecting implants. There - fore, we divide teeth regions into three categories, i.e., l eft, middle, and right and use target region as a prior informatio n to help the network decide the implant position. While such information is not available to the network, it can be integr ated by inputting the text provided by dentists, when using the network for implant position regression. T o this end, we des ign a conditional text guidance (CTG) module, which consists of a conditional text interaction and the knowledge alignment module. W e implement conditional text interaction by two steps: 1) The directional vocabulary, i.e., ’left’, ’middle’, or ’ri ght’, is processed by the CLIP T ext Encoder to obtain a conditional text embedding vt ∈ R1× D. T o interact with the imagefeatures from the encoder, a series of transformation f(·) and g(·) are applied to vt as follow: Vt = g(f(vt)) ∈ RH× W × 1, (4) where f(·) repeats text embedding vt from R1× D to R1× HW and g(·) reshapes it to RH× W × 1. This operation ensures better interaction between image and text in the same feature space . 2) Vt is concatenated with the last layer of decoder to fuse both modal features. By this means, TCSloT is able to generat e direct prediction according to the conditional text. As there is a knowledge gap between the text embed- ding extracted by the CLIP and image features initialized by ImageNet pre-training, we follow [21] to perform the knowledge alignment, which gradually aligns image feature s to the feature space of pre-trained CLIP . The knowledge alignment is formulated as follows: Lalign = |mT e − vi|, (5) where mT e ∈ R1× D is the output feature map of TVP after the attention pooling operation [1] and dimension reduction wi th convolution. vi ∈ R1× D is the image embedding extracted by the CLIP Image Encoder. Using this criteria, the encoder of TCSloT approximates the CLIP image encoder and con- sequently aligns the image features of the encoder with the CLIP text embeddings. D. Slope-Aware Loss (SAL) As the width of each patient’s alveolar bone is different, th e implant should be tilted with a suitable slope during operat ion, which will inﬂuence the variation of implant position at the 2D axial view , i.e., the larger the slope, the greater the displ ace- ment of the implant position. W e use the space transformatio n algorithm [20] to calculate the implant slope: s1 = Np c ∑ Np c i=1 xizi − ∑ Np c i=1 xi × ∑ Np c i=1 zi Np c ∑ Np c i=1 z2 i − ∑ Np c i=1 zi × ∑ Np c i=1 zi , (6) s2 = Np c ∑ Np c i=1 yizi − ∑ Np c i=1 yi × ∑ Np c i=1 zi Np c ∑ Np c i=1 z2 i − ∑ Np c i=1 zi × ∑ Np c i=1 zi , (7) Here s1, s2 is the slope of the 3D spatial line. (xi, y i, z i), i ∈ [1, N p c ] is the coordinate of implant position. Np c is the total number of 2D slices. W e simply add this two slopes together as the ﬁnal slope τ: τ = |s1|+ |s2|. (8) The distribution of implant slope in the dental implant dataset is shown in Fig. 2, from which we can observe that the slope of most implants are small ( τ < 0. 4), while a few are signiﬁcantly large. This phenomenon is consistent with clinical practice that most patients have sufﬁciently wide alveolar bone for dental implant, without the need for tilti ng the implant. Motivated by this observation, we introduce a Slope A ware Loss (SAL) to adopt adaptive weight for each implant, according to its slope. Speciﬁcally, we enforces t he prediction model to pay more attention to the implant with large slope, as these implant positions deviate signiﬁcant ly across slices. As the magnitude of τ is different with that of the total losses, which may disturb the balance between the loss of positive and negative samples and decrease the detection performance [18]. W e follow [11] [12] to normalize τ to ˆτ to keep the sum of total loss unchanged: ˆτ = τ · ∑ Np c i=1 Li h ∑ Np c i=1 τLi h , (9) where Li h denotes the regression loss of implant position i, implemented by the Focal loss [6]. W e re-weight the regressi on loss of each object with ˆτ and all the 2D axial views slices belonging to the same implant share the same ˆτ: LSAL = Np c∑ i=1 ˆτLi h. (10) E. Regression Head The regression head of TCSloT consists of a heatmap head and a local offset head, which is used for predicting the impl ant position. The heatmap head generates an Gaussian heatmap F ∈ [0, 1] W g × H g , where g is the down sampling factor of the prediction and set as 4. Following the standard practice in CenterNet [25], the ground-truth position is transformed i nto the heatmap F using a 2D Gaussian kernel: Fxy = exp(− (x − ˜tx)2 + (y − ˜ty)2 2σ2 ), (11) where (˜tx, ˜ty) is ground-truth annotation in F . σ is an object size-adaptive standard deviation. The heatmap head is opti - mized by the proposed SAL loss. The local offset head computes the discretization error caused by g, which is used to further reﬁne the predicted location. The loss of the local offset Loffset is optimized by L1 loss [3]. The overall training loss of SA-TrdientNet is: Ltotal = LSAL + Loffset + Lalign (12) III. E X P E RIM E N T A. Dataset The dental implant dataset used for evaluation is collected from the Shenzhen University General Hospital (SZUH), and all the implant positions were annotated by three experienc ed dentists. Speciﬁcally, the dataset contains 154 patients, from which 3045 2D slices of tooth crown are selected. All the CBCT data were captured using the KaV o 3D eXami machine, manufactured by Imagine Sciences International LLC. Denti sts ﬁrstly designed the virtual implant based on the CBCT data using the surgical guide design software. Then the implant position can be determined as the center of the virtual impla nt.Fig. 4. V isualization of the attention map and prediction re sult at different slices. The yellow and red circles represent the predicted i mplant position and ground-truth position, respectively . B. Implementation Details Pytorch is used for model training and testing. W e use a batch size of 8, Adam optimizer and a learning rate of 0.0005 for network training. Three data augmentation methods, i.e . random crop, random scale and random ﬂip are employed. The network is trained for 80 epochs and the learning rate is divided by 10 at 40th and 60th epochs, respectively. All the models are trained and tested on the platform of TESLA A100 GPU. For the training of other baseline detectors, MMDetection library and ImageNet pre-training models are used. C. Evaluation Criteria The diameter of the implant is 3.5 ∼ 5mm, and clinically the mean error between the predicted and ideal implant position is required to be less than 1mm, i.e., around 25% of the size of implant. Therefore, AP75 is used as the evaluation criteria. As the average radius of implants is around 20 pixels, a boundin g- box with size 21 × 21 centered at the keypoint is generated. The calculation of AP is deﬁned as follows: P recition = T P T P + F P (13) Recall = T P T P + F N (14) AP = ∫ 1 0 P (r)dr (15) Here TP , FP and FN are the number of correct, false and missed predictions, respectively. P(r) is the PR Curve wher e the recall and precision act as abscissa and ordinate, respe c- tively. Fig. 5. V isual comparison of the TCSloT with or without the CT G module. The yellow and red circles represent the predicted implant p osition and ground-truth position, respectively . The blue ellipses de note false positive detections. From top to bottom, the text condition are left, middle, and right respectively . T ABLE I T H E A B L AT I O N E X P E R I M E N T S O F D I FFE R E N T S A M P L I N G I N T E RV A L S I N T CS L OT. Sampling Intervals k AP75%1 2 3 4 5 6 7 8 9 10 ✓ 18.3± 0.4 ✓ 17.9± 0.6 ✓ 19.1± 0.3 ✓ 18.8± 0.3 ✓ 19.5± 0.4 ✓ 19.7± 0.2 ✓ 20.4± 0.3 ✓ 19.6± 0.5 ✓ 19.3± 0.4 ✓ 18.7± 0.2 D. P erformance Analysis Sensitivity of Different Sampling Intervals. Using dif- ferent sampling intervals k will inﬂuence the extraction of contextual information in TCSloT . T o determine a suitable value of k, we conduct a sensitivity analysis to compare the performance of setting different k. Results are listed in T able I. From the table we can observe that with the increment of k, the network performance increases gradually. When the samplin g interval reaches seven, the network performance begins to decline. The reason for this phenomenon is that the large sampling interval ( k > 7) will cause the drastic variation of teeth texture between slices, which inﬂuences the predicti on of implant position. Therefore, we set k = 7 in this work. Component Ablation. T o demonstrate the effectiveness of the proposed modules, i.e., TVP , CTG, and SAL, we conduct ablation experiments for them. When the TVP moduleFig. 6. The distribution of euclidean distances between the ground truth position and the predictions of TCSloT with or without the SA L loss. is removed, the output of the shared weights backbone is concatenated together and performs a channel reduction via a convolution. When the SAL loss is removed, focal loss is applied for the supervision of heatmap regression. The experimental results are listed in T able II. From the table w e can observe that the proposed components are beneﬁcial for TCSloT , among which the TVP module, the CTG module, and the SAL loss improve the performance by 3.9%, 3.6% and 3.8%, respectively. When combining the TVP or CTG module with the SAL loss, the improvement of performance reaches 6.5% and 7.0%, respectively. When combining all these components, the improvement reaches 9.5%. In Fig. 4, we visualise the attention map and prediction result of TCSloT on a patient who has both implant and missing teeth, to further verify the effectiveness of the TV P module. W e remove the CTG module to ensure no additional information is available. From the ﬁgure we can observe that the TVP module simultaneously perceives the implant and missing teeth region in the third slice, and only perceives the missing teeth region in the other slices. In addition, th e T ABLE II T H E A B L AT I O N E X P E R I M E N T S O F T H E P RO P O S E D C O M P O N E N T S I N T CS L OT. Network TVP CTG SAL AP75% TCSloT 10.9± 0.2 ✓ 14.8± 0.6 ✓ 14.5± 0.3 ✓ 14.7± 0.3 ✓ ✓ 17.4± 0.1 ✓ ✓ 17.9± 0.2 ✓ ✓ 18.3± 0.4 ✓ ✓ ✓ 20.4± 0.3 T ABLE III CO M PA R I S O N W I T H T H E T E X T-G U I D E D DE T E C T I O N ME T H O D S . Network Encoder AP75% F1 Score TransVG ResNet-50 13.2± 0.8 13.1± 0.6 VL TVG 14.1± 0.7 13.9± 0.7 JointNL T 15.3± 0.6 15.1± 0.6 SA-TrindentNet(ours) 20.4± 0.3 20.1± 0.3 prediction results of TCSloT without the CTG module do not generate false alarms in the ﬁrst two slices. These visual re sults indicate that the TVP module enables the SA-TridentNte to include all the information of multiple slices. In Fig. 5, we visualize the detection results of the TCSloT with or without the CTG module. The visual results indicates that the CTG module can greatly help the TCSloT to generate accurate implant position on the patient with multiple miss ing teeth. These visual results demonstrated the effectivenes s of using text condition to assist the implant position predict ion. In Fig. 6, we visualise the euclidean distance between the ground truth position and the predictions of TCSloT with or without the SAL loss, to further demonstrate the effectiven ess of the proposed SAL loss. The euclidean distances are summed in an interval of ﬁve. Smaller the distance, more accurate th e implant position prediction. From the ﬁgure we can observe that the distance of both networks mainly distribute in the range of 0 to 15 pixels. For the TCSloT with the SAL loss, more than 70% predictions are located within 10 pixels from groundtruth position, while for TCSloT without the SAL loss, only about 60% predictions are located within 10 pixels. Considering that the diameter of implant is around 2 0 pixels, the predictions with distance more than 10 pixels ar e meaningless. Therefore, the SAL loss are greatly helpful fo r the prediction of implant position. Comparison with T ext-guided Detection Methods. T o further validate the performance of the proposed CTG module , we also compare TCSloT with other text-guided detction methods, e.g., TransVG, VL TVG and JointNL T . For a fair comparison, the widerly used ResNet-50 is employed as the encoder. Results are listed in T able III. From the table we can observe that their AP75% values are 13.2%, 14.1%, and 15.3%, respectively, which are signiﬁcantly lower than tha t of our proposed TCSloT (20.4%). In terms of F1 score, TCSloT also performs the best. These results demonstrated the effectiveness of the proposed CTG module that uses textT ABLE IV CO M PA R I S O N W I T H OT H E R M A I N S T R E A M D E T E C TO R S . Method Network Encoder AP75% F1 Score CNN-based CenterNet ResNet-50 10.9± 0.2 10.8± 0.1 A TSS 12.1± 0.2 11.9± 0.2 VFNet 11.8± 0.8 11.8± 0.1 RepPoints 11.2± 0.1 11.1± 0.3 ImplantFormer 11.5± 0.3 11.3± 0.3 TCEIP 17.8± 0.3 17.6± 0.2 TCSloT(ours) 20.4± 0.3 20.1± 0.3 MSPENet - 15.4± 0.3 15.2± 0.2 TSIPR 15.7± 0.4 15.6± 0.3 Transformer-based Conditional DETR ResNet-50 12.7± 0.2 12.6± 0.2 Deformable DETR 12.8± 0.1 12.5± 0.1 ImplantFormer V iT -B-ResNet-50 13.7± 0.2 13.6± 0.2 TCSloT(ours) DeiT -B 20.9± 0.2 20.7± 0.3 Swin-T 21.6± 0.2 21.3± 0.4 SegFormer-B3 21.9± 0.5 21.8± 0.2 condition to assist the implant position prediction. Comparison with the Mainstream Detectors. T o further demonstrate the superiority of our method, we compare the location performance of the proposed TCSloT with other detectors. As little useful texture is available around the center of implant, the anchor-based detectors cannot regress impl ant position successfully. Only the CNN-based anchor-free det ec- tors (VFNet [23], A TSS [24], RepPoints [22], CenterNet [25] ), TCEIP [21], MSPENet [19], TSIPR [19]), transformer-based detectors (Conditional DETR [9], Deformable DETR [26], ImplantFormer [20]) are employed for comparison. For a fair comparison, ResNet-50 is employed as the encoder. Results are listed in T able IV. From the table we can observe that text condition can signiﬁcantly help improve the performance of implant posit ion prediction, e.g., TCEIP achieved 17.8% AP , which is 2.1% higher than the best performed CNN-based network - TSIPR. The proposed TCSloT achieves the best AP value - 20.4%, among all benchmarks, which surpasses the TCEIP with a large gap. In addition to ResNet50, we have adopted other advanced encoder (e.g., DeiT [13], Swin Transformer [8], an d SegFormer [17]), and the AP75 value are 20.9%, 21.6% and 21.9%, respectively. These experimental results demonstr ate the effectiveness of TCSloT . IV . C O N CL U S IO N In this work, we develop a ext Guided 3D Context and Slope A ware Triple Network (TCSloT) for CBCT data based implant position prediction, which takes a triple of the upp er, current and lower slices as input. W e use three backbones wit h shared weights to extract feature maps of different slices a nd design a T exture V ariation Perception (TVP) module to fuse them. A conditional text guidance (CTG) module is designed to integrate the text condition into the TCSloT to assist the implant position prediction. Additionally, a Slope-A ware Loss (SAL) is proposed to dynamically assign adaptive weights fo r the regression head, which takes into account the inﬂuence of different slopes on the offset of implant position. Exten sive experiments on a dental implant dataset via the ﬁve-fold cro ss- validation demonstrated that the proposed TCSloT achieves state-of-the-art performance. ACK N OW L E D G M E N T This work was supported by the National Natural Science Foundation of China under Grant 82261138629; Guangdong Basic and Applied Basic Research Foundation under Grant 2023A1515010688 and 2021A1515220072; Shenzhen Munic- ipal Science and T echnology Innovation Council under Grant JCYJ20220531101412030 and JCYJ20220530155811025. RE F E RE N CE S [1] Alexey Dosovitskiy , Lucas Beyer, Alexander Kolesnikov , Dirk W eis- senborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehgha ni, Matthias Minderer, Georg Heigold, Sylvain Gelly , et al. An i mage is worth 16x16 words: Transformers for image recognition at sc ale. arXiv preprint arXiv:2010.11929, 2020. [2] Jordi Gargallo-Albiol, Oscar Salom ´ o-Coll, Naroa Loza no-Carrascal, Hom-Lay W ang, and Federico Hern´ andez-Alfaro. Intra-osse ous heat generation during implant bed preparation with static navi gation: Multi- factor in vitro study . Clinical Oral Implants Research , 32(5):590–597, 2021. [3] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitend ra Malik. Rich feature hierarchies for accurate object detection and sema ntic segmen- tation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 580–587, 2014. [4] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. De ep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 770–778, 2016. [5] Sevda Kurt Bayrakdar, Kaan Orhan, Ibrahim Sevki Bayrakd ar, Elif Bilgir, Matvey Ezhov , Maxim Gusarev , and Eugene Shumilov . A deep learning approach for dental implant planning in cone-beam computed tomography images. BMC Medical Imaging , 21(1):86, 2021. [6] Tsung-Y i Lin, Priya Goyal, Ross Girshick, Kaiming He, an d Piotr Doll´ ar . Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision , pages 2980–2988, 2017. [7] Y un Liu, Zhi-cong Chen, Chun-ho Chu, and Fei-Long Deng. T ransfer learning via artiﬁcial intelligence for guiding implant pl acement in the posterior mandible: an in vitro study . 2021. [8] Ze Liu, Y utong Lin, Y ue Cao, Han Hu, Y ixuan W ei, Zheng Zhan g, Stephen Lin, and Baining Guo. Swin transformer: Hierarchic al vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 10012–10022, 2021. [9] Depu Meng, Xiaokang Chen, Zejia Fan, Gang Zeng, Houqiang Li, Y uhui Y uan, Lei Sun, and Jingdong W ang. Conditional detr for fast t raining convergence. In Proceedings of the IEEE/CVF International Conference on Computer V ision, pages 3651–3660, 2021.[10] Alec Radford, Jong W ook Kim, Chris Hallacy , Aditya Rame sh, Gabriel Goh, Sandhini Agarwal, Girish Sastry , Amanda Askell, Pamel a Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning , pages 8748–8763. PMLR, 2021. [11] Josip ˇSari´ c, Marin Orˇ si´ c, T on´ ci Antunovi´ c, Sacha Vraˇ zi´ c,and Siniˇ sa ˇSegvi´ c. Single level feature-to-feature forecasting wit h deformable convolutions. In P attern Recognition: 41st DAGM German Conference, DAGM GCPR 2019, Dortmund, Germany , September 10–13, 2019, Proceedings 41, pages 189–202. Springer, 2019. [12] Josip Saric, Marin Orsic, T onci Antunovic, Sacha Vrazi c, and Sinisa Segvic. W arp to the future: Joint forecasting of features an d feature motion. In Proceedings of the IEEE/CVF Conference on Computer V ision and P attern Recognition, pages 10648–10657, 2020. [13] Hugo T ouvron, Matthieu Cord, Matthijs Douze, Francisc o Massa, Alexandre Sablayrolles, and Herv´ e J ´ egou. Training data- efﬁcient image transformers & distillation through attention. In International conference on machine learning , pages 10347–10357. PMLR, 2021. [14] Endre V arga Jr, M ´ ark Antal, L ´ aszl ´ o Major, Ram ´ ona Ki scsat´ ari, G ´ abor Braunitzer, and Jozsef Piffk ´ o. Guidance means accuracy: A randomized clinical trial on freehand versus guided dental implantati on. Clinical oral implants research , 31(5):417–430, 2020. [15] R V inci, M Manacorda, R Abundo, AG Lucchina, A Scarano, C Crocetta, L Lo Muzio, EF Gherlone, and F Mastrangelo. Accura cy of edentulous computer-aided implant surgery as compared t o virtual planning: a retrospective multicenter study . Journal of Clinical Medicine, 9(3):774, 2020. [16] Monica Widiasri, Agus Zainal Ariﬁn, Nanik Suciati, Cha stine Fatichah, Eha Renwi Astuti, Rarasmaya Indraswari, Ramadhan Hardani P utra, and Choiru Za’in. Dental-yolo: Alveolar bone and mandibula r canal detection on cone beam computed tomography images for denta l implant planning. IEEE Access , 10:101483–101494, 2022. [17] Enze Xie, W enhai W ang, Zhiding Y u, Anima Anandkumar, Jo se M Alvarez, and Ping Luo. Segformer: Simple and efﬁcient desig n for se- mantic segmentation with transformers. Advances in Neural Information Processing Systems, 34:12077–12090, 2021. [18] Jinrong Y ang, Songtao Liu, Zeming Li, Xiaoping Li, and J ian Sun. Real- time object detection for streaming perception. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 5385–5395, 2022. [19] Xinquan Y ang, Xuguang Li, Xuechen Li, W enting Chen, Lin lin Shen, Xin Li, and Y ongqiang Deng. T wo-stream regression network f or dental implant position prediction. Expert Systems with Applications , 2023. [20] Xinquan Y ang, Xuguang Li, Xuechen Li, Peixi Wu, Linlin S hen, Xin Li, and Y ongqiang Deng. Implantformer: V ision transformer based implant position regression using dental cbct data. arXiv preprint arXiv:2210.16467, 2022. [21] Xinquan Y ang, Jinheng Xie, Xuguang Li, Xuechen Li, Xin L i, Linlin Shen, and Y ongqiang Deng. Tceip: T ext condition embedded re gression network for dental implant position prediction. International Conference on Medical Image Computing and Computer-Assisted Interven tion, 2023. [22] Ze Y ang, Shaohui Liu, Han Hu, Liwei W ang, and Stephen Lin . Rep- points: Point set representation for object detection. In Proceedings of the IEEE/CVF International Conference on Computer V isio n, pages 9657–9666, 2019. [23] Haoyang Zhang, Y ing W ang, Feras Dayoub, and Niko Sunder hauf. V arifocalnet: An iou-aware dense object detector . In Proceedings of the IEEE/CVF Conference on Computer V ision and P attern Recogni tion, pages 8514–8523, 2021. [24] Shifeng Zhang, Cheng Chi, Y ongqiang Y ao, Zhen Lei, and S tan Z Li. Bridging the gap between anchor-based and anchor-free dete ction via adaptive training sample selection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 9759– 9768, 2020. [25] Xingyi Zhou, Dequan W ang, and Philipp Kr¨ ahenb ¨ uhl. Ob jects as points. arXiv preprint arXiv:1904.07850 , 2019. [26] Xizhou Zhu, W eijie Su, Lewei Lu, Bin Li, Xiaogang W ang, a nd Jifeng Dai. Deformable detr: Deformable transformers for end-to- end object detection. arXiv preprint arXiv:2010.04159 , 2020.",
      "references": [
        "An image is worth 16x16 words: Transformers for image recognition at scale.",
        "Intra-osseous heat generation during implant bed preparation with static navigation: Multi-factor in vitro study.",
        "Rich feature hierarchies for accurate object detection and semantic segmentation.",
        "Deep residual learning for image recognition.",
        "A deep learning approach for dental implant planning in cone-beam computed tomography images.",
        "Focal loss for dense object detection.",
        "Transfer learning via artificial intelligence for guiding implant placement in the posterior mandible: an in vitro study.",
        "Swin transformer: Hierarchical vision transformer using shifted windows.",
        "Conditional detr for fast training convergence.",
        "Learning transferable visual models from natural language supervision.",
        "Single level feature-to-feature forecasting with deformable convolutions.",
        "Warp to the future: Joint forecasting of features and feature motion.",
        "Training data-efficient image transformers & distillation through attention.",
        "Guidance means accuracy: A randomized clinical trial on freehand versus guided dental implantation.",
        "Accuracy of edentulous computer-aided implant surgery as compared to virtual planning: a retrospective multicenter study.",
        "Dental-yolo: Alveolar bone and mandibular canal detection on cone beam computed tomography images for dental implant planning.",
        "Segformer: Simple and efficient design for semantic segmentation with transformers.",
        "Real-time object detection for streaming perception.",
        "Two-stream regression network for dental implant position prediction.",
        "Implantformer: Vision transformer based implant position regression using dental cbct data.",
        "Tceip: Text condition embedded regression network for dental implant position prediction.",
        "Rep-points: Point set representation for object detection.",
        "Varifocalnet: An iou-aware dense object detector.",
        "Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection.",
        "Objects as points.",
        "Deformable detr: Deformable transformers for end-to-end object detection."
      ],
      "meta_data": {
        "arxiv_id": "2308.05355v1",
        "authors": [
          "Xinquan Yang",
          "Jinheng Xie",
          "Xuechen Li",
          "Xuguang Li",
          "Linlin Shen",
          "Yongqiang Deng"
        ],
        "published_date": "2023-08-10T05:51:21Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Addresses CBCT-based dental implant position prediction where prior work uses single 2D slices (losing 3D context) and ignores implant slope, leading to false alarms and reduced precision. Proposes TCSloT, a text-guided triple-slice network that (1) fuses contextual information from adjacent axial slices, (2) explicitly accounts for implant slope-induced cross-slice displacement via a slope-aware loss, and (3) uses CLIP-derived directional text (left/middle/right) to disambiguate cases with multiple missing teeth. Achieves state-of-the-art AP75 on a dental implant dataset with 5-fold cross-validation.",
        "methodology": "Input is a triplet of slices (t−k, t, t+k) around the tooth crown. Three shared-weight encoders (typically ResNet-50; also tested DeiT/Swin/SegFormer) extract feature maps for each slice. A Texture Variation Perception (TVP) module performs cross-attention using the current-slice features as query and upper/lower slice features as key/value, adds residual connections with the current slice, and concatenates the two fused outputs. A decoder with deconvolutions upsamples to high resolution. Conditional Text Guidance (CTG) uses CLIP text encoder to embed a directional token (left/middle/right), reshapes/repeats it to a spatial map, concatenates with decoder features, and applies a CLIP knowledge-alignment loss aligning pooled TVP features to CLIP image embeddings. Regression head follows CenterNet-style keypoint detection with a Gaussian heatmap plus an offset head. Slope-Aware Loss (SAL) computes implant slope from 3D centerline points (via prior space transformation), normalizes slope to keep overall loss scale, and reweights heatmap regression (focal loss) so high-slope implants receive more emphasis. Total loss = SAL-weighted heatmap loss + L1 offset loss + alignment loss.",
        "experimental_setup": "Dataset: 154 patients collected at Shenzhen University General Hospital; 3045 2D tooth-crown slices selected from CBCT scans (KaVo 3D eXam). Implant centers annotated by three experienced dentists based on virtual implant planning software. Training: PyTorch, batch size 8, Adam lr=5e-4, 80 epochs with lr drops at 40 and 60, augmentations (random crop/scale/flip), trained on Tesla A100. Evaluation: 5-fold cross-validation. Metric: AP75 (keypoint converted to 21×21 box; implant diameter ~3.5–5 mm ~20 px; clinically target <1 mm). Also reports F1 score. Ablations: sampling interval k (1–10; best at k=7), module ablations for TVP/CTG/SAL. Baselines: CNN anchor-free (CenterNet, ATSS, VFNet, RepPoints), prior implant methods (ImplantFormer, TSIPR, MSPENet, TCEIP), transformer detectors (Conditional/Deformable DETR), and text-guided detectors (TransVG, VLTVG, JointNLG).",
        "limitations": "Relies on selecting/identifying an appropriate tooth-crown slice index t and fixed triplet spacing k; performance degrades for large k due to drastic texture changes. Uses only three slices rather than full 3D volumes, so long-range 3D context is still partially lost. Text guidance assumes a coarse directional label (left/middle/right) is available at inference and may be insufficient for finer localization or complex anatomies. Slope computation depends on a prior space-transformation/centerline estimation method and assumes slope summarization τ=|s1|+|s2| captures relevant 3D orientation; may not model full 3D pose or uncertainty. Dataset is single-center with 154 patients and specific CBCT device/protocol; generalization across scanners, populations, and annotation variability is not established. AP75 values are relatively low overall, suggesting task difficulty and room for improvement; clinical validation in mm after full 3D projection is not fully detailed in the provided text.",
        "future_research_directions": "Extend from triple-slice to variable-length or full-volume 3D/2.5D models with efficient attention to capture broader context while controlling compute. Replace fixed k with adaptive slice selection conditioned on anatomy or predicted slope, or use learned temporal/slice spacing. Improve slope modeling to full 3D pose (incl. angulation and depth) and incorporate uncertainty-aware or curriculum reweighting beyond SAL. Enrich text prompts beyond left/middle/right (tooth number, jaw/arch, anatomical constraints) and explore multimodal prompting with clinician inputs. Evaluate cross-domain generalization with multi-center, multi-scanner datasets and perform prospective clinical studies measuring 3D errors (mm/angle) and downstream surgical-guide outcomes. Integrate automatic tooth/missing-tooth segmentation or landmark detection to reduce reliance on preselected tooth-crown slices and to handle severe artifacts.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Extrapolative Controlled Sequence Generation via Iterative Refinement",
      "full_text": "On correctly assessing the reversibility of the magnetocaloric effect from indirect measurements R. Kiefe,1 R. Almeida,2 J. H. Belo,2 and J. S. Amaral1 1)Departamento de F´ ısica and CICECO, Universidade de Aveiro, 3810-193 Aveiro, Portugal 2)IFIMUP, Departamento de F´ ısica e Astronomia, Faculdade de Ciˆ encias, Universidade do Porto, rua do Campo Alegre s/n, 4169-007 Porto, Portugal (Dated: 8 February 2024) The adiabatic temperature change (∆Tad) of a magnetic refrigerant can be indirectly estimated through field (H) and temperature ( T) dependent magnetization ( M) and specific heat ( Cp) measurements. A direct integration approach for this estimation is frequently reported, which is an approximation to a rigorous mathematical approach. In this work, we propose an iterative method in smallH steps, to estimate ∆Tad from indirect measurements. We show that this approach is able to reproduce the reversibility of the magnetocaloric effect, and provides a more accurate estimation of ∆ Tad, up to 10% when considering a detailed M(H, T) and Cp(H, T) dataset that reproduces the magnetothermal properties of gadolinium, a benchmark room- temperature magnetic refrigerant. Keywords: Magnetocaloric effect, adiabatic temperature change, specific heat, magnetization, indirect mea- surements I. INTRODUCTION The magnetocaloric effect (MCE) gives rise to a tem- perature change in magnetocaloric materials when ex- posed to a magnetic field, serving as the fundamental principle behind magnetic refrigeration. The MCE is a reversible process, and so, under adiabatic conditions, ap- plying and removing an external magnetic field will keep the system at its starting temperature. The thermody- namics describing the MCE is well established, with over a century of research, and its discovery credited to Weiss and Piccard1,2, in 1917. Magnetic refrigeration benefits from a large MCE which will depend on the magnetocaloric material (re- frigerant) used, and it largely dictates the performance of such devices 3–7. The study of magnetic refrigeration hinges on accurately gauging the adiabatic temperature change (∆Tad), either from indirect measurements (the refrigerant’s magnetization and specific heat at different temperatures and external magnetic fields) or by direct measurement of ∆Tad. The accurate direct measurement of ∆ Tad is challenging, requiring dedicated equipment. As an alternative, ∆ Tad can be estimated using a de- tailed magnetization and specific heat dataset, both as a function of temperature and magnetic field. II. MCE - THERMODYNAMICS The total differential of the total entropy of a magne- tocaloric material can be written as 8; dS = CH,p T dT + \u0012∂M ∂T \u0013 H,p dH − αT V dp, (1) CH,p is the heat capacity under constant magnetic field and pressure, M is the magnetization of the material, and αT is the bulk thermal expansion coefficient. In an adiabatic-isobaric process ( dp = 0 and dS = 0), we can write the infinitesimal temperature change due to the MCE as CH,p T dT + \u0012∂M ∂T \u0013 H,p dH = 0 (2) This mathematical description is standard, and eq. 2 is frequently seen throughout literature6,9,10. From here, obtaining ∆Tad diverges into two different branches7,10: ∆Tad = − Z H2 H1 T Cp ∂M ∂T dH (3) ∆Tad ≈ − T Cp,H ∆SM(T)∆H (4) Yet, eq. 2 is a total differential equation, which cannot be rigorously solved with neither eq. 3 nor 4, as ∂M ∂T and Cp are both functions of temperature and magnetic field. Reversing the limits of integration in equation 3 evidently only changes the sign of ∆ Tad, as the integral path is the same. So, the direct use of equation 3 results in a non-reversible ∆ Tad, an incorrect description of the MCE. Smith et al. have reported this inaccuracy, proposing the use of eq. 3, but “numerically integrating in suf- ficiently small increments”, updating T on each subse- quent integral7. Also, Pecharsky and Gschneidner10 have criticized the use of eq. 4, suggesting as an alternative calculating ∆Tad from the isentropic difference between the S(T)Hi and S(T)Hf : ∆ Tad ≈ [T(S)Hf − T(S)Hi ]S. arXiv:2402.04940v1  [cond-mat.mtrl-sci]  7 Feb 20242 A. Correctly assessing the reversible MCE Instead of successive numerical integration, as sug- gested by Smith et al., ∆Tad can be estimated by approx- imating the total differential equation (eq. 1), by taking small steps in magnetic field (δH), to which a small tem- perature change δT is associated. This methodology is equivalent to a finite difference approach and is grounded in the accurate physical description of the MCE, δT (Ti, Hi) = − Ti Cp,Hi ∂M ∂T (Ti, Hi)δH. (5) Then, letting the temperature evolve by iteration; Ti+1 = Ti + δT (Ti, Hi) Hi+1 = Hi + δH, until Hi = Hf , where Hf is the final magnetic field in- tensity desired. The adiabatic temperature change from this method is simply the difference between the final and initial temperatures: ∆ Tad = Tf − T0. To obtain ∆ Tad from M(H, T) and Cp(H, T), these thermophysical properties for a magnetocaloric material are necessary and, for the iterative method (eq. 5), its calculation requires detailed information onM(H, T) and Cp(H, T). In this work, we have considered detailed sim- ulated M(H, T) and Cp(H, T) data that adequately repli- cate the thermophysical properties of gadolinium, the benchmark material for room-temperature magnetic re- frigeration. These were calculated via a hexagonal close packed model lattice of spin 7/2 Ising spins, by Monte Carlo sampling of its Joint Energy and Magnetization dependent Density of States (JDOS) 11. The nearest- neighbor magnetic exchange parameter J was chosen to lead to the experimentally observed Tc value of gadolin- ium, and the value used was≈ 5.3 meV. The total specific heat is then the sum of the magnetic specific heat and the lattice contribution described by the Debye model, with a Debye temperature TD = 169 K 12. Further details on the model, Monte Carlo methodology and comparison with experimental data are available elsewhere13. Figure 1 shows M(H, T) and Cp(H, T) for 5 (out of 102) differ- ent external magnetic fields: [0 , 0.5, 1, 1.5, 2] (T), for 79 temperature values between 80 and 440 K. III. RESULTS Using the iterative method of eq. 5, the adiabatic tem- perature change ∆ Tad was calculated for a field change from 0 to 2 T, using the full dataset of Fig. 1. To verify the convergence, a set of field steps were considered, δH: [0.01, 0.05, 0.025, 0, 001](T). By comparing each ∆Tad as- sociated to a δH, with the ∆Tad obtained using the small- est field step considered ( δH=0.001 T), a maximum rel- ative difference < 0.1% was observed. Also, the ∆ Tad FIG. 1. M(H, T) data (top) and Cp(H, T) data (bottom) for H = 0 : 0 .5 : 2 T, of an Ising spin 7/2 HCP lattice, where the lattice contribution to Cp is obtained from the Debye model with TD = 169 K. The dashed line indicates the critical temperature for the phase transition. from field application and removal describes a reversible process, within a maximum error of < 0.2%. FIG. 2. Top - ∆Tad from field application using M(H, T) and Cp(H, T) data from simulations that replicate Gd (Figure 1), estimated using eq. 5 (green and blue dots) and eq. 3 (red dots). Bottom - Relative difference of the ∆ Tad estimated from eq. 3 and 5, for field application. As shown in Figure 2, the magnitude of ∆Tad from field application, using the iterative method (eq. 5) is closer to the field application estimate using eq. 3. Still, even assuming this best case scenario comparison, the differ- ence is considerable, with a maximum relative difference larger than 10%.3 IV. CONCLUSION The conventional procedure for calculating the adi- abatic temperature change (∆ Tad) from M(H, T) and Cp(H, T) data (eq. 3) is shown to not result in a re- versible process, contrary to the thermodynamic descrip- tion of the system. To correctly estimate the adia- batic temperature change, indirectly from M(H, T) and Cp(H, T) data, preserving the reversibility of the process, we propose approximating the total differential equation by an iterative method of using small steps in magnetic field (eq. 5). From detailed M(H, T) and Cp(H, T) data, with com- parable magnetothermoal properties to gadolinium, we show that estimating ∆ Tad from direct integration (eq. 3) leads to a result that is 10% deviated from the more accurate value obtained through the proposed iterative method (eq. 5). This deviation is maximized near the Curie temperature, which is the temperature of interest, as it typically established the working temperature range for a given magnetic refrigeration device. While the proposed methodology allows for the accu- rate indirect estimate of the ∆ Tad from magnetization and specific heat data, we also encourage the careful use of widely reported approximations. ACKNOWLEDGMENTS This work was developed within the scope of the project CICECO-Aveiro Institute of Mate- rials, UIDB/50011/2020, UIDP/50011/2020 & LA/P/0006/2020, financed by national funds through the FCT/MCTES (PIDDAC), and projects PTDC/EME-TED/3099/2020, UIDP/04968/2020- Program´ atico, UIDB/04968/2020, NECL-NORTE- 010145-FEDER-022096. J.H. Belo acknowledges FCT for contract DL57/2016 reference SFRH-BPD- 87430/2012 and R. Almeida acknowledges FCT for PhD. grant reference 2022.13354.BD. REFERENCES 1P. Weiss and A. Piccard, “Le ph´ enom` ene magn´ etocalorique,” J. Phys. Theor. Appl. 7, 103–109 (1917). 2A. Smith, “Who discovered the magnetocaloric effect?” The Eu- ropean Physical Journal H 38, 507–517 (2013). 3J. Romero G´ omez, R. Ferreiro Garcia, A. De Miguel Catoira, and M. Romero G´ omez, “Magnetocaloric effect: A review of the thermodynamic cycles in magnetic refrigeration,” Renewable and Sustainable Energy Reviews 17, 74–82 (2013). 4B. Yu, M. Liu, P. W. Egolf, and A. Kitanovski, “A review of magnetic refrigerator and heat pump prototypes built before the year 2010,” International Journal of Refrigeration 33, 1029–1060 (2010). 5K. A. Gschneidner, Jr. and V. K. Pecharsky, “Thirty years of near room temperature magnetic cooling: Where we are today and future prospects,” International journal of refrigeration-revue in- ternationale du froid 31, 945–961 (2008). 6V. Franco, J. Bl´ azquez, J. Ipus, J. Law, L. Moreno-Ram´ ırez, and A. Conde, “Magnetocaloric effect: From materials research to refrigeration devices,” Progress in Materials Science 93, 112–232 (2018). 7A. Smith, C. R. Bahl, R. Bjørk, K. Engelbrecht, K. K. Nielsen, and N. Pryds, “Materials challenges for high performance mag- netocaloric refrigeration devices,” Advanced Energy Materials 2, 1288–1318 (2012). 8A. Tishin and Y. Spichkin, The Magnetocaloric Effect and its Applications (CRC Press, 2003). 9A. Tishin, “Magnetocaloric effect: Current situation and future trends,” Journal of Magnetism and Magnetic Materials316, 351– 357 (2007). 10V. K. Pecharsky and J. Gschneidner, K. A., “Magnetocaloric effect from indirect measurements: Magnetization and heat ca- pacity,” Journal of Applied Physics 86, 565–575 (1999). 11J. C. In´ acio, A. L. Ferreira, and J. S. Amaral, “Accurate estimate of the joint density of states via flat scan sampling,” (2022), arXiv:2203.02718 [cond-mat.stat-mech]. 12R. W. Hill, S. J. Collocott, K. A. G. Jr, and F. A. Schmidt, “The heat capacity of high-purity gadolinium from 0.5 to 4 k and the effects of interstitial impurities,” Journal of Physics F: Metal Physics 17, 1867 (1987). 13R. Almeida, S. C. Freitas, C. R. Fernandes, R. Kiefe, J. P. Ara´ ujo, J. S. Amaral, J. O. Ventura, J. H. Belo, and D. J. Silva, “Rotating magnetocaloric effect in polycrystals—harnessing the demagne- tizing effect,” Journal of Physics: Energy 6, 015020 (2024).",
      "references": [
        "Le ph´ enom` ene magn´ etocalorique,",
        "Who discovered the magnetocaloric effect?",
        "Magnetocaloric effect: A review of the thermodynamic cycles in magnetic refrigeration,",
        "A review of magnetic refrigerator and heat pump prototypes built before the year 2010,",
        "Thirty years of near room temperature magnetic cooling: Where we are today and future prospects,",
        "Magnetocaloric effect: From materials research to refrigeration devices,",
        "Materials challenges for high performance mag- netocaloric refrigeration devices,",
        "The Magnetocaloric Effect and its Applications",
        "Magnetocaloric effect: Current situation and future trends,",
        "Magnetocaloric effect from indirect measurements: Magnetization and heat ca- pacity,",
        "Accurate estimate of the joint density of states via flat scan sampling,",
        "The heat capacity of high-purity gadolinium from 0.5 to 4 k and the effects of interstitial impurities,",
        "Rotating magnetocaloric effect in polycrystals—harnessing the demagne- tizing effect,"
      ],
      "meta_data": {
        "arxiv_id": "2402.04940v1",
        "authors": [
          "R. Kiefe",
          "R. Almeida",
          "J. H. Belo",
          "J. S. Amaral"
        ],
        "published_date": "2024-02-07T15:18:07Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Identifies that the common “direct integration” formula used to estimate adiabatic temperature change ΔTad from indirect magnetization M(H,T) and specific heat Cp(H,T) measurements is mathematically inconsistent with the underlying total differential and can yield non‑reversible ΔTad upon field cycling. Proposes a physically consistent iterative/finite‑difference approach in small magnetic-field steps that updates temperature along the integration path, thereby preserving reversibility and improving accuracy. Demonstrates on a Gd-like benchmark dataset that the conventional approach can deviate by >10% (worst near Tc), while the iterative method converges and reproduces reversibility within ~0.2%.",
        "methodology": "Starts from the entropy differential dS = (Cp,H,p/T)dT + (∂M/∂T)H,p dH − αTV dp and considers adiabatic-isobaric conditions (dS=0, dp=0) to obtain Cp/T dT + (∂M/∂T)H dH = 0. Instead of integrating with fixed T and Cp approximations, discretizes the path in small δH steps and computes an incremental temperature change δT(Ti,Hi) = −(Ti/Cp(Ti,Hi))*(∂M/∂T)(Ti,Hi)*δH, then iterates Ti+1=Ti+δT, Hi+1=Hi+δH until Hf. Uses numerical differentiation of M vs T at fixed H (implied) and interpolation across a detailed M(H,T), Cp(H,T) grid. Compares against the widely used direct path integral ΔTad = −∫(T/Cp)(∂M/∂T)dH with T and Cp treated as independent of the evolving temperature (the source of inconsistency).",
        "experimental_setup": "Uses simulated, high-resolution thermophysical property grids designed to reproduce gadolinium (Gd) magnetothermal behavior (benchmark room-temperature refrigerant). Magnetization and magnetic specific heat are obtained from a hexagonal close-packed lattice model of spin-7/2 Ising spins via Monte Carlo sampling of the joint energy–magnetization dependent density of states (JDOS). Exchange parameter J≈5.3 meV is tuned to match Gd’s experimental Curie temperature Tc. Total Cp(H,T) = magnetic Cp + lattice contribution from a Debye model with Debye temperature TD=169 K. Data include 79 temperatures spanning 80–440 K and 102 magnetic-field values between 0 and 2 T (figures display a subset H = 0, 0.5, 1, 1.5, 2 T). Evaluates ΔTad for a 0→2 T field change; tests convergence using δH ∈ {0.05, 0.025, 0.01, 0.001} T and uses δH=0.001 T as reference. Validates reversibility by comparing ΔTad for field application vs removal; reports maximum mismatch <0.2%. Quantifies error of conventional method relative to iterative method, observing >10% maximum relative deviation (largest near Tc).",
        "limitations": "Demonstration is based on simulated Gd-like data rather than experimental datasets; real measurements may have noise, hysteresis, demagnetization effects, and limited field/temperature resolution that can affect numerical derivatives (∂M/∂T) and interpolation. The method requires detailed Cp(H,T) and M(H,T) over the full path; sparse data may reduce accuracy or require regularization. Pressure/volume term is neglected (assumes dp=0 and effectively ignores magnetoelastic contributions beyond Cp and M). The comparison focuses on a second-order-like transition (Gd); behavior near first-order transitions with hysteresis/metastability may need additional treatment to define reversibility and path dependence. No full uncertainty propagation or computational-cost analysis is provided.",
        "future_research_directions": "Apply the iterative finite-step method to experimental M(H,T) and Cp(H,T) datasets, including uncertainty propagation and robustness to noise via smoothing/regularized differentiation. Extend to materials with first-order transitions and hysteresis by incorporating path dependence, latent heat, and non-equilibrium effects, and by defining practical criteria for reversible vs irreversible components. Incorporate demagnetizing-field corrections (internal field), anisotropy/rotating MCE, and magnetoelastic coupling (pressure/strain terms) to broaden applicability. Develop adaptive step-size schemes in H (and possibly T) to balance accuracy and data sparsity; provide open-source implementations and recommended measurement grids. Compare against alternative rigorous approaches (e.g., isentropic S(T,H) reconstruction and T(S) mapping) and benchmark across multiple refrigerants and device-relevant operating cycles.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Full-Atom Protein Pocket Design via Iterative Refinement",
      "full_text": "HiRIS: an Airborne Sonar Sensor with a1024 Channel Microphone Array forIn-Air Acoustic Imaging Preprint, compiled February 21, 2024 Dennis Laurijssen ID 1,2∗ , Walter DaemsID 1,2, and Jan SteckelID 1,2 1Cosys-Lab, Faculty of Applied Engineering, University of Antwerp, Antwerp, Belgium 2Flanders Make Strategic Research Centre, Lommel, Belgium Abstract Airborne 3D imaging using ultrasound is a promising sensing modality for robotic applications in harsh environments. Over the last decade, several high-performance systems have been proposed in the literature. Most of these sensors use a reduced aperture microphone array, leading to artifacts in the resulting acoustic images. This paper presents a novel in-air ultrasound sensor that incorporates 1024 microphones, in a 32-by- 32 uniform rectangular array, in combination with a distributed embedded hardware design to perform the data acquisition. Using a broadband Minimum Variance Distortionless Response (MVDR) beamformer with Forward-Backward Spatial Smoothing (FB-SS), the sensor is able to create both 2D and 3D ultrasound images of the full-frontal hemisphere with high angular accuracy with up to 70dB main lobe to side lobe ratio. This paper describes both the hardware infrastructure needed to obtain such highly detailed acoustical images, as well as the signal processing chain needed to convert the raw acoustic data into said images. Utilizing this novel high-resolution ultrasound imaging sensor, we wish to investigate the limits of both passive and active airborne ultrasound sensing by utilizing this virtually artifact-free imaging modality. 1 I ntroduction While microphone arrays have been around for more than 50 years [1], the landscape of microphone array sensors and its technology have advanced tremendously in the last two decades with the rise of MEMS (micro-electro-mechanical system) tech- nology. Furthermore, the last decade has given rise to many novel 3D in-air ultrasound sensors which allow the formation of acoustic images in 3D. These sensors hold great promise for robotic applications in harsh environments, as ultrasound signals are minimally affected by medium distortions such as dust, fog and water spray. However, the sensors developed in the past typically use a reduced aperture due to cost and complexity limitations, with microphone counts typically ranging from 1 to 64. These reduced apertures inevitably cause either artifacts in the resulting 3D images, or images with a limited dynamic range and spatial resolution. Ultrasound signals often exhibit a large Helmholtz number in relationship to the environments where they are applied, imply- ing that the reflected energy impinging on the sensor is mainly specular in nature. On the other hand, diffraction echoes should arise from acoustic theory [2], but so far these echos have been mostly neglected due to their low intensity. In order to assess the relative importance of these echoes in real-world environ- ments, as well as to investigate what the virtual upper limit is of ultrasound sensing in real-world environments, a sensor with a high spatial resolution and high dynamic range is necessary. This paper tries to address the need for a sensor with high spatial resolution and dynamic range, and introduces a dense, large aperture in-air ultrasound microphone array which should pro- vide these high spatial resolutions, dynamic ranges and signal to noise ratios. The system is consists of 1024 synchronously sampled microphones, increasing the number of microphones of our previously developed systems by a factor of 32 [3, 4, 5]. This sharp rise in microphone channel count is achieved by leveraging a distributed hardware architecture, which is built upon a decade of ultrasound sensor development. In this pa- per, we demonstrate a successful implementation of this novel acoustic sensor, and demonstrate its functionality through both simulation and real-world measurements. In order for the readers to accurately follow the developments, we encourage them to get familiar with our previous work in which we describe in detail the development of a single 32- channel microphone array [ 3, 6], as the sensor in this paper consists of a distributed version of that single 32-channel module. However, this paper still stands on its own, allowing the reader to follow the development of the data-acquisition methods and signal processing techniques and understand the performance analysis of the system where we compare it with our previously developed 32-channel microphone array [3, 6]. In the pages that follow, we will touch on the design choices that were made to achieve the hardware architecture of the devel- oped ultrasound sensor unit that we named the High Resolution Imaging Sonar (HiRIS) sensor, together with a more detailed description of the implementation. In the subsequent section, the data acquisition and signal processing are described followed by a section on the experimental setup and its results. In the final section, we will present the conclusions of the proposed system and its envisioned applications as future work. 2 H ardware Architecture Achieving the envisioned objective of constructing a synchro- nized ultrasound sensor array featuring 1024 microphone chan- nels, coupled with a versatile yet timely data transfer interface, poses a nontrivial challenge. It requires considering several trade-offs in different design aspects, such as the choice of com- ponents and their associated costs, design time influenced by arXiv:2402.13110v1  [eess.SP]  20 Feb 2024Preprint – HiRIS: an Airborne Sonar Sensor with a1024 Channel Microphone Array forIn-Air Acoustic Imaging 2 familiarity with a platform, as well as the time allocated for implementation and testing. This section aims to delve into the deliberations behind these design choices, exploring considera- tions related to component types, cost implications, design famil- iarity impact on time, and the overall implementation process. Additionally, we will introduce the selected implementation and provide an overview of the proposed system. The hardware design of the HiRIS sensor is a highly complex one, which warrants its own extensive description as the devil is in the details. Indeed, the overall system consists of 1024 mi- crophones, 33 microcontrollers, 4963 electronical components, distributed over 2 PCBs, and over 127m of PCB traces. The road to a successful implementation of such a system is riddled with pitfalls, which we aim to clarify in the subsequent sections. 2.1 Design Choices Over the last decade, the embedded products market has witnessed a notable surge in diversity, propelled by swift technological advancements. The integration of highly capable and feature-rich (ARM) microcontrollers, along with Field- Programmable Gate Arrays (FPGAs) and System on Chips (SoCs), has been instrumental in enhancing the capabilities of embedded sensor systems. The emergence and widespread adoption of Single Board Computers (SBCs), coupled with the proliferation of Internet of Things (IoT) devices, have been spurred by the demand for technological progress in the era of Industry 4.0. Concurrently, the growing community of online hobbyists in the domain of embedded electronics hardware has contributed to the development of tools and libraries, facilitating the rapid creation of embedded platforms. While the three aforementioned types of embedded de- vices, being FPGAs [ 4, 3, 7, 8, 9], SoCs [ 10, 11, 12] and ARM microcontrollers [13, 14, 5, 15, 16], have been used for the construction of high-resolution ultrasound sensing arrays, each of these device types have their distinct advantages and disadvantages. Field-Programmable Gate Arrays (FPGAs) o ffer notable advantages in terms of flexibility and customization. Their field-configurable nature, in combination with very high GPIO pin counts, allows for their rapid adaptation to diverse tasks, especially in real-time applications with very tight timing constraints. However, the complexity of FPGA design, coupled with the relatively higher power consumption and component cost can be considered drawbacks. Indeed, developing complex hardware designs in FPGAs is complicated due to the need for tight timing closures, in order to yield stable data-acquisition systems. Systems-on-a-Chip (SoCs) integrate multiple components, pro- cessing cores and peripherals onto a single chip, streamlining the design and reducing the need for external components. This leads to space and power e fficiency. While the large amount of diverse peripherals on the SoC is attractive, the amount of customization options are significantly more limited when com- pared to FPGAs. Therefore, complexity in the envisioned design may lead to significant challenges during the development. On the other hand, timing closure is guaranteed by design, leading to far less potential for race conditions compared to FPGA-based designs. ARM microcontrollers excel in power e fficiency and simplic- ity, due to their standardized architecture and interface design. The low cost of ownership, coupled to wide industry adoption make them accessible for a wide range of applications, from automotive, over consumer goods, and indeed, to high-speed data-acquisition. This high degree of standardisation leads to a reduced customization potential when compared to FPGAs, or the integrated capabilities of SoCs, limiting their suitability for certain high-performance or specialized tasks. Despite the apparent drawbacks of ARM-based microcontroller systems, we deemed this to be the most promising candidate for the development of the hardware architecture for HiRIS. While the other two options (being SoCs and FPGAs) are certainly viable options for implementing such a systems, we chose for an ARM-based architecture, because of the fact that a) the pe- ripherals on the chosen ARM platform are ideally suited for our intended application, b) a distributed architecture is more error robust than a single monolithic implementation, and c) our group is well versed in the development of ARM-based systems, which is a non-neglectable reason for choosing a particular approach. 2.2 Distributed Architecture When considering the design of complex hardware systems, it often pays o ff to approach the implementation using a distributed architecture. Indeed, when using a distributed architecture, robustness increases due to the lack of single point-of-failures. In the case of the embedded hardware design of HiRIS, it can be beneficial to split up the hardware over multiple printed circuit boards (PCBs) that are tied together using one or multiple appropriate connectors. The hardware components can be grouped by functionality and can hence be isolated in the design process, which in turn can have advantages during the implementation and testing phase. This is especially important for testing individual boards with high component counts, as this distributed approach allows them to be tested without inducing dangerous voltages or currents to other parts of the device. Furthermore, sections of the design can easily be redesigned if deemed necessary after testing (i.e., it facilitates an iterative design approach), without having to reassemble the non-faulty parts of the system. This modular approach also has the added benefit of being able to make use of an extra spatial dimension in the hardware design by connecting multiple PCBs on top of each other, reducing the surface of the total design to its volume. When designing acoustic array sensors of the proposed complexity encountered in HiRIS, we often separate the microphones and some of their essential peripherals to a front-end PCB, and place the rest of the electronic components to a so called back-end PCB. As a beneficial side e ffect, this creates the potential for leaving front-facing side of the front-end PCB component-less, which is essential for eliminating distorting multipath e ffects in the acoustic reception pathways. Another advantage of the distributed architecture can be found in reusing known, verified and tested schematic and component layouts, used extensively in previous designs (i.e. the so-called battle-tested designs). By reusing parts of bothPreprint – HiRIS: an Airborne Sonar Sensor with a1024 Channel Microphone Array forIn-Air Acoustic Imaging 3 32x Subordinate Node  Primary Node  External SDRAM (IS42S16320D) ARM-M4 Microcontroller (STM32F429) High-Speed USB PHY (USB3300) Micro-USB Connector Linear Voltage Regulator Linear Voltage Regulator Micro-USB Connector Clock x16 GPIO Trigger 32x Trigger output ... ... USB - UART Bridge (FT230X) Full-Duplex  RS-485 Driver (SN65HVD77DR) BNC Output 32x Clock Output Clock Buﬀer Driver (CDCLVC1108PWR) x4 Clock ARM-M4 Microcontroller (STM32F429) Ext. I/O Clock x16 Stereo PDM  Mic Data 32x 32-element array Linear Voltage Regulator 16x VReg 32-Element Microphone Array (SPH0641LU4H-1) ...... +5V Front-end Back-end +3.3V +3.3V +3.3V b) c)a) d d d d e e f Figure 1: Overview of the HiRIS hardware architecture. Panel a) show a schematic representation of the system architecture, distinguishing between the front-end and back-end PCBs. On the front-end, there are 32 groups of 32 microphones (each arranged in an 8x4 grid). The back-end PCB has one primary node which does clock distribution, triggering and synchronization of the subordinate nodes. The subordinate nodes each sample a 32-channel microphone group, using 16 IO pins with the microphones operating in dual-channel stereo mode. Panel b) shows the assembled front-end PCB measuring 180 mm by 170mm. Box e) indicates a single 8x4 group of microphones. Boxes labeled d) indicate the interconnect connectors between the front-end and back-end PCBs. Panel c) shows the assembled back-end PCB, which has the same dimensions as the front-end PCB. Panel e) indicates the primary node, and box f) shows a single subordinate node. the schematics and component layouts from previously built hardware, these parts can be distilled into design blocks, which then can be combined in the larger overarching design. These design blocks allowed us to quickly create a distributed hard- ware architecture of 32 microphone nodes by 33 microcontroller nodes, where every 32-element microphone node on the front end PCB is connected with an ARM microcontroller node with its peripherals on the back-end PCB. To orchestrate the 32 microcontroller nodes, an additional primary node was added on the back end. Employing this distributed design method of reusing existing design blocks has proved to be a highly productive and cost-efficient design methodology. 2.3 Front end The front-end board mainly incorporates 1024 Knowles SPH0641LU4H-1 MEMS microphones, sixteen AP2112K-3.3V linear voltage regulators that convert +5V to +3.3V that is used as the power source for the sensing elements. While each of the microphones typically only consumes 850 µA, we have provided for ample headroom in the power budget of the voltage regulators, which is why these regulators each power a group of 64 microphones. The aforementioned MEMS microphones are configured in a 32-by-32 uniform rectangular array with regular grid spacing of 3.9mm. When designing phased arrays, the impact of its configuration on the resulting frequency dependent steerable beam directivity pattern is determined by the sensing element spacing d. We can apply the Nyquist theorem to the spatial domain [17] and determine that grating lobes [ 18] are introduced into the directivity pattern, as spatial aliases, when the sensing element spacing d ⩾ λ 2 . Rearranging this simple equation, we can calculate the maximum frequency fmax for which this array geometry can be used for beam steering without spatial aliases: d = λ 2 = v 2 f ⇔ f = v 2d assuming the speed of sound in air v is approximately 343m/s, fmax is found to be 43.974kHz. Besides the small form factor, low power consumption and a frequency response curve reaching far into the ultrasonic spectrum [4], the key advantage of the SPH0641LU4H-1 micro- phones is their built-in Σ∆ ADC (analog-to-digital converter) that converts the captured acoustic wavefront into a PDM (pulse density modulation) 1-bit signal. The aforementioned Σ∆ ADC uses an external clock signal of 4.5MHz for sampling the analog signals to their digital 1-bit representation. Given the relatively large wavelength of an electrical wave of 4.5MHz in copper being approximately 44.4m, we can distribute these clock signals in phase to all 1024 microphones, thus allowing simultaneous sampling all of the microphones distributed over the PCB. This synchronous sampling is important for the subsequent operation of the sensor in an array fashion, which often leads to significant complications in RADAR-based sensing applications [19, 20, 21]. Using microphones with a built-in 1-bit ADC has as a major advantage a significant reduction in board complexity. Indeed, if microphones with an analog voltage output were to be used, each of these microphone signals would need amplification and a dedicated ADC chip, which adds a significant amount of complexity (as demonstrated in our earlier designs [22, 23]). Interfacing with 1-bit signals can be easily done using a wide GPIO register on a microcontroller. Further reduction of the necessary GPIO lines can be realizedPreprint – HiRIS: an Airborne Sonar Sensor with a1024 Channel Microphone Array forIn-Air Acoustic Imaging 4 by utilizing the stereo-capability of the used PDM microphones. When using this implemented feature, one microphone will deliver its data on the rising edge of the clock signal where the other will deliver it on the falling edge. The latching of the data will occur on the opposite edges of the clock signal of the microphones. This stereo setup halves the number of data lines that are required from 1024 (without using the stereo feature) to 512 (when using the stereo feature). It should be noted that the while the induced phase di fferences of sampling on both the rising and falling edge of the data sampling is 180◦, this equates to 10µs, which equates to a negligible phase di fference when compared to the targeted acoustic signals between 25kHz and 100kHz. In addition to the aforementioned voltage regulators, mi- crophones and various passive components e.g. resistors and decoupling capacitors, eight high density FX10A-120P connectors can also be found on this PCB to connect the power, multiple synchronous clock lines and data lines to the back-end PCB of this design. These connectors ensure a high-fidelity link of the digital signals from the front end to the back end, ensuring robust operation of the sensor during field-trials. 2.4 Back end The back end of the HiRIS sensor can be split up in a primary node and 32 subordinate nodes. The latter are 32 identical design blocks with the STM32F429 ARM Cortex M4 microcon- troller at its core, in combination with an IS42S16320D external SDRAM memory of 64 MB, a USB3300 high-speed USB PHY , an AP2112K-3.3V linear voltage regulator, a micro USB connector and passive connectors for decoupling and impedance matching of the transmission lines. These nodes each have 16 GPIO pins respectively connected to 32 microphones in their stereo configuration, together with a separate GPIO pin acting as clock signal. Based on the rising and falling edges of the clock signal, the 16 GPIO pins are synchronously sampled and temporarily stored in memory until the next clock period. Upon receiving a trigger signal from the primary node, the subordinate nodes will store these temporarily into the SDRAM until a predetermined number of samples have been recorded. Given the 64MB of SDRAM memory capacity that every subordinate node has and a data stream of 18 MB/s per 32 microphones that are connected to it, a single continuous measurement of approximately 3.55s can be recorded. The primary node, that also has a STM32F429 ARM Cortex M4 microcontroller at its core, uses a single timer periph- eral that generates 4 synchronous square wave signal outputs at 4.5MHz that in turn each are connected to a Texas Instruments CDCLVC1108PWR low jitter, 1:8 LVCMOS fan-out clock buffer IC, effectively yielding 32 synchronous 4.5 MHz clock signals. These clock signals are distributed to the 32 subordinate node on the back end and their respective microphones on the front-end PCB. This ensures on-the-clock-true sampling of all microphones, which is important for the subsequent processing pipeline. While every subordinate node is equipped with a bi-directional high speed USB interface for establishing a connection with a computer, this interface does not su ffice to initiate a synchronous start of a measurement for all nodes, due to the unpredictability of the timing of data transfer over 32 separate USB channels. In order to establish a synchronous trigger to start a single measurement, the primary node uses 16 GPIO pins that are connected to the 32 subordinate nodes to initiate measurements on the latter by synchronously asserting a short pulse on these pins. Since the primary node does need a communication interface to a computer, it does not need to transfer large quantities of data in a short amount of time. Therefore an FTDI FT231X USB-to-UART bridge has been used in the primary node to establish a low-speed but reliable interface. Besides a USB connection to the primary node for initi- ating measurements, an external TTL-input can be used to trigger measurements, which allows for easy integration of the HiRIS sensor in measurement pipelines. For increased robust- ness, e.g. long cable lengths or noisy environments, the option of using differential signaling for external I /O was chosen by incorporating a SN65HVD77DR RS-485 driver into the design. Since this is a full-duplex interface, a pulse can also be gener- ated to trigger external devices along with the subordinate nodes. While the HiRIS is designed as a passive measurement device, a BNC connector was also fitted to the back end that is connected to the analog DAC-output of the primary node. The DAC peripheral can be triggered simultaneously with the subordinate nodes where it will generate an analog signal on its output based on a sequence that was either pre-defined in the firmware or uploaded to the primary node through its USB interface. This enables us to use this sensor as a high-channel pulse-echo sonar device when combined with an external amplifier and ultrasound transducer, similar to the sensors described in our earlier work [3, 4, 5, 14, 22, 23]. 2.5 Physical Realisation of the HiRIS Sensor The proposed 1024-microphone ultrasound array sensor, referred to as the HiRIS sensor (High-Resolution Imaging Sonar), comprising of the front and back end PCBs measures 180mm by 170mm by 20mm (including its protruding micro USB connectors). The two boards combined feature nearly 5000 components, 127m of PCB traces and costs e3962 to produce. Once powered up and operational, the overall sensor system consumes approximately 65W. This provided an unanticipated heat output of the sensor, which called for an adequate cooling solution. Indeed, when measuring the surface temperature of the PCBs, we noticed areas that reached up to 80◦C in a room with an ambient temperature of approximately 22 ◦C. While electronics are often rated to cope with higher temperatures it was deemed that including a method for passively cooling the sensor would be beneficial for its lifespan and the safety of its users. As a cooling solution, a copper slab of 250 mm by 170mm by 5mm was used between the front end and back end with a thermal interface on both sides for providing an optimal surface contact between the components and copper. To further increase the heat dissipation of the cooling solution, extra heat sinks were bolted onto the protruding ends of the copper slab. The effect of this cooling solution yielded a temperature decrease of 35◦C with maximum surface temperatures reaching up to 45◦C. As mentioned in the previous subsection, the HiRIS sen-Preprint – HiRIS: an Airborne Sonar Sensor with a1024 Channel Microphone Array forIn-Air Acoustic Imaging 5 Figure 2: Coordinate system in relationship to the HiRIS sensor, showing the X, Y and Z axis, and the azimuth angle θ and elevation angle ϕ. sor can be expanded by connecting external devices through its external I/O or BNC connections but could also be further expanded with an additional PCB that stacks on the backside of the back end. This envisioned additional PCB would incorporate multiple USB3 hub ICs in order to reduce the amount of cable clutter. Another feature that would be integrated is a JTAG SWD programmer in combination with a multiplexer to alleviate the tedious work of plugging and unplugging the programmer when pushing firmware changes to the microcontrollers on the back end. 3 D ata acquisition andProcessing Chain In this section, we will detail the process of initiating and cap- turing a set of waveform data from the microphone array, and the subsequent processing using a bank of adaptive spatial filters (MVDR beamforming) for 3D image generation. 3.1 DAQ The HiRIS sensor comprises of 33 microcontrollers (1 primary node and 32 subordinate nodes), each connected over a USB2.0 connection to a host PC. To aggregate all the USB connections, a chain of USB3 hubs is used, which aggregates all the USB connections to a single USB3.0 connection. The USB protocol is a CDC Virtual Com Port (VCP) emulation [ 24], each initializing a virtual serial port on the host PC. A custom Python script using the Multiprocessing API [ 25], looks for specific serial ports connected to the system with specific Vendor ID and Product ID combinations and opens all these ports, which allows for bidirectional communication with all the sensor nodes. As stated before, the primary-subordinate architecture of our sensor implies that the single primary node of the HiRIS sensor listens to a command originating from a controlling PC over the VCP. In turn, the primary node asserts a trigger pulse to the 32 subordinate nodes, which each perform a measurement of a set duration (typically 70 ms). This data is then sent by each subordinate node over its serial port to the host PC, which combines all the data and stores them in a binary format on a mass storage device for subsequent processing. A single measurement of 70 ms is around 1.25 MB of data per subordinate node, equating to 40MB for a single measurement. Reworking this implies a datastream of 1GB per second when measuring with a 100 percent duty cycle. In practice, a duty cycle of 20 percent is more realistic, leading to a datarate of 200MB per second. 3.2 Image Formation using MVDR To process the massive amount of microphone data into a spacial spectrum, we follow an approach similar to the one outlined in [14]. The microphone signals are PDM modulated using single-bit Σ∆ modulation. In order to demodulate these sig- nals, we pass them through a low-pass filter, and decimate the resulting signal: sM,i(t) = hPDM ∗sPDM,i(t) The cutoff frequency of hPDM equals to 100kHz, which is well before the rise of the colored quantization noise, induced by a noise shaping on the MEMS microphones. This results in the i-th microphone signal sM,i, of which there are 1024 in the case of the HiRIS sensor. These 1024 microphone signals are all passed through a matched filter. The base signal sb(t) is the emitted signal in the case of an active sonar measurement (where the sensor emits a signal), or a Dirac delta function in case of a passive measurement (where the sensor listens to environmental signals: sMF,i(t) = F−1 \u0014 F[sb(t)]∗·F[sMF,i(t)] \u0015 Next, these signals are converted into a time-frequency distribu- tion using the short-time Fourier Transform (STFT), yielding a spectrum S MF,i(t, f ) for each i-th microphone signal. We choose a certain operating frequency f , in this case 42 kHz, and select the column of the STFT according to that frequency. This yields a complex signal xi(t) for each microphone. These signals are all concatenated into an observation matrix called X( f,t): X( f,t) = h x1( f,t) x2( f,t) ... xk( f,t) i For the spatial filtering, we apply a function on X( f,t), depend- ing on the kind of beamforming we want to achieve. We limit the scope of this paper to Minimum Variance Distortionless Response beamforming (MVDR) [6, 14, 26]. We use forward- backward spatial smoothing with diagonal loading in order to overcome the limitations posed by the sonar sensing modality in that only a single snapshot can be used to perform spatial filtering. Indeed, in many applications such as radar or mobile communications, multiple snapshots are available. However, due to the limited speed of sound, this is not possible in the HiRIS application [6, 14]. Therefore, we apply spatial smoothing by selecting sub-arrays of size 28x28, yielding 25 virtual snapshots from the 25 subarrays formed during the spatial smoothing pro- cess. From this, we build the sample covariance matrix Rb, and then calculate the weights of the MVDR beamformer: wMVDR(ψ) = R−1 b ·A(ψ) A(ψ)H ·R−1 b ·A(ψ)Preprint – HiRIS: an Airborne Sonar Sensor with a1024 Channel Microphone Array forIn-Air Acoustic Imaging 6 where A(ψ) is the array manifold matrix for spatial direction ψ given the subarray geometry and frequency of operation. Using these MVDR-weights wMVDR we can then apply the spatial filter on the observation matrix X( f,t) by complex multiplication, yielding a beamformed signal xψ(t, f ) in direction ψ. To obtain spatial images of the surrounding of the sensor, various sampling strategies for ψcan be derived [ 27]. Indeed, in order to form a 2D image in the horizontal plane, we sample the azimuth angle θ in a regular manner from -90 ◦ to 90◦ (e.g., in steps of 1 ◦), while keeping the elevation angle ϕconstant and zero. For 3D images, we sample the direction vector ψuniformly on a sphere using a recursive zonal sphere partitioning algorithm [ 28], as this is the optimal sampling strategy for 3D scenes without prior knowledge. 4 V erification ofHiRIS 4.1 Simulation of Point Spread Functions In order to verify the operation of the HiRIS sensor, a simulation model of the sensor was built, following the equations derived in [26]. We calculate a so-called Point-Spread Function [29, 5] of the sensor system, which describes the image obtained by the sensor in response to a Dirac-like point source in space. We placed the point source in three spatial locations, defined by their azimuth angle (θ) and elevation angle (ϕ): (θ,ϕ) = (0◦,0◦), (30◦,0◦) and (−45◦,45◦). The resulting Point Spread Functions (PSFs) can be found in figure 3. Panels a-c show the point-spread function calculated using conventional Bartlett beamforming, and panels d-f show the PSF when using MVDR beamforming with spatial smoothing, a sub-array size of 28x28 (yielding 25 subarray snapshots), a signal to noise ratio of 5 dB and a diagonal loading of 0.1. What becomes clear from these point- spread functions is their extremely narrow opening angle, and, especially in the case of the MVDR beamformer, excellent peak to sidelobe ratio (approaching 70dB), which in turn will allow the construction of high-resolution acoustic images. 4.2 Real-world validation: Setup The realized prototype of HiRIS can be seen in figure 4. Panel a) shows the front-view of the sensor with the microphone port- holes and the copper slabs used for cooling. Panel b) shows the backside of the back-end PCB, with the USB cables connecting all the nodes to the USB hubs. These four USB hubs are then connected to an aggregate USB hub, which is connected to the host computer. 4.3 Real-world validation: Passive measurement In order to validate the Point-Spread Function of the realized prototype, we performed a passive acoustic measurement using an 40-kHz ultrasonic source placed in front of the microphone array, emitting this pure tone at approximately 70 dB SPL. A recording was made and the resulting data processed by the processing pipeline outline previously. The resulting images of the Point-Spread functions can be seen in figure 5. Panels a and b) show the resulting PSF when the data is processed using a broadband time-domain beamformer (delay-and-sum [3]), both on a logarithmic (a) and linear (b) scale. Panels c) and d) show the response of the system when using an MVDR beamformer, -60-70 a) b) c) -50 -40 -30 -20 -10 0 d) e) f) Figure 3: Point Spread Functions of point sources placed at different spatial locations: panel a & d) (θ,ϕ) = (0◦,0◦), panel b & e) (θ,ϕ) = (30◦,0◦), and panel c & f) (θ,ϕ) = (−45◦,45◦). Panels a-c) show the response of the system using Bartlett beam- forming, and panels d-f) show the response when using the MVDR beamformer. The PSFs are shown on a logarithmic scale. outlined in the previous section. High Peak-to-sidelobe ratios can be noted in these PSFs, however with some deviations from the simulated PSFs shown in figure 5. The reason for this dis- crepancy is most-likely the slight phase di fferences between the simulation models (which assumes a zero-phase transfer function of each microphone) and the real-world microphones (where a slight variation might occur in the phase response). However, calibration techniques to compensate these transfer function differences exist and can be easily incorporated into the processing pipeline [21, 30, 31, 32]. Finally, panels e) and f) show the response of the system to conventional Bartlett beam- forming, again corresponding to the responses simulation in figure 3. A much higher ’noise floor’ can be observed, caused by the more prominent sidelobes present in conventional beam- forming. 4.4 Active Measurements As a final experiment, we performed an active measurement. In this case, the HiRIS sensor uses a Senscomp 7000 trans- ducer [3, 29, 22, 23, 5, 33] to emit a broadband hyperbolic chirp. This chirp is generated by the DAC of the primary node and amplified using a custom high-voltage amplifier to a signal with an amplitude of 200V, superimposed on a bias of 200V. This emitted signal is reflected by the environment, and the subse-Preprint – HiRIS: an Airborne Sonar Sensor with a1024 Channel Microphone Array forIn-Air Acoustic Imaging 7 d) c) a) USB Cables b) USB Hubs Cooling Figure 4: The realized prototype of HiRIS. Panel a) shows the front-view of the sensor with the microphone port-holes, and the 33 USB cables used to connect the nodes to the USB hubs. Panel b) shows the backside of the back-end PCB, with the USB cables connecting all the nodes to the USB hubs,and shows the copper cooling solution provisioned for heat management. The four USB hubs are then connected to an aggregate USB hub, which is connected to the host computer. Panel c) shows the front-view of the HiRIS sensor, where the component-less front-side is visible, with the exception of the holes for the bottom-mounted MEMS microphones. Panel d) shows the cluttered office space which has been ensonified during the active measurement experiment. quent reflections are recorded by the microphones. The signals are then processed into a 2D image using the method outlined before. The resulting 2D image can be found in figure 5, panel g). It shows the Cartesian representation of the 2D polar image (range and azimuth), on a logarithmic intensity scale. These images are often referred to as Energyscapes [ 22] or B-mode images [34, 35]. They show narrow responses both in angle (due to the MVDR beamforming using the 1024 microphone array), as well as narrow range localization (due to the broadband signal used in the matched filtering step), and a high signal-to-noise ratio due to the high number of microphones used. 5 D iscussion andFuture Work In this paper, we have presented HiRIS, the High Resolution Imaging Sonar, a sonar sensor with 1024 microphones. This microphone array is, to the best of the knowledge of the authors, the largest microphone array developed for ultrasound imaging in air to date of writing. We detailed the hardware architecture of the HiRIS sensor, indicating design choices and potential pitfalls when reproducing the hardware system. We provided a reasoning on why certain design choices have been made, and which can be used to inform future decisions when building similar hardware systems. Furthermore, we detailed the data-acquisition pipeline and signal processing approach, and tried to develop an intuition about the scales involved when dealing with a sensor of this complexity. We validated the operation of the system first by simulating the Point-Spread Function of the HiRIS sensor, and compared these resulting PSFs to real-life measurements. Furthermore, we performed an ensonification experiment of a cluttered office environment, and generated B-mode images of the resulting datastreams. With HiRIS, we have developed a novel sensor system which is a step change in imaging capabilities of in-air sonar sensors, and which will allow virtually artifact-free imaging of real-world scenes. Therefore, we see the HiRIS as a virtual upper limit of in-air sonar imaging: more complex sensors could indeed be implemented, but the industrial relevance of systems of this complexity can be debated. Evidently, the approach we have taken during the development of HiRIS is in stark contrast to the developments of our eRTIS line of sensors [3, 14], during which component cost reduction was the major driving force during development. These sensors have been utilized in real-world applications under industrial constraints [36, 37, 38, 39, 40], which has lead us to the ultimate question: what is the upper limit of ultrasound sensing that can be achieved, given the specular reflection model [2] under which the majority of ultrasound sensors operate. With HiRIS, we take the opposite approach: what is the upper limit that, given unrestricted sensing capabilities, can be achieved with in-air ultrasonic imaging, which in turn should lead to answers about the validity of the specular reflection model, the relative importance of diffraction echoes, and how semantic information about the environment is being translated into the ultrasonic sensing domain. To conclude, we believe that the HiRIS sensor will al- low us to uncover the underlying mechanics of in-air ultrasound sensing in a previously unobtainable level of detail, which will then inform the development of future installments of 3D ultrasound sensors for industrial applications. In future work, we aim to further quantify the perfor- mance of the HiRIS sensor, both in laboratory settings as well as real-world measurements. We will produce high-resolution datasets, which will be made open-source for the sensing community to evaluate and use. Using these datasets it should become possible to quantify how information-rich real-world ultrasound measurements really are, and how this information can be leveraged to provide robots with a rich understanding of their environments using ultrasound as a primary sensing modality. From these measurements, the e ffect of applying reduced-aperture microphone arrays instead of the large 1024 element array can be accurately calculated, as virtually any reduced aperture can be adequately simulated using the HiRIS array.Preprint – HiRIS: an Airborne Sonar Sensor with a1024 Channel Microphone Array forIn-Air Acoustic Imaging 8 -10 -5 0 5 10 Cross Range (m) 0 2 4 6 8 10 12Range (m) -30 -25 -20 -15 -10 -5 0 Image Intensity (dB) -70 -60 -50 -40 -30 -20 -10 0 (a, c, e): Image Energy (dB) 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 (b, d, f):Image Energy (Linear) a) c) e) f) d) b) g) Figure 5: Experimental results of the HiRIS sensor. Panels a-c show the response of a 40-kHz source placed in front of the HiRIS sensor, using various processing techniques (a: Delay and Sum, b: MVDR and c: Bartlett beamforming), on a logarithmic scale. Panels b-f) show the response on a linear scale. Panel g) shows the B-mode image of a scene ensonified using a broadband chirp, and processed using the algorithm described in this paper. Acknowledgements The authors would like to thank the Bijzonder OnderzoeksFonds (BOF) of the University of Antwerp for funding this research project. References [1] Ulf Michel. History of acoustic beamforming. In 1st. Berlin Beamforming Conference, 2006. [2] Allan D Pierce. Acoustics: an introduction to its physical principles and applications. Springer, 2019. [3] Robin Kerstens, Dennis Laurijssen, and Jan Steckel. ertis: A fully embedded real time 3d imaging sonar sensor for robotic applications. In 2019 International Conference on Robotics and Automation (ICRA), pages 1438–1443, 2019. doi: 10.1109/ICRA.2019.8794419. [4] Robin Kerstens, Dennis Laurijssen, and Jan Steckel. Low- cost one-bit mems microphone arrays for in-air acoustic imaging using fpga’s. In 2017 IEEE SENSORS, pages 1–3. IEEE, 2017. [5] Robin Kerstens, Dennis Laurijssen, and Jan Steckel. An optimized planar mimo array approach to in-air synthetic aperture sonar. IEEE Sensors Letters, 3(11):1–4, 2019. [6] Thomas Verellen, Robin Kerstens, and Jan Steckel. High- resolution ultrasound sensing for robotics using dense mi- crophone arrays. IEEE Access, 8:190083–190093, 2020. doi: 10.1109/ACCESS.2020.3032177. [7] Gianni Allevato, Jan Hinrichs, Matthias Rutsch, Jan Philipp Adler, Axel Jäger, Marius Pesavento, and Mario Kupnik. Real-time 3-d imaging using an air-coupled ultrasonic phased-array. IEEE transactions on ultrason- ics, ferroelectrics, and frequency control, 68(3):796–806, 2020. [8] Gianni Allevato, Matthias Rutsch, Jan Hinrichs, Christoph Haugwitz, Raphael Müller, Marius Pesavento, and Mario Kupnik. Air-coupled ultrasonic spiral phased array for high-precision beamforming and imaging. IEEE Open Journal of Ultrasonics, Ferroelectrics, and Frequency Con- trol, 2:40–54, 2022. [9] Gianni Allevato, Christoph Haugwitz, Matthias Rutsch, Raphael Müller, Marius Pesavento, and Mario Kupnik. Two-scale sparse spiral array design for 3d ultrasound imaging in air. IEEE Open Journal of Ultrasonics, Ferro- electrics, and Frequency Control, 2023. [10] Erik Verreycken, Walter Daems, and Jan Steckel. Dis- tributed low-cost microphone array for 3d localization for bio-acoustic applications. In 2017 IEEE SENSORS, pages 1–3. IEEE, 2017. [11] Erik Verreycken, Walter Daems, and Jan Steckel. Passive acoustic sound source tracking in 3d using distributed mi- crophone arrays. In 2018 International Conference on Indoor Positioning and Indoor Navigation (IPIN), pages 1–8. IEEE, 2018. [12] Erik Verreycken, Ralph Simon, Brandt Quirk-Royal, Wal- ter Daems, Jesse Barber, and Jan Steckel. Bio-acoustic tracking and localization using heterogeneous, scalable microphone arrays. Communications biology, 4(1):1275, 2021. [13] Dennis Laurijssen, Robin Kerstens, Girmi Schouten, Wal- ter Daems, and Jan Steckel. A flexible low-cost biolog- ically inspired sonar sensor platform for robotic applica-Preprint – HiRIS: an Airborne Sonar Sensor with a1024 Channel Microphone Array forIn-Air Acoustic Imaging 9 tions. In 2019 International Conference on Robotics and Automation (ICRA), pages 9617–9623. IEEE, 2019. [14] Thomas Verellen, Robin Kerstens, Dennis Laurijssen, and Jan Steckel. Urtis: A small 3d imaging sonar sensor for robotic applications. In ICASSP 2020-2020 IEEE Interna- tional Conference on Acoustics, Speech and Signal Pro- cessing (ICASSP), pages 4801–4805. IEEE, 2020. [15] Dennis Laurijssen, Anthony Schenck, Girmi Schouten, Robin Kerstens, Sebastiaan Aussems, Eric Paillet, Randy Gomez, Keisuke Nakamura, Walter Daems, and Jan Steckel. Bio-inspired gesture recognition with baffled trans- ducers using temporal and spectral features. In 2023 IEEE SENSORS, pages 1–4. IEEE, 2023. [16] Dennis Laurijssen, Steven Truijen, Wim Saeys, Walter Daems, and Jan Steckel. A flexible embedded hardware platform supporting low-cost human pose estimation. In 2016 International Conference on Indoor Positioning and Indoor Navigation (IPIN), pages 1–8. IEEE, 2016. [17] Jacek Dmochowski, Jacob Benesty, and Sofiène Affès. On spatial aliasing in microphone arrays. IEEE Transactions on Signal Processing, 57(4):1383–1395, 2008. [18] Eric Konetzke, Matthias Rutsch, Maik Hoffmann, Alexan- der Unger, René Golinske, Dirk Killat, Sivaram Nishal Ramadas, Steve Dixon, and Mario Kupnik. Phased array transducer for emitting 40-khz air-coupled ultrasound with- out grating lobes. In 2015 IEEE International Ultrasonics Symposium (IUS), pages 1–4. IEEE, 2015. [19] Da Liang, Kaiyu Liu, Heng Zhang, Yunkai Deng, Dacheng Liu, Yafeng Chen, Chuang Li, Haixia Yue, and Robert Wang. A high-accuracy synchronization phase- compensation method based on kalman filter for bistatic synthetic aperture radar. IEEE Geoscience and Remote Sensing Letters, 17(10):1722–1726, 2019. [20] Xiaopeng Yang, Pilei Yin, and Tao Zeng. Time and phase synchronization for wideband distributed coherent aperture radar. 2013. [21] Shivansh Chaudhary and Abhay Samant. Characteriza- tion and calibration techniques for multi-channel phase- coherent systems. In 2015 IEEE AUTOTESTCON, pages 334–338. IEEE, 2015. [22] Jan Steckel, Andre Boen, and Herbert Peremans. Broad- band 3-d sonar system using a sparse array for indoor nav- igation. IEEE Transactions on Robotics, 29(1):161–171, 2012. [23] Jan Steckel and Herbert Peremans. Batslam: Simultaneous localization and mapping using biomimetic sonar. PloS one, 8(1):e54076, 2013. [24] Class definitions for communication devices 1.2. https://www.usb.org/document-library/ class-definitions-communication-devices-12 . [25] Navtej Singh, Lisa-Marie Browne, and Ray Butler. Parallel astronomical data processing with python: Recipes for multicore machines. Astronomy and Computing, 2:1–10, 2013. [26] Harry L Van Trees. Optimum array processing: Part IV of detection, estimation, and modulation theory. John Wiley & Sons, 2002. [27] Jonas Reijniers, Robin Kerstens, and Jan Steckel. An optimized spatial sampling strategy for wide-view planar array 3-d sonar sensors. IEEE Transactions on Ultrasonics, Ferroelectrics, and Frequency Control, 67(6):1236–1241, 2020. [28] Paul Leopardi. A partition of the unit sphere into regions of equal area and small diameter. Electronic Transactions on Numerical Analysis, 25(12):309–327, 2006. [29] Jan Steckel. Sonar system combining an emitter array with a sparse receiver array for air-coupled applications. IEEE Sensors Journal, 15(6):3446–3452, 2015. [30] Jack J Schuss, Thomas V Sikina, Joseph E Hilliard, Patrick J Makridakis, Je ffrey Upton, Joseph C Yeh, and Stephen M Sparagna. Large-scale phased array calibration. IEEE Transactions on Antennas and Propagation, 67(9): 5919–5933, 2019. [31] Guolong He, Xin Gao, and Rentian Zhang. Impact analysis and calibration methods of excitation errors for phased array antennas. IEEE Access, 9:59010–59026, 2021. [32] Ashok Agrawal and Allan Jablon. A calibration tech- nique for active phased array antennas. In IEEE Interna- tional Symposium on Phased Array Systems and Technol- ogy, 2003., pages 223–228. IEEE, 2003. [33] Dennis Laurijssen, Steven Truijen, Wim Saeys, and Jan Steckel. Three sources, three receivers, six degrees of free- dom: An ultrasonic sensor for pose estimation & motion capture. In 2015 IEEE SENSORS, pages 1–4. IEEE, 2015. [34] Elisabetta Sassaroli, Calum Crake, Andrea Scorza, Don- Soo Kim, and Mi-Ae Park. Image quality evaluation of ultrasound imaging systems: advanced b-modes. Journal of applied clinical medical physics, 20(3):115–124, 2019. [35] Giulia Matrone, Alessandro Stuart Savoia, Giosuè Caliano, and Giovanni Magenes. The delay multiply and sum beam- forming algorithm in ultrasound b-mode medical imaging. IEEE transactions on medical imaging, 34(4):940–949, 2014. [36] Anthony Schenck, Walter Daems, and Jan Steckel. Airleak- slam: Automated air leak detection. In Advances on P2P , Parallel, Grid, Cloud and Internet Computing: Proceed- ings of the 14th International Conference on P2P , Parallel, Grid, Cloud and Internet Computing (3PGCIC-2019) 14, pages 746–755. Springer, 2020. [37] Anthony Schenck, Walter Daems, and Jan Steckel. Air- leakslam: detection of pressurized air leaks using passive ultrasonic sensors. In 2019 IEEE SENSORS, pages 1–4. IEEE, 2019. [38] Robin Kerstens, Wouter Jansen, Gauthier de Borrekens, Stefaan Ides, and Jan Steckel. Tracking moored vessel movement in multiple dof using active sensing methods. IEEE Sensors Letters, 7(2):1–3, 2023. [39] Thomas Verellen, Florian Verbelen, Kurt Stockman, and Jan Steckel. Beamforming applied to ultrasound analysis in detection of bearing defects. Sensors, 21(20):6803, 2021. [40] Wouter Jansen, Dennis Laurijssen, and Jan Steckel. Real- time sonar fusion for layered navigation controller.Sensors, 22(9):3109, 2022.",
      "references": [
        "History of acoustic beamforming.",
        "Acoustics: an introduction to its physical principles and applications.",
        "ertis: A fully embedded real time 3d imaging sonar sensor for robotic applications.",
        "Low- cost one-bit mems microphone arrays for in-air acoustic imaging using fpga’s.",
        "An optimized planar mimo array approach to in-air synthetic aperture sonar.",
        "High- resolution ultrasound sensing for robotics using dense mi- crophone arrays.",
        "Real-time 3-d imaging using an air-coupled ultrasonic phased-array.",
        "Air-coupled ultrasonic spiral phased array for high-precision beamforming and imaging.",
        "Two-scale sparse spiral array design for 3d ultrasound imaging in air.",
        "Dis- tributed low-cost microphone array for 3d localization for bio-acoustic applications.",
        "Passive acoustic sound source tracking in 3d using distributed mi- crophone arrays.",
        "Bio-acoustic tracking and localization using heterogeneous, scalable microphone arrays.",
        "A flexible low-cost biolog- ically inspired sonar sensor platform for robotic applica- tions.",
        "Urtis: A small 3d imaging sonar sensor for robotic applications.",
        "Bio-inspired gesture recognition with baffled trans- ducers using temporal and spectral features.",
        "A flexible embedded hardware platform supporting low-cost human pose estimation.",
        "On spatial aliasing in microphone arrays.",
        "Phased array transducer for emitting 40-khz air-coupled ultrasound with- out grating lobes.",
        "A high-accuracy synchronization phase- compensation method based on kalman filter for bistatic synthetic aperture radar.",
        "Time and phase synchronization for wideband distributed coherent aperture radar.",
        "Characteriza- tion and calibration techniques for multi-channel phase- coherent systems.",
        "Broad- band 3-d sonar system using a sparse array for indoor nav- igation.",
        "Batslam: Simultaneous localization and mapping using biomimetic sonar.",
        "Class definitions for communication devices 1.2.",
        "Parallel astronomical data processing with python: Recipes for multicore machines.",
        "Optimum array processing: Part IV of detection, estimation, and modulation theory.",
        "An optimized spatial sampling strategy for wide-view planar array 3-d sonar sensors.",
        "A partition of the unit sphere into regions of equal area and small diameter.",
        "Sonar system combining an emitter array with a sparse receiver array for air-coupled applications.",
        "Large-scale phased array calibration.",
        "Impact analysis and calibration methods of excitation errors for phased array antennas.",
        "A calibration tech- nique for active phased array antennas.",
        "Three sources, three receivers, six degrees of free- dom: An ultrasonic sensor for pose estimation & motion capture.",
        "Image quality evaluation of ultrasound imaging systems: advanced b-modes.",
        "The delay multiply and sum beam- forming algorithm in ultrasound b-mode medical imaging.",
        "Airleak- slam: Automated air leak detection.",
        "Air- leakslam: detection of pressurized air leaks using passive ultrasonic sensors.",
        "Tracking moored vessel movement in multiple dof using active sensing methods.",
        "Beamforming applied to ultrasound analysis in detection of bearing defects.",
        "Real- time sonar fusion for layered navigation controller."
      ],
      "meta_data": {
        "arxiv_id": "2402.13110v1",
        "authors": [
          "Dennis Laurijssen",
          "Walter Daems",
          "Jan Steckel"
        ],
        "published_date": "2024-02-20T16:03:02Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Introduces HiRIS, a high-resolution airborne ultrasound imaging sensor that addresses artifacts and limited dynamic range of prior reduced-aperture microphone arrays by using a dense 1024-channel (32×32) MEMS microphone array with synchronized sampling and a distributed embedded acquisition architecture. Demonstrates near artifact-free 2D/3D imaging over the frontal hemisphere with high angular accuracy and simulated peak-to-sidelobe ratios up to ~70 dB using MVDR+FB spatial smoothing; validates via simulated point-spread functions and real passive/active measurements.",
        "methodology": "Hardware: 1024 Knowles SPH0641LU4H-1 PDM MEMS microphones on a 3.9 mm-pitch URA; distributed DAQ with 32 subordinate STM32F429 nodes (each sampling 32 mics via 16 GPIO using stereo PDM edges) plus a primary STM32F429 node for 4.5 MHz clock fan-out (CDCLVC1108) and synchronous triggering; external SDRAM (64 MB/node) buffering; USB CDC-VCP streaming to PC; optional DAC output for active chirp emission. Signal processing: PDM demodulation via low-pass filtering (100 kHz) and decimation; matched filtering (emitted waveform for active, impulse for passive); STFT to obtain narrowband snapshots (e.g., 42 kHz bin); MVDR beamforming with diagonal loading and Forward–Backward Spatial Smoothing (FB-SS) to synthesize multiple snapshots from one measurement by forming 28×28 subarrays (25 snapshots) and estimating covariance; spatial sampling for 2D via azimuth sweep and for 3D via uniform spherical sampling (recursive zonal sphere partitioning).",
        "experimental_setup": "Verification includes (1) simulation of point-spread functions using array-processing model: compares Bartlett (conventional) vs MVDR+FB-SS with subarray 28×28, SNR 5 dB, diagonal loading 0.1; evaluates sources at (θ,ϕ)=(0°,0°),(30°,0°),(-45°,45°). (2) Real-world prototype: two stacked 180×170 mm PCBs (front-end mics, back-end electronics), 33 USB2 links aggregated via USB3 hubs, ~65 W power with copper-slab passive cooling. Passive validation: 40 kHz tone source at ~70 dB SPL in front of array; forms PSF images using delay-and-sum, MVDR, and Bartlett; compares sidelobes/noise floor with simulation. Active experiment: Senscomp 7000 transducer emits broadband hyperbolic chirp from primary-node DAC amplified to ~200 V amplitude with 200 V bias; matched filtering + MVDR produces 2D range–azimuth B-mode (energyscape) in a cluttered office scene.",
        "limitations": "High cost/complexity and non-industrial form factor: ~4963 components, €3962 build cost, 33 USB connections, large data volume (~40 MB per 70 ms capture; up to ~1 GB/s at 100% duty cycle), and high power (~65 W) requiring thermal management. Spatial aliasing limit from 3.9 mm pitch implies grating lobes above ~44 kHz for steering, constraining artifact-free narrowband operation (though broadband processing is used for ranging). MVDR performance depends on accurate microphone phase responses; real measurements deviate from ideal simulations due to microphone transfer-function variations, implying need for calibration. Snapshot scarcity in sonar necessitates spatial smoothing assumptions (subarray stationarity) and diagonal loading choices. Evaluation is mostly PSF/qualitative imaging; limited quantitative benchmarking across diverse environments and no open dataset yet.",
        "future_research_directions": "Develop and apply calibration pipelines for per-microphone phase/amplitude and timing to close the sim-to-real gap and further suppress sidelobes. Produce and release open high-resolution datasets (passive and active) to quantify information content of airborne ultrasound, test specular vs diffraction-echo models, and enable learning-based scene understanding. Systematically study reduced-aperture/subsampling strategies using HiRIS as ground truth to derive cost–performance tradeoffs for industrial sensors. Expand hardware integration: onboard USB3 hub backplane, consolidated cabling, easier firmware flashing (JTAG/SWD mux), and potentially on-device preprocessing/compression to reduce host bandwidth. Broaden quantitative evaluation: resolution/dynamic-range metrics, multi-target scenarios, 3D reconstructions with uniform spherical sampling, robustness to environmental factors (temperature, airflow), and integration into robotic perception stacks.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Iteratively Refined Behavior Regularization for Offline Reinforcement Learning",
      "full_text": "Utilising high-dimensional data in randomised clinical trials: a review of methods and practice Research Methods in Medicine & Health Sciences 0(0):1–12 ©The Author(s) 2022 Reprints and permission: sagepub.co.uk/journalsPermissions.nav DOI: 10.1177/ToBeAssigned www.sagepub.com/ SAGE Svetlana Cherlin1, Theophile Bigirumurame1, Michael J Grayling1, J´er´emie Nsengimana1, Luke Ouma1, Aida Santaolalla2, Fang Wan3, S Faye Williamson1, James M S Wason1 Abstract Introduction: Even in effectively conducted randomised trials, the probability of a successful study remains relatively low. With recent advances in the next-generation sequencing technologies, there is a rapidly growing number of high- dimensional data, including genetic, molecular and phenotypic information, that have improved our understanding of driver genes, drug targets, and drug mechanisms of action. The leveraging of high-dimensional data holds promise for increased success of clinical trials. Methods: We provide an overview of methods for utilising high-dimensional data in clinical trials. We also investigate the use of these methods in practice through a review of recently published randomised clinical trials that utilise high- dimensional genetic data. The review includes articles that were published between 2019 and 2021, identified through the PubMed database. Results: Out of 174 screened articles, 100 (57.5%) were randomised clinical trials that collected high-dimensional data. The most common clinical area was oncology (30%), followed by chronic diseases (28%), nutrition and ageing (18%) and cardiovascular diseases (7%). The most common types of data analysed were gene expression data (70%), followed by DNA data (21%). The most common method of analysis (36.3%) was univariable analysis. Articles that described multivariable analyses used standard statistical methods. Most of the clinical trials had two arms. Discussion: New methodological approaches are required for more efficient analysis of the increasing amount of high- dimensional data collected in randomised clinical trials. We highlight the limitations and barriers to the current use of high-dimensional data in trials, and suggest potential avenues for improvement and future work. Keywords Genetic data, High-dimensional information, Precision medicine, Randomised clinical trials, Statistical analysis Introduction Randomised controlled trials (RCTs) are the gold standard for assessing the safety and efficacy of an experimental treatment. However, despite the growing cost and time associated with developing and evaluating drugs, the probability of success of RCTs is relatively low. 1 One of the reasons is that there is rarely a “one size fits all” approach in most clinical areas because treatment typically has a heterogeneous effect on patients with different pathogenic mechanisms. For example, in a study that investigated predictors of response to Methotrexate in early rheumatoid arthritis, 2 75% of the patients experienced a good response rate, according to the EULAR response criteria.3 This study found that several demographic and clinical characteristics (including age, sex, smoking status and symptom duration) are associated with response to Methotrexate. Subsequently, a double-blind phase IV clinical trial in patients with rheumatoid arthritis identified genetic markers that could partly explain the heterogeneity of response to Methotrexate.4 With recent advances in the next-generation sequencing technologies, there is a rapidly growing number of human molecular biomarkers that could inform drug mechanisms and increase the success of clinical trials. 5 Molecular biomarkers are measurable molecular characteristics (small molecules) that could identify relatively homogeneous disease subsets in terms of clinical features, diagnosis, prognosis, or response to treatment. With the advent of personalised medicine, molecular biomarkers are gaining importance in clinical research. 6 The most common types of molecular biomarkers are genomic biomarkers such as deoxyribonucleic acid (DNA) and ribonucleic acid (RNA). Single nucleotide polymorphisms (SNPs), which are the most abundant type of genetic variation, represent a difference in a single nucleotide. SNPs are often measured 1Population Health Sciences Institute, Newcastle University, Newcastle upon Tyne, UK 2Translational Oncology & Urology Research Group, Centre for Cancer, Society & Public Health, King’s College London, UK 3Department of Mathematics and Statistics, Lancaster University, Lancaster, UK Corresponding author: Svetlana Cherlin, Population Health Sciences Institute, Newcastle University, Ridley Building 1, Queen Victoria Road, Newcastle upon Tyne, NE1 7RU, UK. Email: svetlana.cherlin@newcstle.ac.uk Prepared usingsagej.cls [Version: 2017/01/17 v1.20] arXiv:2305.10174v2  [stat.AP]  5 Feb 20242 Research Methods in Medicine& Health Sciences 0(0) (genotyped) across the genome, and the associations between genome-wide SNPs and different human traits, i.e. genome- wide association studies (GW AS), are extensively used in genetics. 7 GW AS to date have analysed hundreds of thousands of genetic variants generated by next-generation sequencing technologies. Proteomics and metabolomics also play an important role in many medical applications and are being increasingly used in drug research and development. 8 Proteomics is a study of molecules in proteins that allows characterisation of protein structure and function. Protein biomarkers are also increasingly used in clinical trials in patient stratification, disease diagnosis, and prognosis. 9 Another commonly used genetic biomarker is gene expression, which is a process that regulates the amount of protein or other molecules expressed by the cell, and thus is measured by the amount of the molecules or protein. The advantage of microarray technology is to allow for gene expression profiling which consists of measuring levels of thousands of genes. Changes in gene expression can reflect the change in a cell’s environment, such as disease state,10 response to treatment11 or treatment side effect. 12 Metabolites, small molecules produced by the body when it breaks down food or drugs, are useful for biomarker discovery because they can be utilised to examine the underlying biochemical activity of cells. Modern technologies, such as mass spectrometry, allow for a large number of metabolies to be measured thus creating a metabolomic profile. 13 Metabolic changes are informative of the response to treatment and therefore have the potential to be useful in clinical trials. 14 For example, a randomised placebo-controlled clinical trial that examined the effect of sertraline on major depressive disorder patients found that baseline metabolic signatures could be predictive of response or non-response to sertraline.15 In clinical trials, biomarkers serve multiple purposes, such as prognosis of the likely progression of a disease, and prediction of the likely clinical outcome. 16 Prognostic biomarkers are those that are associated with disease prognosis in the absence of treatment or in the presence of a standard of care treatment. Predictive biomarkers are those that are associated with the effectiveness of a specific treatment. Predictive biomarkers could be used to identify subsets of patients who are likely to respond to treatment. For example, a pooled analysis of randomised trials found that women whose breast tumours have overexpressed the human epidermal growth factor receptor 2 ( HER 2) protein or amplified HER 2 gene (HER 2-positive) benefited from adjuvant treatment with anthracyclines, while women with HER 2-negative breast tumours derived no added benefits from adjuvant chemotherapy with anthracyclines. 17 Thus, the HER 2 status of a breast tumour is a predictive biomarker for response to adjuvant treatment with anthracyclines. Prognostic and predictive biomarkers are usually measured once, before the start of treatment. Biomarkers that are measured repeatedly during the trial could be used as a surrogate endpoint, i.e. as a proxy for a clinical endpoint. Biomarkers based on a continuous single gene measurement can be used as classifiers by considering a threshold, or a series of thresholds, to specify a biomarker-positive and biomarker-negative group 18,19. However, identifying single-gene biomarkers requires knowledge and biological interpretation of the disease pathway, which may not always be available. Recent advances in whole genome biotechnology allow for measuring multiple genetic variants during clinical trials.20–23 This allows biomarkers across multiple genes to be developed, i.e. biomarkers based on high-dimensional data. A variety of predictive and prognostic biomarkers based on high-dimensional molecular profiling have been proposed in oncology. 24–26 These biomarkers are especially relevant for finding potential responders to a treatment in settings where an assay for identifying biomarker-positive patients is not yet available.27 While prognostic biomarkers based on high-dimensional data are becoming increasingly available, predictive biomarkers based on high-dimensional data are rare due to the challenge of understanding a treatment’s mechanism of action.28 Additional challenges of using high- dimensional data are identifying which biomarkers to include in the model, and how to effectively/appropriately combine the individual biomarkers.29 In this paper, we provide an overview of several statistical methods for utilising high-dimensional data in the analysis of RCTs. We also present a review of recently published clinical trials that utilised high-dimensional data to investigate how often various methods have been used in practice. Overview of methods for utilising high-dimensional data in clinical trials In this section, we describe statistical methods used for analysing high-dimensional data in RCTs; many of which have been implemented in standard statistical software such as R. 30 A summary of these methods is provided in Table 1. When considering suitability of the methods, it is important to distinguish between testing for association and prediction. Association tests, such as the Chi-Square test, can shed light on the biological processes by providing better understanding of the phenomenon in question. Association tests are useful for testing hypotheses about the differences between the groups of observations, such as the difference between the treatment arms, or for finding biomarkers that are associated with response to treatment. In prediction analysis, statistical models such as regression are applied to data in order to build predictors that could be applied to future studies. The quality of prediction should be assessed on an independent dataset using some measure of the discrepancy between the observed and predicted outcomes. Some of the methods we review in this manuscript focus on either testing for association or on prediction, while others focus on both. However, it is important to note that models that have high power to detect associations do not necessarily have high predictive power.31 Notation In this section we describe a two-arm RCT where participant i (i = 1, . . . , n) is randomised to either an intervention arm ( ti = 1) or control arm ( ti = 0). For each participant i, a set of j = 1, . . . , mbiomarkers, xij, are collected, and an outcome yi is measured. Regression modelling is often used to model the outcome yi as a function of the covariates xij, which are measurable quantities related Prepared usingsagej.clsCherlin and Wason 3 Table 1. Summary, advantages and disadvantages of methods utilising high-dimensional data. Method Summary Advantages Disadvantages Univariable approach Testing one biomarker at a time Simplicity. Multiple testing issue Multivariable approach Testing a number of biomarkers simultaneously Fitting a single model for a several biomarkers Overfitting Penalised approach Penalises regression coefficients, causing them to shrink, maybe to zero Prevention of overfitting Tuning of parameters Random forests Collection of regression or classification trees Allows modelling non- linear interactions Lack of intuitive interpretation Support vector machines Building a classifier by fitting a hyperplane between different groups of observations Allows modelling non- linear interactions Computational complexity Cluster analysis Grouping data based on a measure of similarity Allows modelling non- linear interactions Sensitivity to outliers Gene sets and networks Undirected graphs representing associations between the genes Dimensionality reduction Computational complexity Principal component analysis Transforming high-dimensional data into low-dimensional variables that account for most of the original data’s variation Allows modelling non- linear interactions Lack of intuitive interpretation of the principal components Adaptive signature design Constructing a low-dimensional score from high-dimensional data Finding group of patients benefiting from treatment Multiple testing issue to the outcome. For different types of outcome, different types of regression are used. The most common types are linear regression (for continuous outcomes), logistic regression (for binary outcomes) and Cox regression (for time-to-event outcomes). Linear regression models the mean of the continuous outcome, assuming that the outcome is normally distributed. Logistic regression models the log odds, logit(pi) = log \u0010 P(Yi=1) 1−P(Yi=1) \u0011 , where P(Yi = 1) denotes the probability of a successful outcome. Cox regression models the hazard ratio of an event at time t, log \u0010 hi(t) h0i(t) \u0011 , where h0i(t) is the baseline hazard at time t. In these regression models, the link function of the response variable connects the covariates with the expected value of the outcome variable in a linear way, while the covariates are being weighted by their coefficients. The null hypothesis of a specific coefficient being zero represents testing for an effect of the corresponding covariate. Univariable approach A univariable approach consists of testing a single biomarker’s relationship to a response variable. In linear regression, the outcome yi for patient i takes the form yi = βj0 + βj1ti + βj2xij + βj3tixij + ϵi, where ϵi ∼ N(0, σ2) is the error term. In logistic regression, the probability of the outcome yi for patient i takes the form: logit(pi) =βj0 + βj1ti + βj2xij + βj3tixij. In Cox regression, the hazard ratio of an event at time t for patient i takes the form: log \u0012 hi(t) h0i(t) \u0013 = βj1ti + βj2xij + βj3tixij. The null hypothesis Hj2 : βj2 = 0 represents testing for a prognostic effect of biomarker j, while the null hypothesis Hj3 : βj3 = 0 represents testing for a predictive effect of biomarker j. These hypotheses could then be tested using a Wald test, for example. Applying statistical tests to one biomarker at a time could result in an inflated number of false positives, due to multiple independent comparisons. 32 To prevent this, the Bonferroni correction33 is often applied, which adjusts the significance level of individual tests to levelα/m, where m is the number of tests and α is the desired family-wise error rate. To reduce multiple testing burden, a two-step procedure has been proposed 34 that accounts for correlation between the biomarkers via penalised regression. In the first stage of the procedure, a screening test selects a subset of biomarkers, and in the second stage, only the selected biomarkers are tested for interaction. An additional challenge in detecting interactions is due to the large sample size required to obtain high power. 35,36 In the case of a binary biomarker, in which the trial population can be divided into biomarker-positive and biomarker- negative subgroups, the sample size for testing a null hypothesis of no interaction is at least four times higher than the sample size needed to test the main effect (see Appendix). Univariable analysis models are straightforward to fit and produce intuitive results. However, in the real word there is often more than just one biomarker involved. Analysing one biomarker at a time ignores the correlation between the biomarkers, which could lead to incorrectly concluding that some biomarkers are predictive. Multivariable approach A multivariable regression takes into account two or more biomarkers. Similarly to the univariable regression, there are three commonly used regression types: linear (for continuous outcomes), logistic (for binary outcomes) and Cox regression (for time-to-event outcomes), which take the following form when m biomarkers are simultaneously adjusted for: yi = βj0 + β1ti + mX j=1 βj2xij + mX j=1 βj3tixij + ϵi, Prepared usingsagej.cls4 Research Methods in Medicine& Health Sciences 0(0) logit(pi) =βj0 + β1ti + mX j=1 βj2xij + mX j=1 βj3tixij, and log \u0012 hi(t) h0i(t) \u0013 = β1ti + mX j=1 βj2xij + mX j=1 βj3tixij, respectively. Multivariable analysis estimates the contribu- tion of each biomarker xij while adjusting for the effect of other biomarkers or covariates. Therefore, unlike univariable analyses, it takes into account correlation between biomark- ers. The main drawback of the multivariable approach is the large number of parameters that may be included. With high-dimensional data, this approach can lead to a model with more parameters than observations (i.e. the “curse of dimensionality”). In this case, multivariable linear regression cannot be used because the unique ordinary least squares estimators of the regression coefficients are not defined. To reduce the complexity of the model, several variable selection approaches have been proposed, including machine learning approaches (discussed below). However, a large number of parameters in the model could still lead to overfitting, which is the phenomenon of modelling the observed data too precisely so that it captures the noise in the data. In this case, the model shows an inferior performance when applied to a new dataset. To reduce the potential effects of overfitting, a rule-of-thumb is that at least ten events are required per variable in logistic and Cox regression models, though this rule is often debated. 37 For linear regression estimated using ordinary least squares, the number of covariates that can be included in the model is generally higher; it has been shown that two subjects per value would be sufficient for adequate estimation of regression coefficients.38 Regularised (penalised) regression Regularised, or penalised, approaches penalise models by shrinking the estimates of the regression coefficients. Suppose a regression model with a (m + 1)-dimensional vector of covariates β = (β0, β1, . . . , βm)T is fitted by maximising the log-likelihood function ℓ(β). In penalised regression, ℓ(β) is maximised subject to a penalty function P(β) and a regularisation parameter λ, that is, ˆβ = argmax[ℓ(β) − λP(β)]. As a result, the regression coefficient estimate ˆβ is shrunk towards zero in comparison to the maximum likelihood estimate, with λ controlling the amount of shrinkage. The method induces different degrees of sparsity, depending of the type of penalty used. For example, the Least Absolute Shrinkage and Selection Operator (LASSO) regression39 allows shrinkage of the coefficients to zero by penalising the model with P(β) =||β||ℓ1 = Pm j=1 |βj| and is therefore a sparse method which allows for variable selection. Another type of penalised regression is ridge regression40 in which the penalty function has the form P(β) =||β||ℓ2 = Pm j=1 β2 j . Ridge regression shrinks the coefficients towards zero, however it does not shrink them to zero. Elastic net 41 is a type of penalised regression in which both penalties are used, i.e. ˆβ = argmax  ℓ(β) − λ  η mX j=1 |βj| + 1 − η 2 mX j=1 β2 j    . The combination of the penalties is controlled by a penalty weight parameter η. When η = 1, the elastic net is identical to LASSO, whereas when η = 0 it is identical to ridge. Elastic net combines setting of the coefficients to zero using LASSO and shrinking of the coefficients using ridge, to improve the model’s performance. A penalised logistic regression model, which included ten genes, was used to predict the overall complete pathologic response rate in a phase II genomic study of ixabepilone as neoadjuvant treatment for breast cancer. 42 A pharmacogenetic study used ridge regression to predict a response to treatment. 11 It has been found that using LASSO regression improved the accuracy of the treatment effect estimator in a RCT. 43 A review of neoadjuvant clinical trials in breast cancer that analysed gene expression data 44 found that penalised methods outperform competing methods when applied to estrogen receptor-positive (ER+) early breast cancer patients treated with neoadjuvant aromatase inhibitor letrozol. However, an application of a penalised high-dimensional Cox model to an early breast cancer RCT of chemotherapy with or without adjuvant trastuzumab resulted in highly variable expected survival probabilities with very large confidence intervals.45 Group-lasso46 is a special case of LASSO that performs selection of important groups of variables. For example, the groups could represent specific biological pathways of the biomarkers, or variables that reflect a specific aspect of a treatment. Extending the group-lasso by considering interactions,47 however, can result in many false positive interactions for high-dimensional problems. Penalised regression requires optimisation of the penalty parameter, which could be done using cross-validation. In the cross-validation procedure, a model is fitted to a subset of the data and its accuracy is assessed on a different subset of the data. The process is repeated multiple times with different partitions of the data for fitting (training subset) and assessing (testing subset). Parameters that lead to the best accuracy are chosen. However, when cross- validation is used to examine model performance, tuning of the parameters requires nested cross-validation, in which the inner cross-validation (for tuning of parameters) is encapsulated inside the outer cross-validation (for assessing model performance). This procedure requires large sample sizes. It is also necessary to ensure homogeneous partitioning of the data with respect to important features, in order to achieve a valid cross-validation procedure48. Machine learning approaches Machine learning is a class of algorithms that analyse data based on existing (training) data. 49 Machine learning algorithms can either be supervised or unsupervised, with the difference being the labelling of the input data. In supervised machine learning algorithms such as classification, the training data is labelled, while in unsupervised methods such as clustering, the training data is not labelled. Supervised Prepared usingsagej.clsCherlin and Wason 5 approaches are used for predictive modelling when the classification of the training data is known in advance, and the trained algorithm is used to predict or classify new data with unknown classification, such as response or non-response to treatment in clinical trials. Unsupervised methods are used for feature selection problems, such as identifying a predictive biomarker in the context of biomarker analysis, and dimensionality reduction50. Random forests Random forests are a type of high- dimensional nonparametric model aimed at prediction,51 and therefore belong to the class of supervised machine learning algorithms. They are represented as a collection of regression trees (for a continuous outcome) or classification trees (for a binary outcome). Each tree is a decision model that consists of a recursive partitioning of a dataset into subsets that are determined by a randomly selected group of input variables. The subsets are homogeneous with respect to the group of variables. At each node of a tree, different groups of variables might be used. Random forests are formed by trees constructed from training datasets sampled with replacement from the original dataset. The remaining samples form the testing datasets and are used for assessing prediction accuracy. For example, the probability of misclassifying an observation could be used as a measure of prediction accuracy. Random forests are flexible in that regression and classification trees can incorporate non-linear interactions between the variables.52 Traditional random forests are designed for one treatment group and are therefore suitable for prognostic, rather than predictive, purposes. A few adaptations of the method for more than one treatment group have been developed that facilitate identification of a subset of patients who benefit from the treatment. For example, the “Virtual Twins” method53 is a random forest-based method of identifying a subgroup of enhanced treatment effect by incorporating treatment-covariate interactions. A variation of the random forest has been developed, that uses a measure based on a difference in survival times as an alternative to the accuracy prediction, for deciding on a best possible split. 54 When applied to a phase III RCT with high-dimensional SNP data, this approach has been shown to outperform a univariable analysis. The challenges of this method include specifying model parameters, such as the number of trees in the forest. Support vector machines Support vector machines (SVM) are a supervised machine learning method for building a classifier that can be used to account for non- linear relationships between variables. 55 SVM assign an observation to a specific category, or class, by fitting a hyperplane between the samples from different classes so that the distance between the hyperplane to the nearest sample is maximised. This distance is maximised using support vectors, i.e. data points that are closer to the hyperplane. SVM involve transforming the data using a kernel function to allow linear separation of the data. An advantage of SVM is that it can effectively incorporate high- dimensional data that can be noisy and/or correlated. It has been widely applied to classification problems using high- dimensional biomarkers. 56–60 SVM could be used in RCTs if treatment-covariate interaction effects are introduced into the feature space of SVM. Using SVM constructed from the combination of brain imaging and demographic and clinical biomarkers, a group of Mild Cognitive Impairment patients who were most likely to cognitively decline has been identified.61 Limitations of SVM include their computational complexity, especially the need to optimise their parameters. Cluster analysis Clustering methods are unsupervised methods of grouping data based on some measure of similarity, so that the obser- vations in each group are similar (but dissimilar to those in other groups). The most common measure of similarity be- tween the observations is correlation. Traditional clustering methods include hierarchical clustering and partitioning. 62 In hierarchical clustering, the data is organised into a tree- shape structure (a dendogram) constructed from hierarchical series of nested clusters, while partitioning does not assume hierarchical relationships between clusters. An example of partitioning is k-means clustering, which partitions the data into a pre-specified number k of mutually exclusive groups so that the the sum of the squared distances between the members of the group and the means of the clusters is min- imised.63 Another example is Partitioning Around Medoids clustering, which is similar to thek-means but is more robust to outliers.64 Hierarchical clustering employs agglomerative and divi- sive strategies. Hierarchical agglomerative clustering starts by treating each sample as a separate cluster and then merges the most similar clusters together. This process is repeated iteratively until all samples are clustered. Hierarchical divi- sive clustering starts by treating all the observation as one cluster, and them recursively splits the cluster into two, until the desired number of clusters is obtained. Hierarchical clus- tering could be used to analyse genes that are differentially expressed between different experimental conditions, such as the different treatment groups in clinical trials. To estimate the number of clusters in the dataset, consensus clustering could be used which utilises bootstrapping to classify each observation multiple times. Finally, observations are as- signed to the cluster with the highest consensus score and the number of clusters is derived from objective metrics.65 Other methods, called model-based clustering, exploit the same idea of making clustering robust to model misspecification and estimation of the number of clusters. They assume that observations follow a mixture of distributions rather than belonging to discrete classes. Clustering is often used in gene expression analysis because it simplifies visualisation and allows one to trace specific biological pathways.66–69 Moreover, it can be used to identify specific disease subtypes. For example, hierarchical clustering was able to identify pre- and post-vaccine samples in a study of the effect of an influenza vaccination on gene expression.70 Gene sets and networks Gene networks belong to the class of the unsupervised machine learning algorithms. They are undirected graphs with nodes representing genes and edges representing gene- gene associations. Genes with similar co-expression patterns are then grouped into modules using clustering techniques. Prepared usingsagej.cls6 Research Methods in Medicine& Health Sciences 0(0) Different types of co-expression networks are discussed elsewhere.71,72 Weighted gene co-expression network analysis (WGCNA)73 is a common co-expression network method that is used for finding clusters of highly correlated genes. It summarises the clusters using the representative gene (the eigengene), thus performing dimensionality reduction. The eigengene is a vector that represents the expression of all the genes in the model. WGCNA has been used to analyse metabolites in an ancillary study of vitamin D supplementation for the prevention of asthma. 74 The eigenvalues for the modules of metabolites were used to find association with asthma. Gene set enrichment (GSE) is another subtype of gene networks that clusters genes into pre-defined sets that share common biological functions, and summarises the gene expressions into a single score for each set. Scores represent the extent of the differences in gene expression between the phenotypic classes of interest, for example tumours that are responsive or non-responsive to treatment. Testing the statistical significance of the scores allows detection of an enrichment signal.75 GSE analysis has been used to compare advanced colorectal cancer subtypes in a RCT of first-line treatment of metastatic colorectal cancer. 76 Gene networks are useful for dimensionality reduction of a large number of correlated genes. To our knowledge, this method has not been used for comparing treatment arms or finding predictive biomarkers in clinical trials. Principal component analysis Principal component analysis (PCA) is a statistical technique that provides information on directions of variability in data. PCA consists of transforming high-dimensional data into a lower-dimensional set of variables (principal components) such that the first principal component (PC) is associated with the largest source of variation, the second PC with the largest remaining source of variation and so on. The procedure of computing the PCs involves computing the eigenvalues and eigenvectors for the covariance matrix of standardised data. PCs are formed by transforming the original data using a matrix constructed from the eigenvectors.77 Each PC is constructed as a linear combination of the original high-dimensional data in such a way that the PCs are mutually uncorrelated. Thus, PCs could prevent multicollinearity issues in regression models and be very useful for correlated biomarkers. Once computed, the PCs can be used as covariates in linear regression models, as well as a dimensionality reduction technique for clustering. PCA also makes high-dimensional data more suitable for visualisation. For example, PCs are widely used to identify genetic variation associated with geographic region, 78 with most geographic variation explained by the first two PCs. However, it has been shown that in the analysis of gene expression data, many more PCs might be needed to detect relevant variability, depending on the sample sizes and effect sizes.79 A challenge of PCA is the interpretation of the PCs, as well as identifying the most informative PCs. In the field of clinical trials, the use of PCA is limited to finding prognostic rather than predictive biomarkers. Adaptive signature design and risk scores Adaptive signature design methods utilise high-dimensional data to construct a low-dimensional (or scalar) signature. They combine information from multiple genetic markers to create a signature that could be used for diagnostic, prognostic or predictive purposes. Adaptive signatures are motivated by the fact that genetics play an important role in the heterogeneity of disease progression and response to treatment, and could therefore be used to facilitate personalised medicine. The original adaptive signature design constructed a low-dimensional signature based on the interaction between the treatment and the high- dimensional baseline biomarker data. 27,80 It was developed for situations with no pre-defined predictive biomarker and utilised a threshold on the number of biomarkers included in the signature. Initially, two non-overlapping groups of trial participants have been used to develop and validate the signature, 27 while later a cross-validation has been implemented, which uses patient information more efficiently.80 A few studies 81–84 construct a signature as a sum of the effects of the interactions between the treatment and each of the covariates separately. In these methods, the adaptive signature is represented by a single score for each patient. Specifically, for a binary outcome, a single covariate logistic model is fitted for each biomarker j = 1, . . . , mas follows: logit(pi) =β0 + β1ti + βj2xij + βj3tixij, where pi is the probability of the outcome of interest, βj2 represents a prognostic effect of biomarker j, and βj3 represents a predictive effect of biomarker j. A risk score for patient i (RSi) is computed as the sum of the maximum likelihood estimate of the treatment-covariate interaction coefficients ˆβj3 weighted by the value of the biomarker xij, i.e. RSi = mX j=1 ˆβj3xij. The collection of risk scores RSi for all i could be subdivided in different ways to represent different strata of patients in terms of the predicted treatment benefit. 83,84 At the end of the trial, a test is performed for the overall comparison between the arms, as well as for the comparison between the arms in the subgroup, using an α-splitting approach to control the type I error rate. Alternatively, they could be used as covariates to test for an association with the outcome.82 Adaptive signature designs often utilise a combination of the previously described approaches. For example, an adaptive signature which is predictive of response to MAGE- A3 immunotherapeutic in patients with metastatic melanoma has been developed and validated in a randomised phase II trial85 using a variation of PCA and hierarchical clustering. Another phase II trial 86 used a combination of a scoring system and the penalised approach. For each patient i, the following risk score was constructed that represented the hazard ratio under the two treatments on the logarithmic scale: RSi = log[h0(t|xi)] − log[h1(t|xi)], Prepared usingsagej.clsCherlin and Wason 7 where hj(t|xi) is the hazard rate for treatmentj = {0, 1} for patient i, and xi is the vector of gene expressions for patient i. The hazard functions were estimated with penalised Cox regression. The adaptive signature designs are applied in a post- hoc manner, i.e. they identify the subgroup of patients at the end of the trial and therefore do not fit the classical definition of an adaptive design. Rather, they are adaptive in the sense that they allow adaptive selection of patient subgroups. For example, an adaptive signature design has been proposed that finds the optimal subgroup in terms of maximising the power for identifying treatment benefit. 87 To address the issue of adaptive changes in trials, the risk scores-based adaptive signature has been utilised in the adaptive enrichment framework, where the trial population is adaptively enriched with patients who are predicted to benefit from the treatment.88 In summary, adaptive signature designs have the advantage of improving the efficiency of clinical trials by identifying enhanced benefit subgroups. More reliably identifying patient subgroups who benefit from the treatment would prevent the situation in which a potentially effective treatment is disregarded because the treatment effect in the overall population is overlooked. Moreover, adaptive signature designs have the potential to avoid patients who receive no benefit from receiving the treatment, thus preventing unnecessary exposure to possible side effects. However, adaptive signature designs come with a statistical challenge of a multiple comparisons issue. Additionally, there may be a need for dimensionality reduction in situations with a large number of baseline biomarkers.89 Current use of methods for utilising high-dimensional data in RCTs Review methods We performed the following literature search of RCTs using the PubMed database: (\"gene expression\" OR \"nucleotide*\" OR \"*omic*\" OR \"genetic signature\" OR \"SNPs\") AND (trial[Title/Abstract]) AND ((ffrft[Filter]) AND (randomizedcontrolledtrial[Filter]) AND (2019/5/1:2021/5/1[pdat])) This search, performed on June 2021, covers publication of RCTs between May 1st 2019 and May 1st 2021, with at least one of the terms: “gene expression”, “nucleotide”, “omics”, “genetic signature” or “SNPs”, appearing in the title or abstract. We included full-text articles published in English. The search identified 174 papers which were screened for eligibility. After preliminary screening of titles and abstracts, eight reviewers (SC, TB, MJG, JN, LO, FW, SFW, JMSW) independently assessed the full text of relevant publications for final inclusion. Papers were deemed eligible if they described RCTs that collected high-dimensional data. Here, data variables refer to biological variables collected at randomisation that could be used for comparing between the treatment arms or stratifying patients. For the purpose of this review, we adopted a flexible definition of high-dimensionality with respect to the number of variables. Specifically, we included studies containing at least 10 variables as they could benefit from methods suitable for high-dimensional data. An additional study that analysed 7 SNPs was included in this review as it used a multivariable approach. We analysed the type of high-dimensional data (e.g. DNA, gene expression, etc.), the number of covariates used, purpose of collecting high-dimensional data, method of analysis of high-dimensional data, clinical area, and number of treatment arms. See the Supplementary Materials for the full summary of extracted data. Results Out of the 174 papers returned, 100 (57.5%) met the inclusion criteria. A summary of the data extracted from included articles is given in Table 2. Table 2. Summary of extracted data. The denominator used to compute the percentages is 100 (number of eligible papers) unless specified. The most common answers appear in bold. Question Answer n(%) Type of high-dimensional data Gene expression 70 (70%) DNA 21 (21%) Metabolomic data 1 (1%) Multiple data types1 4 (4%) Proteomic data 3 (3%) Questionnaire 1 (1%) Number of covariates used <10 1 (1%) 10–100 41 (41%) 101–1000 20 (20%) >1000 38 (38%) Method of analysis2 Univariable approach 58 (36.3%) Multivariable approach 28 (17.5%) Gene sets and networks 12 (7.5%) Cluster analysis 18 (11.25%) Principal component analysis 17 (10.6%) Penalised regression 5 (3.1%) Risk scores 4 (2.5%) Not stated 2 (1.25%) Other3 16 (10%) Clinical area Oncology 30 (30%) Chronic diseases 28 (28%) Nutrition and ageing 18 (18%) Cardiovascular diseases 7 (7%) Other4 17 (17%) Number of treatment arms 2 79 (79%) 3 17 (17%) 4 4 (4%) 1 Questionnaires, omics data, biochemical characteristics and laboratory parameters. 2 The denominator used to compute these percentages is 160, because 42 (42%) studies used multiple methods of analysis. 3 Functional analysis, Shannon entropy and Simpson index, significance analysis of microarrays, single sample predictor classifier, SVM. 4 HIV , malaria, mental health, neuropathy, ophthalmology. Most of the articles were for clinical trials in oncology (30%) and various chronic diseases (28%), including liver, kidney, rheumatic and respiratory diseases. Other clinical areas included nutrition and ageing (18%) and cardiovascular diseases (7%). The majority of articles (70%) analysed gene expression data. The second most common type of data analysed was DNA data (21%), including genome-wide SNP data. Five percent of the articles analysed metabolomic data, protein Prepared usingsagej.cls8 Research Methods in Medicine& Health Sciences 0(0) data and data from questionnaires. Four percent of the articles analysed multiple types of data. We divided the number of analysed covariates into four categories: “ <10”, “10–100”, “101–1000”, and “ >1000”. A large proportion of the analysed articles (41%) had 10– 100 covariates available for analysis. A similar proportion of articles (38%) used>1000 covariates in their analysis. Fewer studies (20%) had 101–1000 covariates for the analysis, and one study had seven covariates, thus falling into the “ <10” category. The methods used in the analyses and their advantages are summarised in Table 1. 42% of the studies used multiple methods of analysis. The most common analysis technique was a univariable analysis (36.3%), followed by a multivariable analysis (17.5%), cluster analysis (11.25%) and PCA (10.6%). Other methods that were reported included: gene networks, multiple correspondence analysis,90 penalised approaches and risk scores, Shannon entropy and Simpson index, 91,92 significance analysis of microarrays,93 single sample predictor classifier,94 SVM. Most trials had two arms (79%), followed by three-arm (17%) and four-arm trials (4%). In this review, we only analysed RCTs and therefore single-arm studies have been excluded. The purpose of collecting high-dimensional data varied substantially between trials and was often not reported clearly. For those trials where it was reported, categorisation of the reasoning proved challenging. Some trials used high- dimensional data as the (primary or secondary) outcome by analysing the effect of the intervention on gene expression, for example. In some cases, high-dimensional data was used to explore predictive biomarkers or to compare treatment arms. In other cases, the prognostic properties of the high- dimensional data were investigated, i.e. they did not compare the treatment arms but analysed the data as if it were observational. Figures 1-3 show the distribution of different types of data, methods of analysis and clinical areas, respectively, stratified by the number of covariates. With regards to data types, most studies used gene expression or DNA and had 10-100 covariates, 100-1000 covariates or>1000 (Figure 1). Regarding analysis methods, most studies using univariable and multivariable approaches utilised 101-1000 covariates, while gene sets and networks, clustering, and PCA most commonly used 10-100 covariates (Figure 2). In oncology, chronic diseases, and nutrition and ageing, the most common number of variables was 10-100; a substantial number of studies across all clinical areas analysed a larger number of covariates (101-1000 and >1000, Figure 3). Discussion In this paper, we provided an overview of methods for analysing high-dimensional data collected in clinical trials. We also reviewed 100 recently published articles reporting RCTs that utilised high-dimensional data to identify which methods are typically used in practice. Although we focused on high-dimensional genetic data, the methods described could be applied to other types of high-dimensional data, such as questionnaires, imaging data or data from wearable technologies. Figure 1. Number of covariates per type of high-dimensional data. Figure 2. Number of covariates per method of analysis. In our search, gene expression and DNA data were the most common data analysed, covering a combined total of 91% of the high-dimensional data types included. A majority of the articles collected a large number of genetic data ( >1000 variables), which reflects the progress in high-throughput technologies and highlights the need for increased uptake of more sophisticated methods to utilise the high-dimensional data efficiently. Although most of the trials we reviewed had two-arms, over 20% had three or four arms. This reflects the additional complexity and challenges of utilising high-dimensional data in conjunction with multi-arm trials. Most clinical trials in this review were in the areas of oncology (30%) and several chronic diseases (28%). One of the challenges of trials for chronic diseases is learning how best to treat patients in the long-term. In particular, different treatments might Prepared usingsagej.clsCherlin and Wason 9 Figure 3. Number of covariates per clinical area. be used for patients at different disease stages. Therefore, efficient methods that utilise changes in high-dimensional data over time are needed, for example methods that utilise longitudinal modelling. Although we found some examples of more sophisticated methods being used to analyse high-dimensional data, the majority implemented straightforward approaches to examine interactions, such as univariable analysis. Methods such as machine learning, penalised approaches and risk scores appeared rarely in the analysis. For example, LASSO was seldom used despite being widely studied and having advantages. Adaptive signature design was not used. In some studies, high-dimensional data were measured, but only a small proportion of it was analysed. Therefore, there is strong potential for much more efficient use of high-dimensional data. We investigated the distribution of the number of covariates across data types, methods of analysis and clinical areas. The number of covariates varied widely in each of these settings, highlighting the need for developing methods that would be applicable to data of different orders of magnitude. High-dimensional data was collected for a variety of reasons, from being the primary outcome to identifying prognostic biomarkers in exploratory analysis to investigat- ing biological pathways. However, few studies used high- dimensional data to compare treatments or to identify pre- dictive biomarkers, which highlights a gap and presents an opportunity to use the data more effectively. The limited use of sophisticated methods could be explained by perceived complexities and limitations of using high-dimensional data in clinical trials. Firstly, high- dimensionality of the data still requires a prioriknowledge of the disease mechanism, in the form of existing disease classification, to efficiently reduce the dimensionality of the data.24 Secondly, there may be a discrepancy between the signature constructed from genetic data and its biological meaning, which obscures the intuitive interpretation of high- dimensional data. For example, it has been found that a large number of breast cancer signatures constructed from a variety of gene sets do not explain the biological mechanism of the disease. 95 In oncology, the most common field that collected high-dimensional data according to this review, this leads to genetic signatures being rarely used in clinical trials. It has been suggested that incorporating different types of omics data and using standardised methodology has the potential to make more effective use of high-dimensional data in clinical trials in order to improve patient outcomes.96 In this review, we have only described the methods that were used in the analysed studies. Alternative methods, such as Bayesian classifiers, 97 also have the potential to analyse high-dimensional data in clinical trials. In conclusion, although we only used a single database and limited timelines, we show that an increasing number of clinical trials are collecting high-dimensional data. Many of them could benefit from implementing more sophisticated analysis methods, such as those outlined in this manuscript. Further research is needed to make full use of the high- dimensional data collected in RCTs. Appendix Consider a hypothetical randomised placebo-controlled clinical trial of n participants with a normally distributed outcome N(µ0, σ2) for the control arm, and N(µ1, σ2) for the experimental arm, The number of participants in each group is the same ( n/2). We would like to test H0 : δ = 0 where δ = µ1 − µ0. A Wald statistic to test H0 would be W = ˆδq 4σ2 n . For a two-sided α significance level, the sample size n required for power 1 − β is n = 4(Z1−α/2 + Z1−β)2σ2 δ2 . Now suppose we have a binary biomarker that divides the population into biomarker-positive and biomarker-negative patients, with r being the proportion of biomarker-positive patients. We assume that the treatment effect is δ+ = µ1+ − µ0+ in biomarker-positive patients, and δ− = µ1− − µ0− in biomarker-negative patients. The treatment-biomarker interaction effect, δ+ − δ−, could be estimated by ˆδ+ − ˆδ− = (ˆµ1+ − ˆµ0+) − (ˆµ1− − ˆµ0−) ∼ N \u0012 δ+ − δ−, σ2 rn + σ2 rn + σ2 (1 − r)n + σ2 (1 − r)n \u0013 . A Wald statistic to test H0 : δ+ − δ− = 0would be Wint = ˆδ+ − ˆδ−q σ2 rn + σ2 rn + σ2 (1−r)n + σ2 (1−r)n = ˆδ+ − ˆδ−q 4σ2 nr(1−r) . For a two-sided α significance level, the sample size nint required for power 1 − β is nint = 4(Z1−α/2 + Z1−β)2σ2 (δ+ − δ−)2r(1 − r) . Prepared usingsagej.cls10 Research Methods in Medicine& Health Sciences 0(0) Thus, nint = n r(1−r) , i.e the sample size required to detect treatment-biomarker interaction increases by factor 1 r(1−r) , with min n r 1(1−r) o = 4for r = 0.5. Therefore, the sample size for detecting the treatment-biomarker interaction is at least four times higher than the sample size needed to detect the main treatment effect. Declaration of conflicting interests The author(s) declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article. Funding This research was supported by the Medical Research Council (MR/S014357/1). JMSW, LO and SC are funded by the National Institute for Health and Care Research (NIHR301614). logit (pi) = log [P (Yi = 1)/ {1 − P (Yi = 1)}] log {hi (t) /h0i (t)} bδ+ − bδ− = (bµ1+ − bµ0+) − (bµ1− − bµ0−) ∼ N \u0010 δ+ − δ−, σ2 rn + σ2 rn + σ2 (1−r)n + σ2 (1−r)n \u0011 References 1. Wong CH and Siah KW. Estimation of clinical trial success rates and related parameters. Biostatistics 2019; 20: 273–286. DOI:10.1093/biostatistics/kxx069. 2. Saevarsdottir S et al. Predictors of response to methotrexate in early DMARD na ¨ıve rheumatoid arthritis: results from the initial open-label phase of the SWEFOT trial. Ann Rheum Dis 2011; 70: 469–475. DOI:10.1136/ard.2010.139212. 3. van Gestel S et al. Development and validation of the european league against rheumatism response criteria for rheumatoid arthritis: Comparison with the preliminary american college of rheumatology and the world health organization/international league against rheumatism criteria. Arthritis Rheumatol1995; 39: 39–40. DOI:10.1002/art.1780390105. 4. Aslibekyan S et al. Genetic variants associated with methotrexate efficacy and toxicity in early rheumatoid arthritis: results from the treatment of early aggressive rheumatoid arthritis trial. Pharmacogenomics J 2014; 14: 48–53. DOI: 10.1038/tpj.2013.11. 5. Nelson MR et al. The support of human genetic evidence for approved drug indications.Nat Genet2015; 47: 856–862. DOI: 10.1038/ng.3314. 6. Buyse M et al. Integrating biomarkers in clinical trials. Expert Rev Mol Diagn 2011; 11: 171–182. DOI:10.1586/ERM.10. 120. 7. Visscher PM et al. 10 years of GW AS discovery: Biology, function, and translation. Am J Hum Genet2017; 101: 5–22. DOI:10.1016/j.ajhg.2017.06.005. 8. Wishart DS. Emerging applications of metabolomics in drug discovery and precision medicine. Nat Rev Drug Discov2016; 15: 473–484. DOI:10.1038/nrd.2016.32. 9. He T. Implementation of proteomics in clinical trials. Proteomics Clin Appl2019; 13: 1800198. DOI:10.1002/prca. 201800198. 10. Emilson V et al. Genetics of gene expression and its effect on disease. Nature 2008; 452: 423–428. DOI:10.1038/ nature06758. 11. Geeleher P et al. Clinical drug response can be predicted using baseline gene expression levels and in vitro drug sensitivity in cell lines. Genome Biol 2014; 15R47. DOI:10.1186/ gb-2014-15-3-r47. 12. Duffy A et al. Tissue-specific genetic features inform prediction of drug side effects in clinical trials. Sci Adv2020; 6: eabb6242. DOI:10.1126/sciadv.abb6242. 13. Shaham-Niv S et al. Metabolite medicine offers a path beyond lists of metabolites. Commun Chem 2021; 4: 225. DOI: 10.1038/s42004-021-00551-w. 14. Kaddurah-Dauok R et al. Metabolomic signatures for drug response phenotypes: pharmacometabolomics enables precision medicine. Clin Pharm Therap2015; 98: 71–75. DOI: 10.1002/cpt.134. 15. Kaddurah-Dauok R et al. Pretreatment metabotype as a predictor of response to sertraline or placebo in depressed outpatients: a proof of concept. Transl Psychiatry 2011; 1: e26–e26. DOI:10.1038/tp.2011.22. 16. Antoniou M et al. Biomarker-guided adaptive trial designs in phase II and phase III: A methodological review. PLoS One 2016; 11(2): p.e0149803. DOI:10.1371/journal.pone.0149803. 17. Gennari A et al. HER2 status and efficacy of adjuvant anthracyclines in early breast cancer: a pooled analysis of randomized trials. J Natl Cancer Inst2008; 100: 14–20. DOI: 10.1093/jnci/djm252. 18. Jiang W et al. Biomarker-adaptive threshold design: a procedure for evaluating treatment with possible biomarker- defined subset effect. J Natl Cancer Inst2007; 99: 1036–1043. DOI:10.1093/jnci/djm022. 19. Simon R. Development and validation of biomarker classifiers for treatment selection. Stat Plan Inference2014; 138:39: 308– 320. DOI:10.1016/j.jspi.2007.06.010. 20. Hu C and Dignam JJ. Biomarker-driven oncology clinical trials: Key design elements, types, features, and practical considerations. JCO Precis Oncol 2019; 1: 1–12. DOI: 10.1200/PO.19.00086. 21. Johnson DH et al. Genome-wide association study of atazanavir pharmacokinetics and hyperbilirubinemia in AIDS clinical trials group protocol A5202. Pharmaco- genet Genomics 2014; 24: 195–203. DOI:10.1097/FPC. 0000000000000034. 22. Wanda V et al. Genome-wide association study of tenofovir pharmacokinetics and creatinine clearance in AIDS clinical trials group protocol A5202. Pharmacogenet Genomics2015; 25: 450–461. DOI:10.1097/FPC.0000000000000156. 23. Wei Y et al. Confident identification of subgroups from SNP testing in RCTs with binary outcomes.Biome J2022; 64: 256– 271. DOI:10.1002/bimj.202000170. 24. Theilhaber J et al. Construction and optimization of gene expression signatures for prediction of survival in two-arm clinical trials. BMC Bioinform2020; 21: 333. DOI:10.1186/ s12859-020-03655-7. 25. Ye Y et al. Identification of a multidimensional transcriptome prognostic signature for lung adenocarcinoma. J Clin Lab Anal 2020; 33: e22990. DOI:10.1002/jcla.22990. 26. Li H et al. Development of a novel transcription factors-related prognostic signature for serous ovarian cancer. Sci Rep2021; 11: 7207. DOI:10.1038/s41598-021-86294-z. 27. Freidlin B and Simon R. Adaptive signature design: an adaptive clinical trial design for generating and prospectively testing a gene expression signature for sensitive patients. Clin Prepared usingsagej.clsCherlin and Wason 11 Cancer Res 2005; 11: 7872–7878. DOI:10.1158/1078-0432. CCR-05-0605. 28. Simon R. Biomarker based clinical trial design. Chin Clin Oncol 2014; 3(3):39. DOI:10.3978/j.issn.2304-3865.2014.02. 03. 29. Johnstone I and Titterington D. Statistical challenges of high- dimensional data. Phil Trans R Soc A2009; 367: 4237–4253. DOI:10.1098/rsta.2009.0159. 30. R Core Team. R: A language and environment for statistical computing. r foundation for statistical computing. Vienna, Austria 2021; DOI:https://www.R-project.org/. 31. Shmueli G. To explain or to predict? Stat Sci2010; 25: 289– 310. DOI:10.1214/10-STS330. 32. Herzog MH et al. The Multiple Testing Problem. Cham: Springer International Publishing. ISBN 978-3-030-03499-3, 2019. pp. 63–66. DOI:10.1007/978-3-030-03499-3 5. 33. Bland JM and Altman DG. Multiple significance tests: the Bonferroni method. Br Med J1995; 310: 170. DOI:10.1136/ bmj.310.6973.170. 34. Wang J et al. Two-stage penalized regression screening to detect biomarker-treatment interactions in randomized clinical trials. Biometrics 2021; 15: 1–10. DOI:10.1111/biom.13424. 35. Brankovic M et al. Understanding of interaction (subgroup) analysis in clinical trials. Eur J Clin Invest2019; 49: e13145. DOI:10.1111/eci.13145. 36. Brookes ST et al. Subgroup analyses in randomized trials: risks of subgroup-specific analyses; power and sample size for the interaction test. J Clin Epidemiol2004; 57: 229–236. DOI: 10.1016/j.jclinepi.2003.08.009. 37. Grant SW et al. Statistical primer: multivariable regression considerations and pitfalls. Eur J Cardiothorac Surg2019; 55: 179–185. DOI:10.1093/ejcts/ezy403. 38. Austin PC and Steyerberg EW. The number of subjects per variable required in linear regression analyses. J Clin Epidemiol 2015; 68: 627–636. DOI:10.1016/j.jclinepi.2014. 12.014. 39. Tibshirani R. Regression shrinkage and selection via the lasso. J R Stat Soc B1996; 56: 267–288. 40. Cessie SL and Houwelingen JCV . Ridge estimator in logistic regression. J R Stat Soc C1992; 41: 191–201. DOI:10.2307/ 2347628. 41. Zou H and Hastie T. Regularization and variable selection via the elastic net. J R Stat Soc B2005; 67: 301–320. DOI: 10.1111/j.14679868.2005.00503.x. 42. Baselga J et al. American society of clinical oncology: Phase II genomics study of Ixabepilone as neoadjuvant treatment for breast cancer. J Clin Oncol 2009; 27(4): 526–534. DOI: 10.1200/JCO.2007.14.2646. 43. Bloniarz A et al. Lasso adjustments of treatment effect estimates in randomized experiments. PNAS 2016; 113: 7383– 7390. 44. Tern `es N et al. Statistical methods applied to omics data: predicting response to neoadjuvant therapy in breast cancers. Curr Opin Oncol 2014; 26: 576–583. DOI:10.1097/CCO. 0000000000000134. 45. Tern `es N et al. Robust estimation of the expected survival probabilities from high-dimensional Cox models with biomarker-by-treatment interactions in randomized clinical trials. BMC Med Res Methodol2017; 17: 576–583. DOI: 10.1186/s12874-017-0354-0. 46. Yuan M and Lin Y . Model selection and estimation in regression with grouped variables. J R Stat Soc B2006; 68: 49–67. DOI:10.1111/j.1467-9868.2005.00532.x. 47. Lim M and Hastie T. Learning interactions via hierarchical group-lasso regularization. J Comput Graph Stat2015; 24: 627–652. DOI:10.1080/10618600.2014.938812. 48. Krstajic D et al. Cross-validation pitfalls when selecting and assessing regression and classification models. Journal of Cheminformatics 2014; 6, 10. DOI:/10.1186/1758-2946-6-10. 49. Jordan MI and Mitchell TM. Machine learning: trends, perspectives, and prospects. J Biopharm Stat2015; 349: 255– 260. DOI:10.1126/science.aaa8415. 50. Wei Y et al. The role of machine learning in clinical research: transforming the future of evidence generation. Trials 2021; 22:537. DOI:10.1186/s13063-021-05489-x. 51. Brieman L. Random forests. Mach Learn 2001; 45: 5–32. DOI:10.1023/A:1010933404324. 52. Reif DM et al. Integrated analysis of genetic and proteomic data identifies biomarkers associated with adverse events following smallpox vaccination. Genes Immun2009; 10: 112– 119. 53. Foster JC et al. Subgroup identification from randomized clinical trial data. Stat Med 2011; 30: 2867–2880. DOI: 10.1002/sim.4322. 54. Ubels J et al. RAINFOREST: a random forest approach to predict treatment benefit in data from (failed) clinical drug trials. Bioinformatics 2020; 36(26): i601–i609. DOI:10.1093/ bioinformatics/btaa799. 55. Vapnik V . The nature of statistical learning theory. New York: Springer, 1995. ISBN 978-1-4757-3264-1. 56. Hua S and Sung Z. Support vector machine approach for protein subcellular localization prediction. Bioinformatics 2001; 17: 721–728. DOI:10.1093/bioinformatics/17.8.721. 57. Dror G et al. Accurate identification of alternatively spliced exons using support vector machine. Bioinformatics 2005; 21: 879–901. DOI:10.1093/bioinformatics/bti132. 58. Liu J et al. Distinguishing protein-coding from non-coding RNAs through support vector machines. PLoS Genet 2006; 2: e29. DOI:10.1371/journal.pgen.0020029. 59. Ng KLS and Mishra SK. De novo SVM classification of precursor micrornas from genomic pseudo hairpins using global and intrinsic folding measures. Bioinformatics 2007; 23: 1321–1330. DOI:10.1093/bioinformatics/btm026. 60. Huang S et al. Applications of support vector machine (SVM) learning in cancer genomics. Cancer Genomics Proteomics 2018; 15: 41–51. DOI:10.21873/cgp.20063. 61. Kohannim O et al. Boosting power for clinical trials using classifiers based on multiple biomarkers. Neurobiol Aging 2011; 31: 1429–1442. DOI:10.1016/j.neurobiolaging.2010.04. 022. 62. Reynolds AP et al. Clustering rules: a comparison of partitioning and hierarchical clustering algorithms. J Math Model Algorithms 2006; 5: 475–504. DOI:10.1007/ s10852-005-9022-1. 63. Hartigan JA. Clustering Algorithms. New York: John Wiley & Sons, 1975. ISBN 0-471-35645-X. 64. Kaufman L and Rousseeuw P. Finding Groups in Data: An Introduction to Cluster Analysis. New York: John Wiley & Sons, 2009. 65. Monti S et al. Consensus clustering: A resampling-based method for class discovery and visualization of gene expression Prepared usingsagej.cls12 Research Methods in Medicine& Health Sciences 0(0) microarray data. Mach Learn2003; 52: 91–118. DOI:10.1023/ A:1023949509487. 66. Jiang D et al. Cluster analysis for gene expression data: A survey. IEEE Trans Knowl Data Eng2004; 16: 1370–1386. DOI:10.1109/TKDE.2004.68. 67. D’haeseleer P. How does gene expression clustering work? Nat Biotechnol 2005; 23: 1499–1501. DOI:10.1038/nbt1205-1499. 68. Vavoulis D et al. DGEclust: differential expression analysis of clustered count data. Genome Biol 2015; 16: 39. DOI: 10.1186/s13059-015-0604-6. 69. Oyelade J et al. Optimized adaptive enrichment designs. Bioinform Biol Insights2016; 10: 237–253. DOI:10.4137/BBI. S38316. 70. Drury R et al. The effect of H1N1 vaccination on serum miRNA expression in children: A tale of caution for microRNA microarray studies. PLoS One 2019; 14: e0221143. DOI: 10.1371/journal.pone.0221143. 71. Zhang B and Horvath S. A general framework for weighted gene co-expression network analysis. Stat Appl Genet Mol Biol 2005; 3: Article17. DOI:10.2202/1544-6115.1128. 72. van Dam S et al. Gene co-expression analysis for functional classification and gene–disease predictions. Brief Bioinform 2018; 19(4): 575–592. DOI:10.1093/bib/bbw139. 73. Langfelder P and Horvath S. WGCNA: an R package for weighted correlation network analysis. BMC Bioinform2008; 9: 559. DOI:10.1186/1471-2105-9-559. 74. Lee-Sarwar KA et al. Integrative analysis of the intestinal metabolome of childhood asthma. AAAAI 2019; 144: 442–454. DOI:10.1016/j.jaci.2019.02.032. 75. Subramanian A et al. Gene set enrichment analysis: A knowledge-based approach for interpreting genome-wide expression profiles. PNAS 2005; 102: 15545–15550. DOI: 10.1073/pnas.0506580102. 76. Takahashi S et al. Advanced colorectal cancer subtypes (aCRCS) help select oxaliplatin-based or irinotecan-based therapy for colorectal cancer. Cancer Sci 2021; 112: 1567– 1578. DOI:10.1111/cas.14841. 77. Jolliffe IT and Cadima J. Principal component analysis: a review and recent developments. Phil Trans R Soc A2016; 374: 20150202. DOI:10.1098/rsta.2015.0202. 78. Abegaz F et al. Principals about principal components in statistical genetics. Brief Bioinform 2019; 20(6): 2200–2216. DOI:10.1093/bib/bby081. 79. Lenz M et al. Principal components analysis and the reported low intrinsic dimensionality of gene expression microarray data. Sci Rep2019; 6: 25696. DOI:10.1038/srep25696. 80. Freidlin B et al. The cross-validated adaptive signature design. Clin Cancer Res2010; 16: 691–698. DOI:10.1158/1078-0432. CCR-09-1357. 81. Radmacher MD et al. A paradigm for class prediction using gene expression profiles. J Comput Biol 2002; 9: 404–511. DOI:10.1089/106652702760138592. 82. Matsui S et al. Developing and validating continuous genomic signatures in randomized clinical trials for predictive medicine. Clin Cancer Res 2012; 18: 6065–6073. DOI: 10.1158/1078-0432.CCR-12-1206. 83. Cherlin S and Wason JMS. Developing and testing high- efficacy patient subgroups within a clinical trial using risk scores. Stat Med 2020; 39: 3285–3298. DOI:10.1002/sim. 8665. 84. Cherlin S and Wason JMS. Developing a predictive signature for two trial endpoints using the cross-validated risk scores method. Biostatistics 2021; DOI:10.1093/biostatistics/ kxaa055. 85. Ulloa-Montoya F et al. Predictive gene signature in MAGE-A3 antigen-specific cancer immunotherapy.J Clin Oncol2013; 31: 2388–2395. DOI:10.1200/JCO.2012.44.3762. 86. Dreno B. MAGE-A3 immunotherapeutic as adjuvant therapy for patients with resected, MAGE-A3-positive, stage III melanoma (DERMA): a double-blind, randomised, placebo- controlled, phase 3 trial. Lancet Oncol 2018; 19: 916–929. DOI:10.1016/S1470-2045(18)30254-7. 87. Zhang Z et al. Subgroup selection in adaptive signature designs of confirmatory clinical trial.J R Stat Soc C2017; 66: 345–361. 88. Cherlin S and Wason JMS. Cross-validated risk scores adaptive enrichment (CADEN) design. https://arxivorg/abs/211102299 2021; . 89. Bhattacharyyaa A and Rai SN. Adaptive signature design- review of the biomarker guided adaptive phase–III controlled design. Contemp Clin Trials Commun2019; 15: 100378. DOI: 10.1016/j.conctc.2019.100378. 90. Greenacre M and Blasius J. Multiple Correspondence Analysis and Related Methods. Chapman and Hall/CRC, 2006. ISBN 9780429141966. DOI:10.1201/9781420011319. 91. Shannon C. A mathematical theory of communication. Bell Syst Tech J1948; 27: 379–423. 92. Simpson E. Measurement of diversity. Nature 1949; 163: 688. DOI:10.1038/163688a0. 93. Tusher VG et al. Significance analysis of microarrays applied to the ionizing radiation response. PNAS 2001; 98: 5116–5121. DOI:10.1073ypnas.091062498. 94. Guinney J et al. The consensus molecular subtypes of colorectal cancer. Nat Med 2015; 21: 1350–1356. DOI: 10.1038/nm.3967. 95. Manjang K et al. Prognostic gene expression signatures of breast cancer are lacking a sensible biological meaning. Sci Rep 2021; 11: 156. DOI:10.1038/s41598-020-79375-y. 96. Qian Y et al. Prognostic cancer gene expression signatures: current status and challenges. Cells 2021; 10: 648. DOI: 10.3390/cells10030648. 97. Lampimen J and Vehtari A. Bayesian approach for neural networks–review and case studies.Neural Netw2001; 14: 257– 274. DOI:10.1016/S0893-6080(00)00098-8. Prepared usingsagej.cls",
      "references": [
        "Estimation of clinical trial success rates and related parameters.",
        "Predictors of response to methotrexate in early DMARD na ¨ıve rheumatoid arthritis: results from the initial open-label phase of the SWEFOT trial.",
        "Development and validation of the european league against rheumatism response criteria for rheumatoid arthritis: Comparison with the preliminary american college of rheumatology and the world health organization/international league against rheumatism criteria.",
        "Genetic variants associated with methotrexate efficacy and toxicity in early rheumatoid arthritis: results from the treatment of early aggressive rheumatoid arthritis trial.",
        "The support of human genetic evidence for approved drug indications.",
        "Integrating biomarkers in clinical trials.",
        "10 years of GW AS discovery: Biology, function, and translation.",
        "Emerging applications of metabolomics in drug discovery and precision medicine.",
        "Implementation of proteomics in clinical trials.",
        "Genetics of gene expression and its effect on disease.",
        "Clinical drug response can be predicted using baseline gene expression levels and in vitro drug sensitivity in cell lines.",
        "Tissue-specific genetic features inform prediction of drug side effects in clinical trials.",
        "Metabolite medicine offers a path beyond lists of metabolites.",
        "Metabolomic signatures for drug response phenotypes: pharmacometabolomics enables precision medicine.",
        "Pretreatment metabotype as a predictor of response to sertraline or placebo in depressed outpatients: a proof of concept.",
        "Biomarker-guided adaptive trial designs in phase II and phase III: A methodological review.",
        "HER2 status and efficacy of adjuvant anthracyclines in early breast cancer: a pooled analysis of randomized trials.",
        "Biomarker-adaptive threshold design: a procedure for evaluating treatment with possible biomarker- defined subset effect.",
        "Development and validation of biomarker classifiers for treatment selection.",
        "Biomarker-driven oncology clinical trials: Key design elements, types, features, and practical considerations.",
        "Genome-wide association study of atazanavir pharmacokinetics and hyperbilirubinemia in AIDS clinical trials group protocol A5202.",
        "Genome-wide association study of tenofovir pharmacokinetics and creatinine clearance in AIDS clinical trials group protocol A5202.",
        "Confident identification of subgroups from SNP testing in RCTs with binary outcomes.",
        "Construction and optimization of gene expression signatures for prediction of survival in two-arm clinical trials.",
        "Identification of a multidimensional transcriptome prognostic signature for lung adenocarcinoma.",
        "Development of a novel transcription factors-related prognostic signature for serous ovarian cancer.",
        "Adaptive signature design: an adaptive clinical trial design for generating and prospectively testing a gene expression signature for sensitive patients.",
        "Biomarker based clinical trial design.",
        "Statistical challenges of high- dimensional data.",
        "R: A language and environment for statistical computing.",
        "To explain or to predict?",
        "The Multiple Testing Problem.",
        "Multiple significance tests: the Bonferroni method.",
        "Two-stage penalized regression screening to detect biomarker-treatment interactions in randomized clinical trials.",
        "Understanding of interaction (subgroup) analysis in clinical trials.",
        "Subgroup analyses in randomized trials: risks of subgroup-specific analyses; power and sample size for the interaction test.",
        "Statistical primer: multivariable regression considerations and pitfalls.",
        "The number of subjects per variable required in linear regression analyses.",
        "Regression shrinkage and selection via the lasso.",
        "Ridge estimator in logistic regression.",
        "Regularization and variable selection via the elastic net.",
        "American society of clinical oncology: Phase II genomics study of Ixabepilone as neoadjuvant treatment for breast cancer.",
        "Lasso adjustments of treatment effect estimates in randomized experiments.",
        "Statistical methods applied to omics data: predicting response to neoadjuvant therapy in breast cancers.",
        "Robust estimation of the expected survival probabilities from high-dimensional Cox models with biomarker-by-treatment interactions in randomized clinical trials.",
        "Model selection and estimation in regression with grouped variables.",
        "Learning interactions via hierarchical group-lasso regularization.",
        "Cross-validation pitfalls when selecting and assessing regression and classification models.",
        "Machine learning: trends, perspectives, and prospects.",
        "The role of machine learning in clinical research: transforming the future of evidence generation.",
        "Random forests.",
        "Integrated analysis of genetic and proteomic data identifies biomarkers associated with adverse events following smallpox vaccination.",
        "Subgroup identification from randomized clinical trial data.",
        "RAINFOREST: a random forest approach to predict treatment benefit in data from (failed) clinical drug trials.",
        "The nature of statistical learning theory.",
        "Support vector machine approach for protein subcellular localization prediction.",
        "Accurate identification of alternatively spliced exons using support vector machine.",
        "Distinguishing protein-coding from non-coding RNAs through support vector machines.",
        "De novo SVM classification of precursor micrornas from genomic pseudo hairpins using global and intrinsic folding measures.",
        "Applications of support vector machine (SVM) learning in cancer genomics.",
        "Boosting power for clinical trials using classifiers based on multiple biomarkers.",
        "Clustering rules: a comparison of partitioning and hierarchical clustering algorithms.",
        "Clustering Algorithms.",
        "Finding Groups in Data: An Introduction to Cluster Analysis.",
        "Consensus clustering: A resampling-based method for class discovery and visualization of gene expression microarray data.",
        "Cluster analysis for gene expression data: A survey.",
        "How does gene expression clustering work?",
        "DGEclust: differential expression analysis of clustered count data.",
        "Optimized adaptive enrichment designs.",
        "The effect of H1N1 vaccination on serum miRNA expression in children: A tale of caution for microRNA microarray studies.",
        "A general framework for weighted gene co-expression network analysis.",
        "Gene co-expression analysis for functional classification and gene–disease predictions.",
        "WGCNA: an R package for weighted correlation network analysis.",
        "Integrative analysis of the intestinal metabolome of childhood asthma.",
        "Gene set enrichment analysis: A knowledge-based approach for interpreting genome-wide expression profiles.",
        "Advanced colorectal cancer subtypes (aCRCS) help select oxaliplatin-based or irinotecan-based therapy for colorectal cancer.",
        "Principal component analysis: a review and recent developments.",
        "Principals about principal components in statistical genetics.",
        "Principal components analysis and the reported low intrinsic dimensionality of gene expression microarray data.",
        "The cross-validated adaptive signature design.",
        "A paradigm for class prediction using gene expression profiles.",
        "Developing and validating continuous genomic signatures in randomized clinical trials for predictive medicine.",
        "Developing and testing high- efficacy patient subgroups within a clinical trial using risk scores.",
        "Developing a predictive signature for two trial endpoints using the cross-validated risk scores method.",
        "Predictive gene signature in MAGE-A3 antigen-specific cancer immunotherapy.",
        "MAGE-A3 immunotherapeutic as adjuvant therapy for patients with resected, MAGE-A3-positive, stage III melanoma (DERMA): a double-blind, randomised, placebo- controlled, phase 3 trial.",
        "Subgroup selection in adaptive signature designs of confirmatory clinical trial.",
        "Cross-validated risk scores adaptive enrichment (CADEN) design.",
        "Adaptive signature design- review of the biomarker guided adaptive phase–III controlled design.",
        "Multiple Correspondence Analysis and Related Methods.",
        "A mathematical theory of communication.",
        "Measurement of diversity.",
        "Significance analysis of microarrays applied to the ionizing radiation response.",
        "The consensus molecular subtypes of colorectal cancer.",
        "Prognostic gene expression signatures of breast cancer are lacking a sensible biological meaning.",
        "Prognostic cancer gene expression signatures: current status and challenges.",
        "Bayesian approach for neural networks–review and case studies."
      ],
      "meta_data": {
        "arxiv_id": "2305.10174v2",
        "authors": [
          "Svetlana Cherlin",
          "Theophile Bigirumurame",
          "Michael J Grayling",
          "Jérémie Nsengimana",
          "Luke Ouma",
          "Aida Santaolalla",
          "Fang Wan",
          "S Faye Williamson",
          "James M S Wason"
        ],
        "published_date": "2023-05-17T12:59:05Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Reviews statistical/machine-learning methods for leveraging high-dimensional (mainly omics/genetic) baseline data in randomized clinical trials (RCTs) to improve efficiency, understand prognosis/prediction, and identify treatment-benefiting subgroups; and audits recent practice (2019–2021 PubMed RCTs) to quantify what data types and analytic methods are actually used. Key findings: among 174 screened papers, 100 eligible RCTs collected high-dimensional data; oncology and chronic diseases dominate; gene expression (70%) and DNA/SNP data (21%) are most common; despite availability of sophisticated methods, analyses most often rely on univariable testing (36.3% of reported methods) and standard multivariable models, with rare use of penalized regression, machine learning, risk scores, and essentially no adaptive signature designs—indicating under-utilization and methodological/practical barriers.",
        "methodology": "Narrative methodological overview plus a structured literature review. Method overview: (i) univariable regression with treatment–biomarker interaction tests (linear/logistic/Cox) and multiple-testing control (Bonferroni; mention of two-stage screening + penalized regression); (ii) multivariable regression with many biomarkers and interaction terms; (iii) regularized/penalized regression (LASSO, ridge, elastic net; group lasso; tuning via (nested) cross-validation); (iv) supervised ML for prediction and subgroup/treatment benefit modeling (random forests, including Virtual Twins and survival-splitting variants; SVM with kernels); (v) unsupervised learning for structure/dimensionality reduction (cluster analysis; gene sets/networks including WGCNA and GSEA; PCA); (vi) adaptive signature designs/risk scores combining interaction effects into a scalar signature and testing with alpha-splitting / cross-validation; includes sample-size considerations for interaction testing (interaction tests can require ≥4× sample size when biomarker prevalence is 0.5).",
        "experimental_setup": "Literature review setup: PubMed search (June 2021) with query (\"gene expression\" OR \"nucleotide*\" OR \"*omic*\" OR \"genetic signature\" OR \"SNPs\") AND trial[Title/Abstract], filtered to randomized controlled trials, full free text, English, publication date 2019/5/1–2021/5/1. Screening: 174 records; 8 reviewers independently full-text assessed. Inclusion: RCTs collecting baseline biological variables considered high-dimensional (flexible threshold ≥10 variables; one study with 7 SNPs included due to multivariable analysis). Data extracted: data type, number of covariates (<10, 10–100, 101–1000, >1000), purpose, analytic methods used (multiple allowed; hence method-count denominator 160), clinical area, number of arms. Descriptive synthesis only; no meta-analysis or re-analysis of trial data. Outcome: 100 eligible papers; most two-arm (79%), 3-arm (17%), 4-arm (4%).",
        "limitations": "Methodological review limitations: not exhaustive; single database (PubMed), narrow 2-year window, English-only and free-full-text filter may bias included studies; definition of high-dimensionality (≥10 variables) is pragmatic and may misclassify. Practice review limitations: relies on reporting quality; purpose of collecting high-dimensional data often unclear; categorization of methods/purpose challenging; counts treat multiple methods per paper (denominator 160) complicating interpretation; no assessment of analytic correctness, robustness, or comparative performance. Broader field limitations highlighted: multiple testing inflation in univariable approaches; low power and large sample size needs for interaction/predictive biomarker detection; multivariable models prone to overfitting/curse of dimensionality; penalized/ML methods require careful tuning (nested CV) and large samples; partitioning for CV must preserve key features; interpretability challenges for ML/PCA/signatures; biological meaning of derived signatures may be weak; need for prior biological knowledge to guide dimensionality reduction; limited uptake in multi-arm and longitudinal settings.",
        "future_research_directions": "Develop and disseminate trial-focused methods that better exploit high-dimensional data for treatment comparison and predictive biomarker discovery (not just prognostic/observational analyses); increase use of regularization, ML, and adaptive signature/risk-score approaches with rigorous validation and type-I error control; create approaches scalable across variable counts (10s to >1000) and across multi-arm trials; incorporate longitudinal high-dimensional measurements (e.g., joint/longitudinal models, dynamic signatures) especially for chronic diseases; improve interpretability/biological grounding of signatures (pathway-informed/group penalties, gene networks, multi-omics integration) and standardize analysis pipelines; address practical barriers—sample size planning for interaction effects, nested cross-validation, robust subgroup discovery, and reporting standards for objectives and analysis of high-dimensional endpoints/biomarkers.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Self Iterative Label Refinement via Robust Unlabeled Learning",
      "full_text": "IFT-UAM/CSIC-23-114 Hadronic physics from a Wilson fermion mixed-action approach: Charm quark mass and D(s) meson decay constants Andrea Bussonea, Alessandro Coniglib,c, Julien Frisond, Gregorio Herdo´ ızab,c, Carlos Penab,c, David Pretie, Alejandro S´ aezb,c, Javier Ugarriob,c a Humboldt Universit¨ at zu Berlin, Institut f¨ ur Physik and IRIS Adlershof Zum Großen Windkanal 6, 12489 Berlin, Germany b Instituto de F´ ısica Te´ orica UAM-CSIC c/ Nicol´ as Cabrera 13-15, Universidad Aut´ onoma de Madrid Cantoblanco E-28049 Madrid, Spain c Dpto. de F´ ısica Te´ orica, Universidad Aut´ onoma de Madrid Cantoblanco E-28049 Madrid, Spain d John von Neumann-Institut f¨ ur Computing NIC, Deutsches Elektronen-Synchrotron DESY Platanenallee 6, 15738 Zeuthen, Germany e INFN, Sezione di Torino Via Pietro Giuria 1, I-10125 Turin, Italy Abstract: We present our first set of results for charm physics, using the mixed-action setup introduced in a companion paper [1]. Maximally twisted Wilson valence fermions are used on a sea of non-perturbatively O(a)-improved Wilson fermions, made up by CLS Nf = 2 + 1 ensem- bles. Our charm-sector observables are free from O(amc) discretisation effects, without need of tuning any improvement coefficient, and show continuum-limit scaling properties consistent with leading cutoff effects of O(a2). We consider a subset of CLS ensembles – including four values of the lattice spacing and pion masses down to 200 MeV – allowing to take the continuum limit and extrapolate to the physical pion mass. A number of techniques are incorporated in the analysis in order to estimate the systematic uncertainties of our results for the charm quark mass and the D(s)-meson decay constants. This first study of observables in the charm sector, where the emphasis has been on the control of the methodology, demonstrates the potential of our setup to achieve high-precision results. arXiv:2309.14154v1  [hep-lat]  25 Sep 20231 Introduction Heavy flavour physics is a key frontline in the endeavour to test the limits of the Standard Model, and look for new fundamental physics. Ever-increasing precision for fundamental pa- rameters such as quark masses and Cabibbo-Kobayashi-Maskawa matrix elements, as well as for weak matrix elements that control the low-energy hadronic contribution to weak decay amplitudes, is necessary to keep pace with experimental developments. First-principles, systematically improvable computations performed in Lattice QCD — possibly, beyond a certain precision threshold, with QED corrections — are of course the basic source of input. When dealing with heavy quark physics, however, lattice computations face a non-trivial multiscale problem. Since computations involve both an ultraviolet cutoff — the inverse lattice spacing a−1 — and an infrared cutoff — the inverse size L−1 of the finite box computations are performed in — all physical scales should best fit comfortably between the cutoffs, lest control on their removal is compromised. A standard criterion for finite-volume effects to be sufficiently suppressed in typical hadronic quantities involves a constraint on the box size mπL ≳ 4; for the typical range of pion masses explored, which nowadays routinely reaches the physical point, this implies box sizes in the 3 to 7 fm ballpark. Having mc ≪ a−1, and especially, mb ≪ a−1, then requires values of the lattice size L/a that are close to or simply beyond current computational capabilities. This problem is much worsened by the extra difficulty to approach the very fine lattice spacings needed to accommodate heavy quark masses: the computational cost of typical simulations scales as ∼ a−7 [2], and for a ≲ 0.05 fm the algorithmic problem of topology freezing sets in, which in practice impedes simulations long enough to control statistical uncertainties reliably [3]. Facing these problems requires a specific toolset for heavy quark physics on the lattice, that, in particular, relies on input from effective theories to try and control the ultraviolet cutoff dependence: Symanzik effective theory [4–7] to understand and suppress the leading cutoff effects, heavy quark effective theory input to guide the construction of lattice actions or the extraction of physics,1 etc. The resulting sophisticated frameworks often rely on assumptions about the systematic impact of the use of effective theory, and/or require the determination of ancillary quantities such as O(a) improvement coefficients. A full overview of lattice techniques and results for heavy quark physics can be obtained from the latest FLAG review [9]. A main theme underpinning all studies in the charm and, especially, the B sector is that having results from a variety of approaches is essential to gain confidence on the systematic uncertainties affecting hadronic observables relevant for flavour physics. The main motivation of the mixed-action setup used in this work, and fully discussed in [1], is to devise an optimal framework for heavy quark physics that bypasses many of the difficulties mentioned above. The first ingredient is the use of CLS Nf = 2 + 1 ensembles obtained with non-perturbatively O(a) improved Wilson fermions [10] and open boundary conditions for the gauge field [11, 12], which allows to enter the realm of very fine lattice spacings while keeping control on statistical uncertainties. The second ingredient is to compute heavy quark observables by means of a valence twisted-mass Wilson setup [13,14], which leads to automatic O(a) improvement [15]. Working with a mixed action of course leads to new requirements, such a precise matching between the valence and sea sectors, and a careful analysis of the relative cutoff effects. This is discussed in the companion paper [1]. Here we will focus on illustrating how the technique is able to obtain precise, reliable results for basic observables in the hadronic 1See, e.g., App. A.1.3 of [8] for a summary of existing approaches. 1sector. Progress report of this long-term project have been given in [16–23]. In particular, we will focus on determining the value of the charm quark mass, and of the leptonic decay constants of the D and Ds mesons. Our results are based on a subset of the available CLS ensembles which allow us to illustrate the properties of the setup. We also emphasise our development of a variant of the existing techniques to assess systematic uncertainties in lattice observables based on information criteria [24,25] applied to appropriate goodness-of-fit estimators [26]. Still, despite the fact that our current results use a subset of the CLS ensembles, they are already at a point where they have competing precision in the context of the state-of-the-art determination of these quantities that enter current FLAG averages [27–45]. Results with a larger set of CLS ensembles, including finer lattice spacings and physical pion mass ensembles, will be the object of upcoming publications. Let us conclude this introduction by describing the organisation of the paper. Sec. 2 summarises the main aspects of our mixed-action approach, discussed at length in [1]. Sec. 3 deals with our approach to matching the scale of our partially-quenched charm quark, and numerical aspects of computations in the charm sector. Secs. 4 and 5 discuss our determination of the charm mass and decay constants, respectively. Finally, Sec. 6 contains our conclusions and outlook. 2 Mixed-action setup In this section we review the basic features of our setup, with an emphasis on their implications for heavy quark physics. We refer the reader to [1] for a fully detailed discussion of our approach. 2.1 Generalities All our results stem from a mixed-action setup. In the sea sector we employ a tree-level im- proved gauge action [6,46], and a non-perturbatively O(a)-improved Wilson fermion action [47]. This has indeed been used in the generation of the CLS Nf = 2 + 1 ensembles [10,48–50] that we employ. In the valence sector, on the other hand, we use a fully-twisted tmQCD [13] fermion action. Both actions include the same massless Wilson-Dirac operator [47,51] D = 1 2γµ(∇∗ µ + ∇µ) − a 2∇∗ µ∇µ + i 4 acswσµν ˆFµν, (2.1) where ∇µ and ∇∗ µ are, respectively, the forward and backward covariant derivatives, σµν = i 2 [γµ, γν], and ˆFµν is the clover-leaf definition of the field strength tensor as spelled out in [7]. The mass term in the sea has the form ¯ψm(s)ψ , (2.2) while the tmQCD action is obtained by adding a mass term of the form [13,14] 2 i ¯ψµγ5ψ + mcr ¯ψψ ; µ = diag(µu, −µd, −µs, µc) , (2.3) 2While, other, versions of the valence sector ` a la Osterwalder-Seiler [52] can be used without substantial changes to the discussion below, in this work the form in Eq. (2.3) will suffice to extract all the relevant physics, and we will therefore stick to it for definiteness. 2where mcr is the standard Wilson critical mass, and the signs have been set so that the values of the twisted masses µf are implied to be positive. We will always work in the isospin limit, where the up and down quark masses take the same values both in the sea and in the valence (i.e., m(s) u = m(s) d and µu = µd ≡ µl). The procedure to fully define the mixed action involves the matching between Wilson and tmQCD valence actions, and a specific prescription to define the critical mass used in our setup. To that purpose, for any given ensemble we first tune µl, µs and mcr such that the quantities ϕ2 and ϕ4 – depending on pion and kaon masses, as defined in Eq. (2.6) – coincide for sea and valence actions, while imposing that the (u,d) standard PCAC quark mass – including all known O( a)-improvement counterterms – vanishes in the valence sector. This ensures equivalent physics and sets the twist angle to π/2, ensemble by ensemble. 2.2 Properties of the twisted valence sector The most interesting property of this setup for the purpose of the results presented in this paper is that it results in automatic O( a) improvement of observables extracted from valence correlation functions [15], up to terms proportional to the trace of the subtracted sea quark mass matrix, atr{m(s) q } [1]. Since the latter only involves up, down, and strange quarks, the value of the trace in lattice units is of O(10−2) on our ensembles. Furthermore, these terms arise from loop effects, and their coefficient is thus formally of perturbative orderα2 s . Given our typical statistical uncertainties, the natural size of these atr{m(s) q } lattice artefacts therefore amounts to a subdominant contribution. This property can be furthermore verified a posteriori by inspecting the scaling of observables towards the continuum limit. This is very important for heavy quark observables, since we are then assured that the leading cutoff effects associated to a quark of mass µh are of order (aµh)2, without need of fine-tuning improvement coefficients to ensure the elimination of linear effects, as would be the case in the standard O( a) improved setup. Note that automatic O( a) improvement holds even in the absence of the clover term in the valence fermion action; we have however kept it for a number of reasons. First, it simplifies the matching between sea and valence, since the regularisations coincide in the chiral limit. Secondly, for the same reason, it allows to use non-perturbative renormalisation constants determined with standard methods — e.g., to obtain renormalised quark masses [53]. Finally, it has been observed that keeping the clover term leads to a better control over the O( a2) flavour-breaking effects induced by the twisted mass term, thus improving the overall scaling of the setup [54,55]. A second, more generic benefit is that the use of a twisted mass regularisation implies multiplicative renormalisation of (twisted) quark masses, and the possibility to determine decay constants without need of finite normalisation factors such asZA. This is a result of the explicit chiral symmetry breaking pattern at full twist, which leaves exactly conserved axial currents. In the twisted quark field basis implicitly assumed when writing our valence mass terms, the relevant on-shell (x ̸= 0) Ward-Takahashi identity reads ⟨∂∗ µ ˜V qr µ (x) O(0)⟩ = i(µq + µr)⟨Pqr(x) O(0)⟩, (2.4) where ∂∗ µ is the backward lattice derivative; O is any gauge-invariant local operator; µq,r are the Lagrangian twisted masses for the corresponding flavours q, r, that are here assumed to 3carry different signs in the twisted mass matrix µ of Eq. (2.3);3 Pqr = ¯ψqγ5ψr is a non-singlet pseudoscalar density; and ˜V qr µ is the point-split vector current 4 ˜V qr µ (x) = 1 2 \u0014 ψ q (x)(γµ − 1)Uµ(x)ψr(x + aˆµ) + ψ q (x + aˆµ)(γµ + 1)U† µ(x)ψr(x) \u0015 . (2.5) Since the current is exactly conserved, there are two important consequences. First, current and Lagrangian quark masses coincide, and renormalise with Zµ = Z−1 P .5 Second, meson decay constants can be extracted from a two-point function of the pseudoscalar density, by setting O = Prq in Eq. (2.4) and using the fact that the l.h.s. of the Ward identity is exactly normalised. These will be the basis of our determinations of the charm quark mass in Sec. 3 and of fD(s) in Sec. 5. 2.3 Ensembles and line of constant physics CLS ensembles have been generated along three different lines of constant physics. Our results are based on a subset of the ensembles generated at (approximately) constant value of tr{m(s) q }, which we list in Table 1. In order to define a precise line of constant physics, we use the quantities ϕ2 ≡ 8t0m2 π , ϕ 4 ≡ 8t0 \u00121 2m2 π + m2 K \u0013 , (2.6) where t0 is the gradient flow scale introduced in [57], and whose value in physical units has already been determined using CLS ensembles in [1,48,58,59]. A renormalised line of constant physics can thus be fixed by setting ϕ4 constant and equal to its physical value; extraction of the physics will then proceed by a combined continuum-chiral limit fit that hits the physical value of ϕ2. The condition that ϕ4 is constant is well approximated by keeping tr {m(s)} fixed, since it is proportional to ϕ4 at leading order in the effective chiral description of QCD dynamics. Small deviations from the correct value of ϕ4 in each ensemble can be corrected by means of the mass shifting prescription introduced in [48], and incorporated into the fitting procedure — see [1] for technical details. Our renormalised chiral trajectory is ultimately set at ϕphys 4 = ϕisoQCD 4 = 1.101(7)(5), where the second error quoted is the systematic uncertainty coming from our Bayesian model averaging (see below), and the first error comprises the statistical uncertainty, the one associated to chiral-continuum extrapolations, and those related to input parameters — improvement coefficients, renormalisation constants, and the input pion and kaon masses. The values of the latter employed to fix ϕ4 are those in the QCD isospin symmetric limit (isoQCD) given by [9] misoQCD π = 134 .9768(5) MeV, (2.7) misoQCD K = 497 .611(13) MeV. (2.8) In the remainder of this paper we will use the superscript “phys” for quantities defined in the isoQCD prescription for the continuum theory, as fixed above. 3With our conventions, this applies to any of the pairs ( u, d), (u, s), (d, c) and (s, c). 4This is indeed the physical axial current, chirally rotated by the relation between physical and twisted quark variables — see, e.g., [13,56]. 5It can be separately proven that renormalisation is indeed multiplicative. 4Id β L/a T/a κ l κs mπ [MeV] mK [MeV] mπL H101 3.40 32 96 0.13675962 0.13675962 416 416 5.8 H102 32 96 0.136865 0.136549339 352 437 4.9 H105 32 96 0.136970 0.13634079 277 462 3.9 H400 3.46 32 96 0.13688848 0.13688848 415 415 5.1 N202 3.55 48 128 0.137000 0.137000 412 412 6.4 N203 48 128 0.137080 0.136840284 346 442 5.4 N200 48 128 0.137140 0.13672086 284 463 4.4 D200 64 128 0.137200 0.136601748 200 480 4.2 N300 3.70 48 128 0.137000 0.136601748 419 419 5.1 J303 64 196 0.137123 0.1367546608 257 474 4.1 Table 1: List of CLS Nf = 2 + 1 ensembles used in the present study. L/a and T/a refer to the spatial and temporal extent respectively of the lattice. The values κl and κs refer to the hopping parameters of the light and strange quark masses in the sea sector. Approximate values of the pion mass mπ, the kaon mass mK, and of the product mπL are provided in the last three columns. In this work we employ our determination of the physical scale from the gradient flow scale t0. To set the scale, we use the following combination of pion and kaon decay constants √ 8t0fπK = √ 8t0 \u00122 3fK + 1 3fπ \u0013 . (2.9) At NLO in SU (3) χPT, this quantity remains constant up to logarithmic terms. From the chiral-continuum extrapolated value of √8t0fπK we eventually extract the flow scale t0 in physical units by using as physical inputs the isoQCD values for fπ and fK. Specifically, we use [9] fisoQCD π = 130 .56(13) MeV, (2.10) fisoQCD K = 157 .2(5) MeV. (2.11) The full details of our scale setting procedure through a combination of the O(a)-improved Wilson results with the ones from the valence Wilson Twisted Mass regularisation can be found in [1]. The resulting value of t0 we will use to convert our results to physical units is q tphys 0 = 0.1445(5)(3) fm , (2.12) where the uncertainty is split in the same way as described above for ϕphys 4 . 3 Charm correlators and scale setting In this section we discuss the technical details behind the computation of physical observables in the charm region from our mixed action setup. We introduce the GEVP setup used to 5extract meson masses and matrix elements throughout this work and explain our strategy to match the charm quark mass to its physical value. 3.1 Computation of correlation functions To extract physical observables we have measured two-point correlation functions at zero mo- mentum on CLS gauge configurations listed in Table 1. Fermionic two-point correlators have the form fq,r(x0, y0) = a6 L3 X ⃗ x,⃗ y ⟨Oq,r Γ (x0, ⃗ x)Or,q Γ′ (y0, ⃗ y)⟩, (3.1) where y0 and x0 are, respectively, the source and sink time coordinates; q and r are flavour indices; and a trace over spin and colour is implicit. Oq,r Γ are quark bilinear operators defined as Oq,r Γ (x) = ψ q (x)Γψr(x), (3.2) where Γ is a spin matrix. The operator content will be denoted by subscripts in straightforward notation — we will refer to fPP when Γ = Γ′ = γ5, fAP when Γ = γ0γ5 and Γ′ = γ5, and so on. In all computations in this work we have fixed the time position of the source at y0 = T/2, to maximise the distance from the boundaries: when dealing with heavy-light and heavy-heavy flavour content in the operators Oq,r Γ in Eq. (3.2), we observe that the region in which the signal for the considered two-point function is accessible lies entirely within the lattice bulk, and that the boundary effects are strongly suppressed. Ten time-dilutedU(1) stochastic sources are employed in the computation of the quark propagators in each gauge field configuration. Moreover, the numerical inversion of the quark propagator in the charm region is performed using distance preconditioning techniques [60, 61], in order to reduce signal deterioration and enhance accuracy at large Euclidean times. Error analysis and propagation are based on the Gamma method of [62] and automatic differentiation, as implemented in the ADerrors package [63]. Light and strange propagators are computed at the values of mcr, µl and µs determined to ensure maximal twist and pion and kaon masses matched to the sea (see Section 2). We note that this is a independent set of computations of the propagators with respect to those employed in the matching procedure [1], where a grid of values for the mass parameters is employed to accurately interpolate to the matching point. Moreover, this grid was also employed to compute the mass corrections to the renormalised chiral trajectory [1]. Heavy propagators are computed at three different values of the twisted mass µ(i) c around the physical charm region (save for one ensemble where only two masses have been used), so that observables are interpolated at the physical value of the charm quark mass. In Table 2 we specify the twisted mass values and the critical hopping parameter ˜κcr used to impose the maximal twist condition for each ensemble used in the analysis. 3.2 Extraction of meson masses In our analysis meson masses are employed to fix the renormalised line of constant physics and match the quark masses to some target physical value. Light and strange quark masses are matched between the sea and valence sectors using ϕ2 and ϕ4 in Eq. (2.6), whereas for the 6Id β ˜κcr aµl aµs aµ(1) c aµ(2) c aµ(3) c H101 3.40 0.137277 0.006592 0.006592 0.237975 0.250500 0.263025 H102 0.137291 0.004711 0.010090 0.228285 0.240300 0.252315 H105 0.137319 0.002958 0.013690 0.230108 0.242219 0.254330 H400 3.46 0.137292 0.006006 0.006006 0.204155 0.214900 0.225645 N202 3.55 0.137298 0.005160 0.005160 0.167105 0.175900 0.184695 N203 0.137307 0.003609 0.010770 0.172805 0.181900 0.190995 N200 0.137310 0.002403 0.008432 0.173375 0.182500 0.191625 D200 0.137316 0.001227 0.013170 0.172900 0.191100 – N300 3.70 0.137207 0.004060 0.004060 0.130910 0.137800 0.144690 J303 0.137212 0.001610 0.009570 0.133000 0.140000 0.147000 Table 2: List of run parameters for each ensemble in Table 1. The critical value of the hopping parameter required to set the valence sector to maximal twist [1] is denoted by ˜κcr. The values of aµl and aµs are the light and strange bare twisted quark masses, in lattice units, that match the corresponding sea quark masses [1]. Finally, the last three columns contain the three values of heavy bare twisted quark masses in the charm region. In the case of the D200 ensemble two values that straddle the charm point were considered. partially quenched charm quark we use different combinations of mesons masses matched to their physical values, as explained in Section 3.3. The ground state meson masses are extracted from a generalised eigenvalue problem (GEVP) variational method defined as C(t)vn(t, tref) = λn(t, tref)C(tref)vn(t, tref) n = 0, . . . , N− 1, t > tref, (3.3) where C(t) is a matrix of Euclidean correlation functions of the form in Eq. (3.1), such that the indices i, jin Cij(t) correspond to different choices of Γ , Γ′ and source/sink location, and t = x0 − y0. This leads to the spectral expansion Cij(t) = ∞X n=0 e−Entφniφ∗ nj, i, j = 0, . . . , N− 1 ; φni ≡ ⟨0|Oi|n⟩. (3.4) Here N denotes the matrix dimension, and we have assumed non-degenerate energy levels. The GEVP is solved in the regime tref ≥ t/2, where a better control over excited state contributions is achieved [64]. The matrix C(t) in our setup is built from pseudoscalar two-point functions fPP shifted in time as CP(t) = \u0014 fPP(t) fPP(t + τ) fPP(t + τ) fPP(t + 2τ) \u0015 , (3.5) where τ is the value of the time shift. Several values of the time shift have been tested, and we observe a mild dependence on small values of τ for the extraction of eigenvalues and 7eigenvectors. We refer to Appendix A for a detailed discussion of our setup, together with sanity checks on the GEVP. In what follows we set τ = 3a. The ground state meson mass is extracted from the eigenvalues of the GEVP using Eq. (A.1). In order to assess the systematic effects and correctly identify the plateau region, we perform several uncorrelated χ2 fits to a constant, by varying the time ranges of the fitting interval. Correlated fits are impractical due to the fact that sample covariance matrices display very small modes and thus have ill-behaved inverses. However, as the data is correlated, the uncorrelated χ2 is not a suitable quantity to assess the goodness-of-fit; we therefore quantify the latter with the expectation value of χ2, denoted χ2 exp, and the corresponding p-value, as introduced in [26]. Through this procedure we assign a weight to each fit based on the χ2 minimisation, and we eventually extract our ground state masses by means of the model aver- aging procedure described in Appendix B. An example of a GEVP plateau for the heavy-light pseudoscalar mass together with a summary of the model average procedure for an ensemble used in the analysis is shown in Figure 1. 3.3 Matching of the charm quark mass In Section 2 we recalled the matching of the light sector worked out in [1], which ensures that physical observables involving only light and strange quarks computed in the valence and sea sectors coincide up to cutoff effects, so that unitarity is recovered in the continuum limit. A similar procedure is needed for the charm quark, designed to ensure that its physical value is obtained upon taking the continuum limit and performing chiral fits. Since the charm is partially quenched this matching procedure involves observables with only valence charm quark propagators. In order to establish a connection with the physical point, we require that some charm-like observable Oc matches its physical value. In this paper we studied three different charm scale settings based on three choices of Oc, all in terms of meson masses; we will denote the latter as m(i) H , i= 1, 2, 3, and often express them in units of √8t0 as ϕ(i) H = √8t0m(i) H . The first possibility, corresponding to ϕ(1) H , consists in using the flavour average meson mass combination m(1) H = mH ≡ 2 3mH + 1 3mHs, (3.6) built from heavy-light H and heavy-strange Hs pseudoscalar meson masses with heavy-quark masses in neighbourhood of the charm. Since we require the considered CLS ensembles to hold a constant value of the flavour average combination of pion and kaon masses – denoted as ϕ4 in Eq. (2.6) – we also expect the flavour average combination ϕ(1) H to remain fairly constant along the chiral trajectory. The physical value of m(1),phys H is obtained by setting mH(s) to the following prescription for the isoQCD values of D(s) meson masses, misoQCD D = 1867.1 ± 2.6 MeV, m isoQCD Ds = 1967.1 ± 1.3 MeV. (3.7) The uncertainties in these isoQCD values are chosen to cover the deviation with respect to the experimental values [65] of the D± and D± s meson masses, mexp D± = 1869.66(5) MeV and mexp D± s = 1968.35(7) MeV, respectively. We observe that the larger uncertainty in the isoQCD inputs of the D and Ds meson masses in Eq. (3.7) — as compared to the corresponding 89 0 20 40 60 80 t/a 0.468 0.470 0.472 0.474 0.476 0.478 0.480 amH {tmin} {tmax} W = 0.448 - tmin/a = 39 - tmax/a = 83 W = 0.28 - tmin/a = 41 - tmax/a = 83 0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 3.2 3.5 3.9 4.3 t [fm] 0.4734 0.4736 0.4738 0.4740 amH 0.0 0.2 0.4 W 0.0 0.2 0.4 0.6 p − value [37,83] [39,80] [39,83] [41,80] [41,83] [43,83] [45,83] Models [tmin/a, tmax/a] 0.0 0.5 1.0 χ2/d.o.f. Figure 1: Illustration of the extraction of the ground-state mass after applying a GEVP anal- ysis, illustrated for the ensemble J303. Top: Heavy-light pseudoscalar meson mass plateau showing the two fit intervals with higher weights W contributing to the model average. We also indicate the range of variations allowed for the interval in Euclidean time where the plateau is taken. Bottom: Summary of determinations of amH when considering variations over the fit intervals [tmin/a, tmax/a] together with the corresponding normalised weights W based on Takeuchi’s Information Criterion (TIC), p-values andχ2/d.o.f.. In the upper panel, the shaded blue band corresponds to the model average result.experimental values — does not induce a significant increase in the uncertainties of our target results. The input values in Eq.( 3.7) lead to the following flavour averaged meson mass, m(1),phys H = mD = 1900.4(1.8) MeV . (3.8) Our second strategy, corresponding toϕ(2) H , is to consider the mass-degenerate pseudoscalar meson mass mconn ηh extracted from the quark-connected two-point correlation function made of heavy quark propagators with a mass in the neighbourhood of the charm mass, m(2) H = mconn ηh . (3.9) The physical value for this mass, m(2),phys H , is set from the experimental value of the ηc meson mass [65], mexp ηc = 2983.9(4) MeV, from which a correction of about 6 MeV, with 100% error, is subtracted to account for the absence of quark-disconnected diagrams and QED effects [43, 66–69]. Specifically, we employ, m(2),phys H = mconn ηc = 2978(6) MeV . (3.10) One potential advantage of this choice of matching observable is that the overall precision of the ηconn c meson mass is substantially better than the one for heavy-light meson masses, as it does not suffer from the increase in noise-to-signal ratio with Euclidean time; this is illustrated in Figure 2, where we show the D, Ds and ηconn c pseudoscalar correlators for a one specific ensemble. Finally, as a third matching quantity we also tested the spin-flavour averaged mass combination m(3) H = mH∗ = 1 12 \u0000 2mH + mHs + 6mH∗ + 3mH∗s \u0001 , (3.11) which involves a combination of heavy-light pseudoscalarmH(s) and vector mH∗ (s) meson masses in the charm region, and is motivated by heavy-quark symmetry. However, we observe that chiral-continuum fits coming from the spin flavour-averaged matching condition lead to worse χ2 values, and as a result their weights are highly suppressed by our model average prescrip- tion. We interpret this finding as a reflection of relatively poor control of heavy-light vector states, whose masses are extracted with significantly larger errors than those of heavy-light pseudoscalar states. In the rest of the discussion we will therefore focus on the results coming from the other two matching conditions. Any of these matching conditions can in principle be imposed ensemble by ensemble, even away from the physical point. However, by doing so we would as a result build in the charm quark mass a dependence on the value of the reference scale tphys 0 , as well as O(a2) effects coming from the specific choice of Oc. To avoid this, we have opted instead for setting the physical charm quark mass jointly with the chiral-continuum extrapolation, in a similar way as the one we employ to hit the physical point in the light and strange sector. What this means in practice is that the charm quark mass dependence of any given observable O is parameterised as O(a, ϕ2, ϕ(i) H ), and we perform a global fit to obtain its physical value O(0, ϕphys 2 , ϕ(i),phys H ). This will be the procedure applied below in the determination of the physical value of the charm quark mass and of the decay constants fD and fDs. Note that, as a consequence of our matching procedure and of working on a line of constant physics where ϕ4 is kept constant, it is non-trivial that by adopting any of our matching procedures the mass of any particular meson reaches its physical value in the chiral-continuum 100 20 40 60 80 t/a 0.980 0.985 0.990 0.995 1.000 1.005 1.010 1.015 1.020 ameﬀ H (t)/amH D 0 20 40 60 80 t/a Ds 0 20 40 60 80 t/a ηconn c 40 60 80 0.999 1.000 1.001 Figure 2: Illustration of the effective meson masses involved in the matching procedure to the physical charm scale for the ensemble J303. We show three cases where the effective mass of the pseudoscalar meson H is that of the D (left), Ds (center) and ηconn c (right), normalised by the central value of the corresponding plateau averaged mass. The horizontal red bands show the results of the highest weight fit contributing to the model average procedure and the corresponding plateau interval. We observe the expected increase of the statistical uncertainties at large time separations when increasing the mass-difference among the quarks propagators of the pseudoscalar two-point correlators. limit; checking that it does is therefore a test of the robustness of our procedure. As an illustration, we show in Fig. 3 how the physical values of the D and Ds meson masses arise when the charm scale is matched through either mD or mconn ηc . In either case we show results for the specific model of the lattice spacing, charm mass and pion mass dependence of the form √ 8t0 mD(s) (a, ϕ2, ϕ(i) H ) = p0 + p1ϕ2 + p2ϕ(i) H + c1 a2 8t0 , (3.12) where i = 1, 2 according to the notation introduced above and where c1 and pj, j = 1, 2, 3, stand for the fit parameters. Note that the agreement is excellent, in spite of the different implications of the two setups for the specific case of mD(s) ; for instance, when mD is used for the matching cutoff effects are very small by construction, while the use of mconn ηc leads to sizeable cutoff effects which are however very well described by an O(a2) term. 4 Determination of the charm quark mass 4.1 Renormalised charm quark masses In Sec. 2 we have summed up the argument why renormalised quark masses can be easily retrieved from bare Lagrangian twisted masses. In our mixed-action setup, as discussed in detail in [1], the resulting O(a)-improved expression for the renormalised charm mass mc(µ) reads mc(µ) = Z−1 P (g2 0, aµ)[1 + abµ(g2 0)tr{m(s)}] µc , (4.1) where ZP(g2 0, aµ) is a suitably defined renormalisation constant for the non-singlet pseudoscalar density at renormalisation scale µ. As we have already discussed, the improvement term 110.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 φ2 3.80 3.85 3.90 3.95 4.00 4.05 4.10 4.15 4.20 √8t0mD φ2 = φphys 2 isoQCD a ≈0.087 fm a ≈0.077 fm a ≈0.065 fm a ≈0.05 fm 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 φ2 3.80 3.85 3.90 3.95 4.00 4.05 4.10 4.15 4.20 √8t0mD φ2 = φphys 2 isoQCD a ≈0.087 fm a ≈0.077 fm a ≈0.065 fm a ≈0.05 fm 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 φ2 3.80 3.85 3.90 3.95 4.00 4.05 4.10 4.15 4.20 √8t0mDs φ2 = φphys 2 isoQCD a ≈0.087 fm a ≈0.077 fm a ≈0.065 fm a ≈0.05 fm 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 φ2 3.80 3.85 3.90 3.95 4.00 4.05 4.10 4.15 4.20 √8t0mDs φ2 = φphys 2 isoQCD a ≈0.087 fm a ≈0.077 fm a ≈0.065 fm a ≈0.05 fm Figure 3: Consistency checks of our charm matching strategy. We show the chiral extrapolation to the physical point of the D(s) meson mass in units of √8t0 using all the ensembles listed in Table 1. The left panels use the flavour-averaged mass combination,m(1) H = mH, while those on the right use the mass-degenerate pseudoscalar meson mass,m(2) H = mconn ηh . The empty symbols correspond to the D(s) meson masses determined on a given ensemble, while the red square symbols show the extrapolated values at the physical point. Dashed lines show the fit forms projected to each individual lattice spacing, and the blue shaded bands are a projection to the continuum limit on the chiral plane. Data points are projected to the physical point ϕ(i),phys H . Finally, the green horizontal band shows the isoQCD input values for the corresponding masses in Eq. (3.7), in units of √8t0. ∝ tr{m(s)} can be neglected in practice, so O(a)-improved renormalised quark masses can be obtained by just applying the renormalisation constants to the exactly known Lagrangian masses. In this work we will use the non-perturbative values of ZP computed in [53] in the Schr¨ odinger Functional scheme, at a fixed renormalisation scaleµhad = 233(8) MeV and for the range of values of g2 0 covered by CLS. It will be used to obtain renormalised quark masses for each of our ensembles, that can then be used to determine the value of the charm quark mass in the continuum and at physical kinematics. Contact with other renormalisation schemes can then be made by computing the renormalisation group invariant (RGI) quark mass MRGI c , using the continuum (flavour-independent) ratio also computed in [53] M m(µhad) = 0.9148(88) . (4.2) Values of renormalised masses in, say, the MS scheme can then be obtained by using the 12perturbative value of m(µ) M at any convenient scale µ. 4.2 Charm quark mass chiral-continuum fits Having determined the renormalised charm quark masses in the Schr¨ odinger Functional scheme at the hadronic renormalisation scale µhad mc(µhad) ≡ µR c , (4.3) for all the ensembles listed in Table 1, we now describe our strategy to obtain results in the continuum limit and at the physical point, following the approach outlined in Sec. 3. The matching procedure of the light and strange sectors is already devised so that the physical value of the kaon mass is recovered atϕ2 = ϕphys 2 , where the physical value ofϕ2 is computed with the isospin-symmetric values of the pion mass quoted in [9], and the physical scale tphys 0 is the one determined in [1]. The charm scale is matched through the two different prescriptions described in Sec. 3. All quantities entering the fit are made dimensionless through the appropriate power of the factor √8t0, and physical units for the final result are restored by using our value for tphys 0 . We parameterise the continuum dependence of the renormalised charm quark mass on ϕ2 and any of the ϕ(i) H with the functional form √ 8t0 µR c (a = 0, ϕ2, ϕH) = p0 + p1ϕ2 + p2ϕH . (4.4) Based on the heavy quark effective theory expansion [70] at lowest order, we expect a linear dependence of the charmed meson masses as a function of the the charm quark mass, hence the latter term in the ansatz. This assumption is supported by our data that show indeed a linear behaviour in the charmed meson masses, as illustrated in Figure 4. Note that this form is used only to describe the dependence within a short interval in mass values, and interpolate the charm scale from points close by. When considering the pion dependence of the charm quark mass, we assume that the leading order contributions exhibit a linear behaviour in ϕ2. With the current set of ensembles employed in this work we do not observe any deviations from the leading order term in the pion mass dependence. Regarding the lattice spacing dependence of the charm quark mass, we assume the leading cutoff effects to be O(a2), as discussed above. Corrections of odd order in a are generically expected to be highly suppressed at maximal twist, by way of the extension of the argument for automatic O(a) improvement; we thus include a4 terms to account for deviations from linear behaviour in a2. Finally, we allow for terms proportional to m2 π and to various powers of the charm mass. The generic ansatz to parameterise lattice spacing dependence thus take the following form cµc(a, ϕ2, ϕH) = a2 8t0 \u0000 c1 + c2ϕ2 + c3ϕ2 H \u0001 + a4 (8t0)2 \u0000 c4 + c5ϕ2 H + c6ϕ4 H \u0001 . (4.5) In order to estimate the systematic effects arising from the model variation, we consider all the possible combinations where some of the ci coefficients vanish, save for c1 which is always kept. Furthermore, following [45], we allow for cutoff effects to enter either linearly or non-linearly, viz., √ 8t0µR,linear c (a, ϕ2, ϕH) = √ 8t0µR,cont c (0, ϕ2, ϕH) + cµc(a, ϕ2, ϕH), (4.6)√ 8t0µR,non-lin c (a, ϕ2, ϕH) = √ 8t0µR,cont c (0, ϕ2, ϕH) \u0000 1 + cµc(a, ϕ2, ϕH) \u0001 . 133.80 3.85 3.90 3.95 4.00 4.05 4.10 4.15 φ(1) H 3.1 3.2 3.3 3.4 3.5 3.6 3.7 √8t0µR c φH = φphys H a ≈0.065 fm a ≈0.087 fm a ≈0.077 fm a ≈0.05 fm 5.9 6.0 6.1 6.2 6.3 6.4 φ(2) H 3.1 3.2 3.3 3.4 3.5 3.6 3.7 √8t0µR c φH = φphys H a ≈0.065 fm a ≈0.087 fm a ≈0.077 fm a ≈0.05 fm Figure 4: Heavy mass dependence of the renormalised charm quark mass µR c in units of √8t0 for the fits with larger weights according to the TIC criteria. Top: Results shown for the flavour-averaged matching condition ϕ(1) H = √8t0mH. Bottom: Results shown for the ηconn h matching condition ϕ(2) H = √8t0mconn ηh . Dependencies other than ϕ(i) H in the chiral-continuum extrapolation have been projected to the physical point. The red square symbols indicate the continuum results at the physical value ϕphys H . We observe a linear dependence of the charm quark mass on the different matching conditions used in this work. We thus end up with a total of 64 functional forms for each of the two charm matching conditions, i.e., a total of 128 models. Fit parameters are estimated minimising an uncorrelated χ2 where, however, the covariance between the independent variables and the data is taken into account. As previously discussed, the goodness-of-fit of fit can still be obtained in this case 14ϕ(1) H ϕ(2) H combined √8t0µR c 3.354(28)(6) 3.363(27)(6) 3.361(26)(7) Table 3: Results of the model average for the renormalised charm quark mass in units of √8t0 based on the two charm quark mass matching conditions — ϕ(1) H denotes the flavour-averaged matching condition in Eq. (3.6) and ϕ(2) H the ηconn h matching prescription in Eq. (3.9). The last column reports the combined result from these two matching procedures according to our model average prescription. The first error is statistical, while the second is the systematic uncertainty arising from the model variation. from the measurement of the χ2 exp and the associated p-value. The TIC result for each model is then fed into the model averaging procedure summarised in App. B, which finally allows to quote a systematic uncertainty that reflects the fluctuations engendered by the variety of fit ansaetze. In Table 3 we report the results for µR c in units of √8t0 obtained with each of the two matching conditions independently, as well as for the combined model average. In Figure 5 we summarise the model average procedure, showing some of the best fit results coming from the functional forms defined in Eq. (4.6) for the two matching conditions studied in this work. Each circle corresponds to a result coming from a particular model, and the opacity is associated to its weight determined from our Takeuchi’s Information Criterion (TIC) as explained in App. B. We observe that for both matching conditions the majority of the models with relevant weights nicely agree, and as a result the systematic error is subleading with respect to the statistical uncertainty. Figure 6 shows a weighted histogram of our results coming from different fits. We observe that models cluster mainly around two values, which are adequately covered by our quoted systematic uncertainty. Figure 7 illustrates typical fits for each of the matching conditions, chosen among those with higher weights according to the TIC prescription. The plot shows the continuum limit behaviour of the charm quark mass in units of √8t0. Results coming from the two matching strategies perfectly agree in the continuum, in spite of displaying a qualitatively different struc- ture in cutoff effects. We observe a scaling of the charm quark mass in reasonable agreement with the O(a2) leading order, confirming the automatic O(a)-improvement of our setup; nev- ertheless, we notice that given the current statistical accuracy, fits with O(a4) terms are the preferred ones from the model average, since they allow to properly describe the curvature in our data. Note also the overall small size of scaling violations, which are at the few percent level. Finally, Figure 8 shows the pion mass dependence of the charm quark mass. As expected, we observe a mild dependence of the charm mass on the light quark masses. 4.3 Results for the charm quark mass The renormalised charm quark mass µR c can be obtained once we combine the results collected in Table 3 with our determination of q tphys 0 in Eq. (2.12). As discussed at the beginning of this section, the knowledge of the renormalisation group running factors allows to quote results for the RGI and MS values of the charm quark mass. 15[’a2’] [’a4’] [’a4h2’] [’a4h4’] [’a2h2’] [’a2l’] [’a4h2’, ’a4’] [’a4h4’, ’a4’] [’a4h4’, ’a4h2’] [’a2h2’, ’a4’] [’a2h2’, ’a4h2’] [’a2h2’, ’a4h4’] [’a2l’, ’a4’] [’a2l’, ’a4h2’] [’a2l’, ’a4h4’] [’a2l’, ’a2h2’] [’a4h4’, ’a4h2’, ’a4’] [’a2h2’, ’a4h2’, ’a4’] [’a2h2’, ’a4h4’, ’a4’] [’a2h2’, ’a4h4’, ’a4h2’] [’a2l’, ’a4h2’, ’a4’] [’a2l’, ’a4h4’, ’a4’] [’a2l’, ’a4h4’, ’a4h2’] [’a2l’, ’a2h2’, ’a4’] [’a2l’, ’a2h2’, ’a4h2’] [’a2l’, ’a2h2’, ’a4h4’] [’a2h2’, ’a4h4’, ’a4h2’, ’a4’] [’a2l’, ’a4h4’, ’a4h2’, ’a4’] [’a2l’, ’a2h2’, ’a4h2’, ’a4’] [’a2l’, ’a2h2’, ’a4h4’, ’a4’] [’a2l’, ’a2h2’, ’a4h4’, ’a4h2’] [’a2l’, ’a2h2’, ’a4h4’, ’a4h2’, ’a4’] 3.30 3.32 3.34 3.36 3.38 √8t0µR c Systematic error Model average 0.005 0.010 0.015 0.020 0.025 0.030 W Figure 5: Model average procedure for the renormalised charm quark mass µR c in units of √8t0. We collect a subset of the best results according to the TIC procedure, coming from different models, for the flavour-averaged matching condition ϕ(1) H . The opacity of each circle data point reflects the associated normalised weights W as given from the TIC. The yellow shaded band represents the systematic error computed with Eq. (B.4), while the left-most red square symbol corresponds to the result extracted from the model average procedure. The labels of the 32 models specified in the horizontal axis are related to the terms appearing in Eq. (4.5) – characterising the lattice spacing dependence – in the following way: ‘a2’ corresponds to the term depending on the fit parameter c1. Similarly, ‘a2l’, ‘a2h2’, ‘a4’, ‘a4h2’, ‘a4h4’ refer to c2, . . . , c6, respectively. Given that the parameter c1 is included in all the models, the associated label is not explicitly specified for all cases appearing in the horizontal axis. After combining the results from our 128 fitting models through the model average proce- dure, and using the running factor in Eq. (4.2), we quote for the three-flavour theory the value for the RGI quark mass MRGI c (Nf = 3) = 1 .485(8)(3)(14)[17] GeV , (4.7) where the first error is statistical, including the uncertainty on tphys 0 , the second accounts for the systematic uncertainty, derived from the model average, the third is the error contribution from the RGI running factor in Eq. (4.2), and the last error in brackets is the total uncertainty. Figure 9 illustrates the relative contribution of various sources of error to the uncertainty of our determination of MRGI c . The dominant source of error comes from the renormalisation group running of Eq. (4.2), while the second most relevant contribution arises from the statis- tical error of the correlation functions computed in each ensembles. The error coming from the uncertainty on tphys 0 based on our scale setting procedure [1], as well as the systematic error from the model average are subleading contributions. We therefore expect that the inclusion in 163.33 3.34 3.35 3.36 3.37 3.38 3.39√8t0µR c 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 Model average Systematic error Figure 6: Weighted histogram illustrating the model average procedure for √8t0 µR c . The result from each of the 128 models – including both matching conditionsϕ(1) H and ϕ(2) H – parameterising the lattice spacing dependence is weighted by its normalised weight W based on the TIC. The vertical line represents the central value from the model average, while the vertical band shows the corresponding estimate of the systematic error. this charm quark mass analysis of further ensembles – with finer lattice spacings and at physical pion masses – will only have a significant impact if combined with improved determinations of the RGI running factor and the scale setting procedure. In order to quote results in the MS scheme, we use five-loop perturbation theory for the quark mass anomalous dimension [71–73] and the beta function [74–76]. The matching between the Nf = 3 and Nf = 4 theories uses the four-loop decoupling effects [77] incorporated into the RunDec package [78–80]. Renormalisation group equations are solved using as input the value Λ(3) MS = 341(12) MeV from [81]. The correlation arising from the fact that a common subset of gauge field configuration ensembles were employed in the computation of Λ (3) MS and the non-perturbative running factor in Eq. (4.2) is taken into account. We thus arrive to the following results for the RGI and MS-scheme charm quark masses in the 4-flavour theory, MRGI c (Nf = 4) = 1 .546(8)(3)(14)(4)Λ(3)trunc.[17] GeV , (4.8) mc(µ = 3 GeV, Nf = 4) = 1 .006(5)(2)(9)(6)Λ(3)trunc.[13] GeV , (4.9) mc(µ = mc, Nf = 4) = 1 .296(5)(2)(8)(11)Λ(5)trunc.[16] GeV , (4.10) where the first and second errors arise from the statistical and systematic errors, respectively, in the value of MRGI c (Nf = 3) in Eq. (4.7), the third error is due to the non-perturbative running factor in Eq. (4.2), the fourth error is related to the uncertainty in Λ (3) MS, the fifth error is an estimate of the truncation uncertainty from the deviation between the 5-loop and 4-loop results, and the last error in brackets is the total error. We observe that at the lower renormalisation scale, µ = mc, the scale invariant MS charm mass, mc(µ = mc, Nf = 4), 170.00 0.01 0.02 0.03 0.04 0.05 a2/8t0 3.225 3.250 3.275 3.300 3.325 3.350 3.375 3.400 3.425 √8t0µR c φ(2) H = √8t0mηconnc φ(1) H = √8t0m ¯D Figure 7: Comparison of the continuum limit approach for the two charm matching prescrip- tions. Shown are two of the fits with highest weights from the TIC, projected onto the lattice spacing dimension. In yellow we show results for the ηconn h matching condition, while the blue points illustrate the flavour-averaged matching. Each data-point in this plot is projected to the physical pion mass and the physical charm quark mass, in order to properly visualise the lattice spacing dependence. receives a large contribution to its error from the uncertainty of Λ (3) MS and from the truncation error. These specific sources of uncertainty are less prominent in the RGI mass, MRGI c (Nf = 4). In Figure 10 we compare our determinations of the charm quark mass in the MS scheme with the results from other lattice QCD calculations also based on Nf = 2 + 1 dynamical simulations and with the corresponding FLAG average [9]. We observe in particular a good agreement with the results from [45] which are also based on CLS ensembles but employ Wilson fermions in the valence sector. 5 Determination of decay constants of charmed mesons 5.1 Computation of decay constants Along with the charm quark mass, in this paper we present a first computation of the D(s) meson decay constants within our setup. In the absence of electromagnetic interactions, the decay constant fully determines the leptonic decay amplitude of flavoured pseudoscalar mesons, and is given by the matrix element of the axial current as \f\f⟨0|Aqr 0 |Pqr(p = 0)⟩ \f\f = fqrmPS √ 2mPSL3 , (5.1) 1819 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 φ2 3.225 3.250 3.275 3.300 3.325 3.350 3.375 3.400 √8t0µR c φ2 = φphys 2 a ≈0.087 fm a ≈0.077 fm a ≈0.065 fm a ≈0.05 fm Figure 8: Pion mass dependence of the charm quark mass for one of the best fits according to the TIC criteria. Results are shown for the flavour-averaged matching condition. Each point corresponds to the value for a given ensemble, projected to the physical charm quark mass. The dashed lines represent the chiral trajectories at finite lattice spacing, while the blue shaded band is a projection to the continuum limit. The red point shows our final result extrapolated at the physical point in the continuum.MRGI c (Nf = 3) RG running to RGI mass Stat. + χ-cont. limit Scale setting Model av. Figure 9: Relative contributions to the total variance of our final result forMRGI c . The dominant piece comes from the error in the non-perturbative determination of the renormalisation group running factor to the RGI mass quoted in Eq. (4.2). The label statistical plusχ-continuum limit stands for the error arising from the statistical accuracy of our data and the chiral-continuum extrapolation, while the scale setting piece comes from the physical value of the gradient flow scale tphys 0 . Finally, the model average piece illustrates the systematic error arising from the set of models considered in this work. where the state |Pqr⟩ is the ground state for a pseudoscalar meson with flavour content qr, and mPS its mass. The factor 1 / √ 2mPSL3 comes from the usual relativistic normalisation of one-particle states in finite volume. With Wilson fermions, the computation of the above matrix elements requires the finite current normalisation factor ZA and, if O(a) effects are to be subtracted, a number of improve- ment coefficients. With our fully twisted valence sector this is completely bypassed: when qr belong in a twisted quark doublet — i.e., have different signs in the twisted mass matrix in Eq. (2.3) — the physical axial current, expressed in twisted quark variables, becomes a vector current, and the Ward identity in Eq. (2.4) allows to obtain it from the pseudoscalar two-point function. The resulting expression of the correctly normalised pseudoscalar decay constant reads fP S= s 2L3 m3 P S (µq + µr)|⟨0|Pqr|Pqr(p = 0)⟩|. (5.2) We will extract the matrix element⟨0|Pqr|Pqr(p = 0)⟩ from the normalised eigenvectorvn(t, t0) of the GEVP according to Eq. (A.3). In order to extract the large time plateau where excited state contributions are suppressed we perform several fits to constant behaviour by varying the fit ranges, and we assign a weight to each fit by means of the TIC prescription as described in App. B. The results for the ground state matrix element are then extracted through the model 200.98 0.99 1.00 1.01 1.02 1.03 [GeV]mc(3 GeV) FLAG21 This work ALPHA 21 Petreczky 19 JLQCD 16 χQCD HPQCD 10 HPQCD 08B 1.25 1.26 1.27 1.28 1.29 1.30 1.31 1.32 [GeV]mc(mc) FLAG21 This work ALPHA 21 Petreczky 19 Maezawa 16 JLQCD 16 χQCD HPQCD 10 HPQCD 08B PDG Figure 10: Comparison of our charm quark mass determinations in the MS scheme with the FLAG average [9] and the results from other lattice QCD calculations based on Nf = 2 + 1 dynamical simulations. In our results, shown in blue, we indicate both the total uncertainty and the error when excluding the uncertainty arising from Λ (3) MS. Left: comparison for the mc(µ = 3 GeV , Nf = 4). Right: comparison for mc(µ = mc, Nf = 4). Starting from the bottom, results are taken from: PDG [65], HPQCD 08B [82], HPQCD 10 [27], χQCD [35], JLQCD 16 [37], Maezawa 16 [83], Petreczky 19 [42], ALPHA 21 [45]. average given by Eq. (B.3). In Figure 11 we show a representative plateau for a heavy-light decay constant, together with a summary of the model average with different fit intervals. 5.2 Chiral-continuum fits and results for fD(s) The chiral-continuum fits for theD(s) meson decay constants are performed similarly to the ones for the charm quark mass. By exploiting Chiral Perturbation Theory with heavy quarks [84,85] to construct appropriate fit functions, we extract the physical point observables trough a global fit of the fD and fDs decays, and estimate the systematic effects by applying the model average procedure based on the TIC. The quantities we fit to are combinations of meson masses and decay constants of the form ΦD(s) = (8t0)3/4fD(s) pmD(s) , (5.3) for which a Heavy Quark Effective Theory (HQET) scaling law in powers of the inverse heavy quark mass exists. The general continuum heavy and light quark mass dependence can be expressed as the product of the individual contributions to arrive at the generic expression ΦD(s) = Φχ h 1 + δΦ D(s) χPT ih 1 + δΦ D(s) a i . (5.4) Here Φχ governs the heavy-quark mass dependence while δΦ D(s) χPT controls the light quark be- haviour as approaching the physical point. Finally the lattice spacing dependence describing cut-off effects is regulated by δΦ D(s) a . In the following, we analyse these terms independently to arrive at a final expression for the Φ D(s) approach to the physical point. 2122 0 20 40 60 80 t/a 0.056 0.058 0.060 0.062 0.064 0.066 afPS {tmin} {tmax} W = 0.105 - tmin/a = 38 - tmax/a = 88 W = 0.104 - tmin/a = 40 - tmax/a = 88 0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 3.2 3.5 3.9 4.3 t [fm] 0.0610 0.0612 0.0614 afPS 0.000 0.025 0.050 0.075 0.100 W 0.0 0.2 0.4 0.6 0.8 p − value [30,88][32,85][32,88][34,82][34,85][34,88][36,82][36,85][36,88][38,82][38,85][38,88][40,82][40,85][40,88][42,82][42,85][42,88][44,82][44,85][44,88][46,82][46,85][46,88] Models [tmin/a, tmax/a] 0.000 0.025 0.050 0.075 0.100 χ2/d.o.f. Figure 11: Illustration of the extraction of the heavy-light pseudoscalar decay constants, after applying a GEVP analysis, for ensemble J303. Top: plateau for the heavy-light pseudoscalar decay constant for the two fit intervals with higher weights in the model average. Bottom: summary of results from different fit ranges together with weights W, p-values and χ2/d.o.f.. The shaded blue band represents the model average result.The continuum heavy-quark mass dependence, Φ χ, admits an expression in HQET of the form Φχ = CHQET(mh) Φ0 \" 1 + p(1) h 1 ϕH + p(2) h \u0012 1 ϕH \u00132 + . . . # , (5.5) where ϕH = √8t0mH monitors the heavy quark mass dependence with mH being the flavour- average m ¯H or the ηconn h pseudoscalar meson masses. In general, this expression is not expected to have high accuracy in the charm mass region, due to it being at the limit of applicability of HQET. Furthermore, perturbative values for the matching factor CHQET(mh) have notoriously poor convergence behaviour. 6 However, we are not interested in modelling the heavy quark mass dependence in a wide region of masses — we rather want to interpolate to the charm point from the nearby values of the heavy masses we compute at. Therefore, we will simply take an expression with the same functional form for the mh power corrections, and a constant overall coefficient, as a convenient ansatz for the interpolation part of our fits. In HQET terms, this amounts to neglecting the small logarithmic dependence on mh in a short interval of values. The light quark mass dependence term, following Heavy Meson χPT (HMχPT) consider- ations, reads [38,85] δΦD χPT = − 1 + 3g2 64π2ϕ2 f \u0014 3Lπ + 2LK + 1 3Lη \u0015 + 4ϕ2 ϕ2 f   p(0) χ + p(2) χ ϕ2 ϕ2 f + p(4) χ ϕH ! , δΦDs χPT = − 1 + 3g2 64π2ϕ2 f \u0014 4LK + 4 3Lη \u0015 + 8 (ϕ4 − ϕ2) ϕ2 f   p(0) χ + p(2) χ ϕ2 ϕ2 f + p(4) χ ϕH ! , (5.7) where p(0,1 ... ) χ are fit parameters and g2 is the H∗Hπ coupling in the static and chiral limits, here treated as a free fit parameter alongside p(i) χ . In Eq. (5.7) we introduced the notation for the chiral logarithm corrections Lπ = ϕ2 log(ϕ2), (5.8) LK = \u0012 ϕ4 − 1 2ϕ2 \u0013 log(ϕ4 − 1 2ϕ2), (5.9) Lη = \u00124 3ϕ4 − ϕ2 \u0013 log(4 3ϕ4 − ϕ2). (5.10) Here ϕ2 and ϕ4 are the usual hadronic combinations introduced in Eq. (2.6), which control the light and strange quark mass dependence. When working at NLO in the chiral expansion, the term ϕf appearing in Eq. (5.7), which introduces the χPT scale, is here replaced by the continuum physical value of √8t0fπK , as determined from our setup [1] at full twist, with fπK given by7 fπK = 2 3 \u0012 fK + 1 2fπ \u0013 . (5.11) 6This is readily observed in the expression for the coefficient in the MS scheme [86,87], CHQET(mh) = [αs(mh)]γ0/2β0 \u0014 1 + αs(mh) 4π \u0012 −8 3 + γ1 2β0 − γ0β1 2β2 0 \u0013 + O(α2 s) \u0015 , (5.6) where, for QCD, γ0 = −4, γ1 = −254/9−56π2/27+20 Nf/9, while the perturbative coefficients of the β function have their usual values β0 = (11 − 2Nf/3) and β1 = (102 − 28Nf/3). 7We remind the reader that fπK is the quantity used to extract the physical scale tphys 0 in our setup. 23Finally, with similar arguments to the one discussed in the case of the charm quark mass, the lattice spacing dependence δΦ D(s) a for the observables ΦD(s) can be parameterised as δΦD a = a2 8t0 h p(0) a + ϕ2 \u0010 p(1) a + p(3) a ϕ2 H \u0011 + p(2) a ϕ2 H i + O(a4), δΦDs a = a2 8t0 h p(0) a + 2 (ϕ4 − ϕ2) \u0010 p(1) a + p(3) a ϕ2 H \u0011 + p(2) a ϕ2 H i + O(a4), (5.12) where p(0,1,2,... ) a are fit parameters. To summarise, for the continuum quark mass dependence of Φ D and ΦDs we adopt the expressions ΦD(0, ϕ2, ϕH) = p0 + 4p1 ϕ2 f ϕ2 + p2 ϕH − 1 + 3g2 64πϕ2 f \u0012 3Lπ + 2LK + 1 3Lη \u0013 + 4ϕ2 ϕ2 f   p(0) χ + p(2) χ ϕ2 ϕ2 f + p(4) χ ϕH ! , ΦDs (0, ϕ2, ϕH) = p0 + 8p1(ϕ4 − ϕ2) ϕ2 f + p2 ϕH − 1 + 3g2 64πϕ2 f \u0012 4LK + 4 3Lη \u0013 + 8 (ϕ4 − ϕ2) ϕ2 f   p(0) χ + p(2) χ ϕ2 ϕ2 f + p(4) χ ϕH ! , (5.13) obtained by combining the light and heavy quark dependencies δΦχPT and Φχ, respectively. Following Eq. (5.4), this then leads to the final ansatz for Φ D(s) of the form ΦD(s) (a, ϕ2, ϕH) = ΦD(s) (0, ϕ2, ϕH) h 1 + δΦ D(s) a i . (5.14) Since many fit parameters are shared between Φ D and ΦDs, we opt for a global fit for deter- mining the two quantities. Moreover, at the symmetric point, i.e., for those ensembles with degenerate light and strange quark masses µl = µs, the two decay constant coincide, and ΦD = ΦDs. Therefore, a global fit also helps to constrain the parameters at the symmetric point. Similarly to the case of the charm quark mass, we consider several specific forms of the fit ansatz, by setting some combination of fit parameters to zero. We furthermore again match the charm scale using the two different procedures described in Sec. 3. The result is a total of 57 different models for each matching condition, and we use our TIC criterion to extract a systematic uncertainty associated to the variation within the full set of fits. In this work, our current approach deliberately excludes fits involving cuts in β or pion masses, as with the current subset of ensembles they are significantly penalised by the TIC. As we look ahead to future updates with the complete set of ensembles we will incorporate cuts in the data within our analysis. In Figure 12 we show the chiral extrapolations for fD and fDs with larger weights in the model average. From our chiral-continuum extrapolations of Φ D and ΦDs, we observe a mild dependence on the choice of the ϕH used to match the charm scale. Therefore, in the Figures we illustrate the flavour-averaged matching condition only. We also notice that Φ D shows some curvature in ϕ2 arising from the chiral logs, while Φ Ds presents a more linear behaviour while approaching the physical point. Figure 13 shows an illustration of the scaling towards the continuum limit of Φ D and ΦDs. We observe that the continuum approach is very well 2425 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 φ2 0.85 0.90 0.95 1.00 1.05 1.10 1.15 1.20 Φ D φ2 = φphys 2 a ≈0.087 fm a ≈0.077 fm a ≈0.065 fm a ≈0.05 fm 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 φ2 0.85 0.90 0.95 1.00 1.05 1.10 1.15 1.20 Φ Ds φ2 = φphys 2 a ≈0.087 fm a ≈0.077 fm a ≈0.065 fm a ≈0.05 fm Figure 12: Chiral behaviour of the best fits according to the TIC criteria applied to Φ D (top) and Φ Ds (bottom). Each point is projected to the physical charm quark mass, and results are shown for the flavour-averaged matching condition ϕ(1) H . Dashed lines refer to the mass dependence at finite values of the lattice spacing, while the blue band represents the projection to the continuum limit. Finally, the red square symbols indicate the physical point results.26 0.00 0.01 0.02 0.03 0.04 0.05 a2/8t0 0.86 0.88 0.90 0.92 0.94 0.96 Φ D CL a ≈0.087 fm a ≈0.077 fm a ≈0.065 fm a ≈0.05 fm 0.00 0.01 0.02 0.03 0.04 0.05 a2/8t0 1.04 1.06 1.08 1.10 1.12 1.14 Φ Ds CL a ≈0.087 fm a ≈0.077 fm a ≈0.065 fm a ≈0.05 fm Figure 13: Continuum limit extrapolation of the best fits according to the TIC criteria applied to ΦD (top) and ΦDs (bottom). Results are shown for the flavour-averaged matching condition ϕ(1) H . The blue band represents the projection to the physical ϕ2 = ϕphys 2 and ϕH = ϕphys H , while the red square symbols denote the results in the continuum.ϕ(1) H ϕ(2) H combined ΦD 0.8624(78)(7) 0.8583(75)(8) 0.8606(76)(21) ΦDs 1.0352(61)(9) 1.0295(60)(11) 1.0328(60)(30) Table 4: Model average results for the observables Φ D and Φ Ds — defined in Eq. (5.3) — which are related to the fD and fDs decay constants, respectively, for the two different match- ing quantities ϕ(i) H . The last column reports the result of the combination of these two matching conditions. The first error is statistical while the second is the estimate of systematic uncer- tainty arising from the model averaging procedure. described by leading cutoff effects of O(a2), as expected for our valence action when it is tuned to maximal twist. In Table 4 we show our determinations of Φ D and ΦDs for each of the two procedures to match the charm scale, as well as the result from their combination. Using this combination we arrive at the following results for the the D(s) meson decay constants, fD = 211 .3(1.9)(0.6) MeV, (5.15) fDs = 247 .0(1.9)(0.7) MeV, (5.16) where the first error is statistical and the second the systematic uncertainty from the model average. The error budget for the D(s) decay constants is dominated by the statistical uncer- tainty of correlators and the error on chiral-continuum extrapolations. Therefore, we expect that a future addition of other ensembles with finer lattice spacing and physical pion masses will contribute to significantly reduce the uncertainty of our current determination. The dif- ferent contributions to the variance of D(s) meson decay constants are shown in Figure 14. Finally, in Figure 15 we show a comparison between our results and other Nf = 2 + 1 lattice QCD determinations. 5.3 Direct determination of fDs/fD In addition to the determination of fD and fDs, we investigate the direct determination of the ratio fDs/fD from a dedicated fit. This allows for a consistency check, since the ratio is dimensionless and thus does not require normalisation with a reference scale such as √8t0. One particular consequence is thus that this approach is only indirectly subject to the uncertainty of the lattice scale setting. Another advantage is that the ratio is exactly 1 by construction when ms = ml, i.e., the symmetric point of our ϕ4 = constant trajectory, which is part of our line of constant physics. We can thus perform a fit that is highly constrained in the unphysical masses region, although at the price of reducing the total number of ensembles entering in the study of the approach to the physical point. A first set of fit ansaetze is derived from the HM χPT expressions considered above for ΦD(s) . The generic form is ΦDs ΦD = h 1 + \u0010 δΦDs χPT − δΦD χPT \u0011i\u0002 1 + \u0000 δΦDs a − δΦDs a \u0001\u0003 . (5.17) 27fD Scale setting Stat. +¬-cont. limit Model av. fDs Scale setting Stat. +¬-cont. limit Model av. Figure 14: Relative contributions to the total error of our determinations of fD (left) and fDs (right). The label statistical plus χ-continuum limit represents the error arising from the statistical accuracy of our data and the chiral-continuum extrapolations. The scale setting label denotes the error coming from the physical valuetphys 0 as determined within our setup [1], while the model average represents the systematic error arising from the model variation according to the TIC procedure. Here δΦ D(s) χPT introduced in Eq. (5.7) labels the light quark mass dependence of the ratio, while δΦ D(s) a from Eq. (5.12) controls the continuum approach. It is worth noticing that at leading order the physical dependence on ϕH, and also the lattice spacing dependence related to ϕH, cancel out when expanding the ratio. Collecting all the terms entering in Eq. (5.17) from the previous section, we end up with ΦDs ΦD = \" 1 − 1 + 3g2 64π2ϕ2 f [2LK + Lη − 3Lπ] + 4 (2ϕ4 − 3ϕ2) ϕ2 f   p(0) χ + p(2) χ ϕ2 ϕ2 f + p(4) χ ϕH !# × \u0014 1 + a2 8t0 (2ϕ4 − 3ϕ2) \u0010 p(1) a + p(3) a ϕ2 H \u0011\u0015 . (5.18) In this expression we consider all the possible combinations of non-vanishing fit parameters, and perform our TIC-weighted model average among the different functional forms tested to quote a systematic uncertainty. Given that various terms cancel in the HM χPT expressions, we will further explore the systematic uncertainties by considering also functional forms based on a Taylor expansion of ΦD(s) . The generic expression then reads ΦD(s) = \u0010 ΦD(s) \u0011 χ [1 + δΦh,Taylor] h 1 + δΦ D(s) m,Taylor ih 1 + δΦ D(s) a i , (5.19) where \u0010 ΦD(s) \u0011 χ is the value in the chiral limit and at the physical value of the heavy-quark 28180 200 220 240 230 250 270 MeVfD fDs FLAG21 This work χQCD 20A RBC/UKQCD 17 χQCD 14 HPQCD 12A FNAL/MILC 11 PACS-CS 11 HPQCD 10A Figure 15: Comparison of our results for fD and fDs with those from lattice QCD collaborations based on simulations with Nf = 2 + 1 dynamical flavours as well as with FLAG21 averages [9]. Only data points with filled symbols contribute to the FLAG averages. Starting from the bottom, results are taken from: HPQCD 10 [28], PACS-CS 11 [88], FNAL/MILC 11 [29], HPQCD 12A [30], χQCD 14 [35], RBC/UKQCD 17 [39], χQCD 20A [89]. mass. In this expansion, the heavy and light mass dependence terms read δΦh,Taylor = p(0) h   1 ϕH − 1 ϕphys H ! + p(1) h   1 ϕH − 1 ϕphys H !2 , δΦD m,Taylor = p(0) m ϕ4 + ϕ2 \" p(1) m + p(2) m ϕ2 + p(3) m   1 ϕH − 1 ϕphys H !# , δΦDs m,Taylor = p(0) m ϕ4 + 2(ϕ4 − ϕ2) \" p(1) m + p(2) m ϕ2 + p(3) m   1 ϕH − 1 ϕphys H !# . (5.20) The lattice spacing dependence δΦ D(s) a can be parameterised in a similar fashion to that in Eq. (5.12). Combining these expressions into a functional form for the ratio of decay constants one then has ΦDs ΦD = \" 1 + (2ϕ4 − 3ϕ2) \" p(1) m + p(2) m ϕ2 + p(3) m   1 ϕH − 1 ϕphys H !## × \u0014 1 + a2 8t0 (2ϕ4 − 3ϕ2) \u0010 p(1) a + p(3) a ϕ2 H \u0011\u0015 . (5.21) Then, in order to arrive at a final determination of fDs/fD we perform a model average among all the HM χPT and Taylor functional forms for the two different matching conditions 29ϕ(1) H ϕ(2) H combined fDs/fD 1.177(15)(6) 1.178(15)(6) 1.177(15)(5) Table 5: Results of the model average forfDs/fD for the two charm-quark matching conditions. The last column reports the combined result. The first error is statistical while the second is the systematic uncertainty arising from the model variation procedure. simultaneously. In Table 5 we report our results for the ratio of decay constants from the model average separately for each charm matching condition, as well as their combination. Also for the ratio we observe good agreement for the two different ϕ(i) H tested in this work. Finally, for the result combining the two matching conditions, we quote fDs fD = 1.177(15)(5), (5.22) where the first error is statistical and the second is the systematic uncertainty based on the model average procedure. In Figure 16 we show the HM χPT chiral-continuum fit of the Φ Ds/ΦD ratio with highest weight in the model averaging procedure. In particular the plot on the left shows the chi- ral approach to the physical point, while the plot on the right represents the lattice spacing dependence. The observed dependence on ϕ2 shows only a mild curvature arising from the chiral logs, while cutoff effects appear to be highly suppressed at the current level of statistical precision of our data. Figure 17 shows a summary of the model average procedure for the ratio Φ Ds/ΦD, dis- playing the fit results for the two matching conditions together with the associated weights, for the HMχPT and Taylor functional forms. In Figure 18 we show the major error sources contributing to our final determination of the ratio, where we notice that the major contribution is given by the statistical and chiral- continuum error. Finally, in Figure 19 we show a comparison between our result for fDs/fD, the FLAG21 average and results from other collaborations. 6 Conclusions and outlook In this work we have described our first computations of physical observables in the charm sector using the Wilson fermion mixed-action setup described in greater detail in [1]. Emphasis is put in setting up our methodology and exhibiting the characteristics of the framework. Our results for the charm quark mass and the D(s) meson decay constants are based on a subset of CLS ensembles, yet they already sport a level of precision similar to that of several state-of-the-art results. We quote the values MRGI c (Nf = 3) = 1.485(8)(3)(14)[17] GeV, fD = 211.3(1.9)(0.6)[2.0] MeV, fDs = 247.0(1.9)(0.7)[2.1] MeV, fDs/fD = 1.177(15)(5)[16], (6.1) 300.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 φ2 1.00 1.05 1.10 1.15 1.20 1.25 Φ Ds/Φ D Mπ = MK φ2 = φphys 2 a ≈0.087 fm a ≈0.065 fm a ≈0.05 fm 0.00 0.01 0.02 0.03 0.04 0.05 a2/8t0 1.17 1.18 1.19 1.20 1.21 1.22 1.23 1.24 Φ Ds/Φ D φ2 = φphys 2 a ≈0.087 fm a ≈0.065 fm a ≈0.05 fm Figure 16: Illustration of the chiral-continuum extrapolation of the ratio Φ Ds/ΦD for the HMχPT model with highest TIC value. Results are shown for the flavour-averaged matching condition. Top: Chiral approach to the physical point. The dashed lines illustrate the chiral trajectories at finite lattice spacing, while the blue shaded band is a projection of the continuum fit. The red square symbol represents the physical result in the continuum. The black cross symbol corresponds to the symmetric point. Data points at finite lattice spacing are projected to the physical charm quark mass. Bottom: Lattice spacing dependence of Φ Ds/ΦD. The red square symbol indicates the continuum result, while the blue shaded band shows the fitted functional dependence on the lattice spacing. Points at finite lattice spacing are projected to the physical values of ϕ2 and ϕH. as our main results. For the RGI charm quark mass in the 3-flavour theory, MRGI c (Nf = 3), the first uncertainty is statistical, the second corresponds to the systematic error arising from 31[’HMChPT’] [’p(4)’] [’p(4)’, ’pa(3)’] [’p(2)’] [’p(2)’, ’pa(3)’] [’p(2)’, ’p(4)’] [’p(2)’, ’p(4)’, ’pa(3)’] [’taylor’] [’pm(2)’] [’pm(2)’, ’pa(3)’] [’HMChPT’] [’p(4)’] [’p(4)’, ’pa(3)’] [’p(2)’] [’p(2)’, ’pa(3)’] [’p(2)’, ’p(4)’] [’p(2)’, ’p(4)’, ’pa(3)’] [’taylor’] [’pm(2)’] [’pm(2)’, ’pa(3)’] 1.18 1.19 1.20 1.21 1.22 1.23 1.24 1.25 Φ Ds/Φ D Systematic error Model average 0.02 0.04 0.06 0.08 0.10 0.12 0.14 W Figure 17: Summary of the model average procedure for the ratio Φ Ds/ΦD based on the combination of the two matching conditions, ϕ(1) H and ϕ(2) H . Each circular symbol represents the result of a specific functional form, and the opacity is associated to the normalised weight W of the model based on its TIC value. The yellow band represents the systematic uncertainty arising from the set of tested models, while the left-most red point is our final averaged result. The labels of the 20 models specified in the horizontal axis are related to the terms characterising the dependencies on the mass and lattice spacing in the following way: ‘HMChPT’ stands for the expression in Eq. (5.18) where only the leading terms depending on the fit parameters p(0) χ and p(1) a are considered . Similarly, ‘taylor’ refers to Eq. (5.18) where only the terms depending on the fit parameters p(1) m and p(1) a are kept. The labels ‘p(2)’ and ‘p(4)’ correspond to the addition of the higher order terms depending on the parameters p(2) χ and p(4) χ in Eq. (5.18), respectively, while ‘pm(2)’ denotes the addition of p(2) m from Eq. (5.21). Finally, ‘p(3)’ denotes the inclusion of the fit parameter p(3) a parameterising higher order lattice spacing dependence appearing in both the HM χPT and Taylor functional forms in Eq. (5.18) and Eq. (5.21). the model selection, the third arises from the RGI running factor in Eq. (4.2), and the last one in brackets is the total error. For the decay constants fD, fDs and their ratio fDs/fD, the first error is statistical and the second is the systematic uncertainty from the model averaging, and the total error is given in brackets. We foresee that these results could be improved in the future by means of a more extensive analysis including additional CLS ensembles with a finer value of the lattice spacing and physical pion mass simulations. This is expected to have a significant impact in reducing the statistical uncertainty of the decay constants. The error on the charm quark mass, on the other hand, is dominated by the uncertainty induced by the non-perturbative renormalisation group running and thus work on that front would be required to improve the precision significantly. 32fDs /fD Stat. +¬-cont. limit Scale Setting Model av. fDs /fD H105 J303 D200 H102 Other Figure 18: Left: Relative contributions to the total error on the determination of the ratio fDs/fD. The label statistical plus χ-continuum limit represents the error arising from the statistical accuracy of our data and the chiral-continuum extrapolation. The scale setting label denotes the error coming from the physical value tphys 0 , while the model average represents the systematic error arising from the model variation according to the TIC procedure. Right: Details of the relative contributions to the statistical and chiral-continuum extrapolation error arising from specific gauge field configuration ensembles. In a related line of work, we are also applying our framework to the computation of semileptonic form factors for charmed meson decay, for which preliminary results have already been presented in [91, 92]. Together with the computations illustrated in this paper, they show how a comprehensive programme of precision heavy-flavour physics can be pursued in the framework of Wilson fermion regularisations, reaching an excellent compromise between the latter’s advantages from the point of view of field-theoretical control and the aim of high- precision computations. Acknowledgements We are grateful to our colleagues in the Coordinated Lattice Simulations (CLS) initiative for the generation of the gauge field configuration ensembles employed in this study. Discus- sions with, and input from, our colleagues Mattia Bruno, Fabian Joswig, Simon Kuberski, Alberto Ramos and Stefan Schaefer is gratefully acknowledged. We acknowledge PRACE for awarding us access to MareNostrum at Barcelona Supercomputing Center (BSC), Spain and to HAWK at GCS@HLRS, Germany. The authors thankfully acknowledge the computer resources at MareNostrum and the technical support provided by Barcelona Supercomput- ing Center (FI-2020-3-0026). We thank CESGA for granting access to Finis Terrae II. This work is partially supported by grants PGC2018-094857-B-I00 and PID2021-127526NB-I00, funded by MCIN/AEI/10.13039/501100011033 and by “ERDF A way of making Europe”, and by the Spanish Research Agency (Agencia Estatal de Investigaci´ on) through grants IFT 331.12 1.14 1.16 1.18 1.20 fDs/fD FLAG21 This work χQCD 20A RBC/UKQCD 18A RBC/UKQCD 17 HPQCD 12A FNAL/MILC 11 PACS-CS 11 Figure 19: Comparison of our determination of fDs/fD with those of the other lattice QCD collaborations based on Nf = 2+1 dynamical simulations as well as with the FLAG average [9]. Only the results with filled symbols contribute to this average. Starting from the bottom, results are taken from: PACS-CS 11 [88], FNAL/MILC 11 [29], HPQCD 12A [30], RBC/UKQCD 17 [39], RBC/UKQCD 18A [90], χQCD 20A [89]. Centro de Excelencia Severo Ochoa SEV-2016-0597 and No CEX2020-001007-S, funded by MCIN/AEI/10.13039/501100011033. We also acknowledge support from the project H2020- MSCAITN-2018-813942 (EuroPLEx), under grant agreementNo. 813942, and the EU Horizon 2020 research and innovation programme, STRONG-2020 project, under grant agreement No 824093. Appendix A GEVP implementation In this work, ground state meson masses and matrix elements are extracted from a gener- alised eigenvalue problem (GEVP) variational method following [64]. The GEVP has the form described in Section 3, cf. Eq. (3.3) and the discussion that follows. Considering only the first N state contributions in the spectral expansion, we can extract their effective energies from the eigenvalues λn(t, tref) as aEeff n (t, tref) ≡ log \u0012 λn(t, tref) λn(t + a, tref) \u0013 = aEn + O(e−(EN+1−En)t). (A.1) Here the asymptotic behaviour O(e−(EN+1−En)t) is ensured exclusively in the regime tref ≥ t/2. Whenever tref is kept fixed the first unresolved excited state is the n + 1, and the asymptotic scaling behaves as O( e−(En+1−En)t), therefore providing shorter plateaus. In Fig. 20 we show 34a comparison of low-lying heavy-heavy pseudoscalar states as extracted from the GEVP with different values of tref. In general, we observe a similar behaviour when comparing different values of tref, with a slightly better convergence when the condition tref ≥ t/2 is fulfilled. In this work we therefore stick to this choice for plateau extraction by setting tref = t/2. As explained in the main text, in order to assess the systematic uncertainty associated with the extraction of the ground state signal from a plateau behaviour in the effective energies, we perform numerous fits by varying the time ranges of the fitting interval, and apply the model averaging procedure described in Appendix B — cf. the illustration in Fig. 1. As additional cross-checks and stability tests we also computed the first excited state from the GEVP. A comparison of the ground state and first excited state as is given in Fig. 21 together with the plateaus of choice. As we are only interested in ground state, we choose to stick to the 2 × 2 matrix formulation of the GEVP. In addition to the meson spectrum, in this work we also extract the matrix element ⟨0|Pqr|Pqr(p = 0)⟩ from the GEVP analysis by considering the normalised eigenvectorvn(t, tref) in Eq. (3.3), where we remind that |Pqr(p = 0)⟩ stands for a ground state. Namely, we define for each state n the number [64] Rn = (vn(t, tref), CP)(t)vn(t, tref))−1/2 eEnt/2, (A.2) where (·, ·) is the usual scalar product and CP is the GEVP matrix from Eq. (3.5). Then, the ground state matrix element is given by peff 0 (t, tref) = (v0(t, tref), CP,0))R0, (CP,0)k = (CP)k0 (A.3) The large distance behaviour of the effective matrix element is governed by peff 0 (t, tref) = p0 + O(e−(EN+1−E0)tref ), p 0 = ⟨0|Pqr|Pqr(p = 0)⟩, (A.4) in the regime where the conditiontref ≥ t/2 is satisfied. We perform constant fits in a number of time intervals and use the model averaging procedure in Appendix B to estimate the systematic uncertainty due to excited-state contamination. In Figure 11 we show a representative plateau for a heavy-light decay constant, together with a summary of the model average with different fit intervals. Appendix B Model averaging procedure In this work, the systematic uncertainties are estimated from a model averaging procedure discussed in detail in [1]. Here we collect the main ideas and point to the relevant background references. As is often the case in lattice QCD calculations, in this study we deal with fits to highly correlated data. The dichotomy thus arises between trying correlated χ2 fits, which typically leads to numerical instabilities and potential biases in statistical estimators, or keeping an uncorrelated χ2, which is however not a suitable quantity to assess the goodness-of-fit. To overcome this situation, we follow an approach introduced in [26] based on the expectation value of the χ2, denoted χ2 exp, and its corresponding p-value, which does allow to quantify the goodness-of-fit in a controlled manner. Furthermore, we make use of the Takeuchi Information 350 10 20 30 40 50 t/a 0.98 0.99 1.00 1.01 1.02 1.03 1.04 1.05 1.06 ameﬀ H (t)/amH tref = a tref = 5a tref = t/2 20 30 40 1.000 1.005 Figure 20: Illustration of the ground-state effective masses determined from a GEVP analysis with three different ways of setting the value of tref for the ensemble J303. The effective masses are normalised by the central value of the mass extracted from conservative plateau choices. The parameter tref is either kept fixed, tref/a = 1, 5, or varied by setting tref = t/2 in such a way that the condition tref ≥ t/2 is fulfilled. Criteria (TIC) proposed in [25] to assign a weight to each model, which then allows to per- form a weighted model average to arrive at a final result for the systematic uncertainty [24]. Specifically, the value of the TIC assigned to each fitting model is TIC = χ2 − 2χ2 exp . (B.1) To each model m in the complete set, consisting of M models, we assign a normalised weight Wm defined as follows Wm ∝ exp \u0012 − 1 2TICm \u0013 , MX m=1 Wm = 1 . (B.2) The result of the model average for an observable O that has been determined for each of the models is then given by ⟨O⟩ = MX m=1 Wm⟨O⟩m . (B.3) 360 10 20 30 40 50 60 t/a 0.475 0.500 0.525 0.550 0.575 0.600 0.625 0.650 amH n = 0 n = 1 Figure 21: Illustration of the ground state and first excited state for a heavy-light pseudoscalar meson mass as extracted from the GEVP for the ensemble J303. We use tref = t/2 such that the condition tref ≥ t/2 is fulfilled. The shaded bands correspond to the plateaus of choice. Finally, to estimate the systematic uncertainty arising from the model variation we employ the weighted variance defined as follows σ2 O = MX m=1 \u0012 Wm⟨O⟩2 m \u0013 − \u0012 MX m=1 Wm⟨O⟩m \u00132 . (B.4) References [1] A. Bussone, A. Conigli, J. Frison, G. Herdo´ ıza, C. Pena, D. Preti, J.´A. Romero, A. S´ aez and J. Ugarrio, Hadronic physics from a Wilson fermion mixed-action approach: Setup and scale setting , to appear . [2] M. L¨ uscher and S. Schaefer,Non-renormalizability of the HMC algorithm , JHEP 04 (2011) 104 [1103.1810]. [3] ALPHA collaboration, Critical slowing down and error analysis in lattice QCD simulations, Nucl. Phys. B 845 (2011) 93 [1009.5228]. 37[4] K. Symanzik, Continuum Limit and Improved Action in Lattice Theories. 1. Principles and φ4 Theory, Nucl. Phys. B 226 (1983) 187. [5] K. Symanzik, Continuum Limit and Improved Action in Lattice Theories. 2. O(N) Nonlinear Sigma Model in Perturbation Theory , Nucl. Phys. B 226 (1983) 205. [6] M. L¨ uscher and P. Weisz,On-Shell Improved Lattice Gauge Theories , Commun. Math. Phys. 97 (1985) 59. [7] M. L¨ uscher, S. Sint, R. Sommer and P. Weisz,Chiral symmetry and O(a) improvement in lattice QCD , Nucl. Phys. B 478 (1996) 365 [hep-lat/9605038]. [8] Flavour Lattice Averaging Groupcollaboration, FLAG Review 2019: Flavour Lattice Averaging Group (FLAG), Eur. Phys. J. C 80 (2020) 113 [1902.08191]. [9] Flavour Lattice Averaging Group (FLAG)collaboration, FLAG Review 2021, Eur. Phys. J. C 82 (2022) 869 [2111.09849]. [10] M. Bruno et al., Simulation of QCD with N f = 2 + 1 flavors of non-perturbatively improved Wilson fermions, JHEP 02 (2015) 043 [1411.3982]. [11] M. L¨ uscher and S. Schaefer,Lattice QCD without topology barriers , JHEP 07 (2011) 036 [1105.4749]. [12] M. L¨ uscher and S. Schaefer,Lattice QCD with open boundary conditions and twisted-mass reweighting, Comput. Phys. Commun. 184 (2013) 519 [1206.2809]. [13] ALPHA collaboration, Lattice QCD with a chirally twisted mass term , JHEP 08 (2001) 058 [hep-lat/0101001]. [14] C. Pena, S. Sint and A. Vladikas, Twisted mass QCD and lattice approaches to the Delta I = 1/2 rule , JHEP 09 (2004) 069 [hep-lat/0405028]. [15] R. Frezzotti and G.C. Rossi, Chirally improving Wilson fermions. 1. O(a) improvement , JHEP 08 (2004) 007 [hep-lat/0306014]. [16] G. Herdo´ ıza, C. Pena, D. Preti, J.A. Romero and J. Ugarrio, A tmQCD mixed-action approach to flavour physics , EPJ Web Conf. 175 (2018) 13018 [1711.06017]. [17] ALPHA collaboration, First results for charm physics with a tmQCD valence action , PoS LATTICE2018 (2018) 271 [1812.05458]. [18] ALPHA collaboration, Heavy-quark physics with a tmQCD valence action , PoS LATTICE2018 (2019) 270 [1812.01474]. [19] ALPHA collaboration, Matching of Nf = 2 + 1 CLS ensembles to a tmQCD valence sector, PoS LATTICE2018 (2019) 318 [1903.00286]. [20] A. Conigli, A. Bussone, J. Frison, G. Herdo´ ıza, C. Pena, D. Preti, J.´A. Romero and J. Ugarrio, Charm physics with a tmQCD mixed action , PoS LATTICE2021 (2022) 091 [2112.00666]. 38[21] A. Bussone, A. Conigli, G. Herdo´ ıza, J. Frison, C. Pena, D. Preti, J.´A. Romero, A. S´ aez and J. Ugarrio, Light meson physics and scale setting from a mixed action with Wilson twisted mass valence quarks , PoS LATTICE2021 (2022) 258. [22] A. S´ aez, A. Conigli, J. Frison, G. Herdo´ ıza, C. Pena and J. Ugarrio,Scale Setting from a Mixed Action with Twisted Mass Valence Quarks , PoS LATTICE2022 (2023) 357. [23] A. Conigli, J. Frison, G. Herdo´ ıza, C. Pena, A. S´ aez and J. Ugarrio,Towards precision charm physics with a mixed action , PoS LATTICE2022 (2023) 351 [2212.11045]. [24] W.I. Jay and E.T. Neil, Bayesian model averaging for analysis of lattice field theory results, Phys. Rev. D 103 (2021) 114502 [2008.01069]. [25] J. Frison, Towards fully bayesian analyses in Lattice QCD , 2302.06550. [26] M. Bruno and R. Sommer, On fits to correlated and auto-correlated data , Comput. Phys. Commun. 285 (2023) 108643 [2209.14188]. [27] C. McNeile, C.T.H. Davies, E. Follana, K. Hornbostel and G.P. Lepage, High-Precision c and b Masses, and QCD Coupling from Current-Current Correlators in Lattice and Continuum QCD, Phys. Rev. D 82 (2010) 034512 [1004.4285]. [28] C.T.H. Davies, C. McNeile, E. Follana, G.P. Lepage, H. Na and J. Shigemitsu, Update: Precision Ds decay constant from full lattice QCD using very fine lattices , Phys. Rev. D 82 (2010) 114504 [1008.4018]. [29] Fermilab Lattice, MILCcollaboration, B- and D-meson decay constants from three-flavor lattice QCD, Phys. Rev. D 85 (2012) 114506 [1112.3051]. [30] H. Na, C.T.H. Davies, E. Follana, G.P. Lepage and J. Shigemitsu, |Vcd| from D Meson Leptonic Decays, Phys. Rev. D 86 (2012) 054510 [1206.4936]. [31] ETM collaboration, B-physics from Nf = 2 tmQCD: the Standard Model and beyond , JHEP 03 (2014) 016 [1308.1851]. [32] B. Chakraborty, C.T.H. Davies, B. Galloway, P. Knecht, J. Koponen, G.C. Donald, R.J. Dowdall, G.P. Lepage and C. McNeile, High-precision quark masses and QCD coupling from nf = 4 lattice QCD, Phys. Rev. D 91 (2015) 054508 [1408.4169]. [33] European Twisted Masscollaboration, Up, down, strange and charm quark masses with Nf = 2+1+1 twisted mass lattice QCD , Nucl. Phys. B 887 (2014) 19 [1403.4504]. [34] C. Alexandrou, V. Drach, K. Jansen, C. Kallidonis and G. Koutsou, Baryon spectrum with Nf = 2 + 1 + 1twisted mass fermions , Phys. Rev. D 90 (2014) 074501 [1406.4310]. [35] Y.-B. Yang et al., Charm and strange quark masses and fDs from overlap fermions , Phys. Rev. D 92 (2015) 034517 [1410.3343]. [36] N. Carrasco et al., Leptonic decay constants fK, fD, and fDs with Nf = 2 + 1 + 1 twisted-mass lattice QCD, Phys. Rev. D 91 (2015) 054507 [1411.7908]. 39[37] K. Nakayama, B. Fahy and S. Hashimoto, Short-distance charmonium correlator on the lattice with M¨ obius domain-wall fermion and a determination of charm quark mass , Phys. Rev. D 94 (2016) 054507 [1606.01002]. [38] A. Bazavov et al., B- and D-meson leptonic decay constants from four-flavor lattice QCD, Phys. Rev. D 98 (2018) 074512 [1712.09262]. [39] P.A. Boyle, L. Del Debbio, A. J¨ uttner, A. Khamseh, F. Sanfilippo and J.T. Tsang, The decay constants fD and fDs in the continuum limit of Nf = 2 + 1 domain wall lattice QCD, JHEP 12 (2017) 008 [1701.02644]. [40] Fermilab Lattice, MILC, TUMQCDcollaboration, Up-, down-, strange-, charm-, and bottom-quark masses from four-flavor lattice QCD , Phys. Rev. D 98 (2018) 054517 [1802.04248]. [41] R. Balasubramamian and B. Blossier, Decay constant of Bs and B∗ s mesons from Nf = 2 lattice QCD, Eur. Phys. J. C 80 (2020) 412 [1912.09937]. [42] P. Petreczky and J.H. Weber, Strong coupling constant and heavy quark masses in ( 2+1 )-flavor QCD, Phys. Rev. D 100 (2019) 034519 [1901.06424]. [43] HPQCD collaboration, Charmonium properties from lattice QCD+QED : Hyperfine splitting, J/ψ leptonic width, charm quark mass, and ac µ, Phys. Rev. D 102 (2020) 054511 [2005.01845]. [44] Extended Twisted Masscollaboration, Quark masses using twisted-mass fermion gauge ensembles, Phys. Rev. D 104 (2021) 074515 [2104.13408]. [45] ALPHA collaboration, Determination of the charm quark mass in lattice QCD with 2 + 1 flavours on fine lattices , JHEP 05 (2021) 288 [2101.02694]. [46] M. L¨ uscher and P. Weisz,Computation of the Action for On-Shell Improved Lattice Gauge Theories at Weak Coupling , Phys. Lett. B 158 (1985) 250. [47] B. Sheikholeslami and R. Wohlert, Improved Continuum Limit Lattice Action for QCD with Wilson Fermions, Nucl. Phys. B 259 (1985) 572. [48] M. Bruno, T. Korzec and S. Schaefer, Setting the scale for the CLS 2 + 1 flavor ensembles, Phys. Rev. D 95 (2017) 074504 [1608.08900]. [49] D. Mohler, S. Schaefer and J. Simeth, CLS 2+1 flavor simulations at physical light- and strange-quark masses, EPJ Web Conf. 175 (2018) 02010 [1712.04884]. [50] D. Mohler and S. Schaefer, Remarks on strange-quark simulations with Wilson fermions , Phys. Rev. D 102 (2020) 074506 [2003.13359]. [51] K.G. Wilson, Confinement of Quarks , Phys. Rev. D 10 (1974) 2445. [52] R. Frezzotti and G.C. Rossi, Chirally improving Wilson fermions. II. Four-quark operators, JHEP 10 (2004) 070 [hep-lat/0407002]. 40[53] ALPHA collaboration, Non-perturbative quark mass renormalisation and running in Nf = 3 QCD, Eur. Phys. J. C 78 (2018) 387 [1802.05243]. [54] R. Frezzotti, G. Martinelli, M. Papinutto and G.C. Rossi, Reducing cutoff effects in maximally twisted lattice QCD close to the chiral limit , JHEP 04 (2006) 038 [hep-lat/0503034]. [55] P. Dimopoulos, H. Simma and A. Vladikas, Quenched B(K)-parameter from Osterwalder-Seiler tmQCD quarks and mass-splitting discretization effects , JHEP 07 (2009) 007 [0902.1074]. [56] A. Shindler, Twisted mass lattice QCD , Phys. Rept. 461 (2008) 37 [0707.4093]. [57] M. L¨ uscher,Properties and uses of the Wilson flow in lattice QCD , JHEP 08 (2010) 071 [1006.4518]. [58] RQCD collaboration, Scale setting and the light baryon spectrum in N f = 2 + 1 QCD with Wilson fermions , JHEP 05 (2023) 035 [2211.03744]. [59] B. Strassberger et al., Scale setting for CLS 2+1 simulations , PoS LATTICE2021 (2022) 135 [2112.06696]. [60] G.M. de Divitiis, R. Petronzio and N. Tantalo, Distance preconditioning for lattice Dirac operators, Phys. Lett. B 692 (2010) 157 [1006.4028]. [61] S. Collins, K. Eckert, J. Heitger, S. Hofmann and W. Soeldner, Charmed pseudoscalar decay constants on three-flavour CLS ensembles with open boundaries , PoS LATTICE2016 (2017) 368 [1701.05502]. [62] ALPHA collaboration, Monte Carlo errors with less errors , Comput. Phys. Commun. 156 (2004) 143 [hep-lat/0306017]. [63] A. Ramos, Automatic differentiation for error analysis of Monte Carlo data , Comput. Phys. Commun. 238 (2019) 19 [1809.01289]. [64] B. Blossier, M. Della Morte, G. von Hippel, T. Mendes and R. Sommer, On the generalized eigenvalue method for energies and matrix elements in lattice field theory , JHEP 04 (2009) 094 [0902.1265]. [65] Particle Data Groupcollaboration, Review of Particle Physics , PTEP 2022 (2022) 083C01. [66] QCD-TARO collaboration, Contribution of disconnected diagrams to the hyperfine splitting of charmonium , JHEP 08 (2004) 004 [hep-lat/0404016]. [67] G.C. Donald, C.T.H. Davies, R.J. Dowdall, E. Follana, K. Hornbostel, J. Koponen, G.P. Lepage and C. McNeile, Precision tests of the J/ψ from full lattice QCD: mass, leptonic width and radiative decay rate to ηc, Phys. Rev. D 86 (2012) 094501 [1208.2855]. [68] HPQCD collaboration, B-meson decay constants: a more complete picture from full lattice QCD, Phys. Rev. D 91 (2015) 114509 [1503.05762]. 41[69] HPQCD‡ collaboration, Precise determination of decay rates for ηc → γγ, J/ψ → γηc, and J/ψ → ηce+e− from lattice QCD, Phys. Rev. D 108 (2023) 014513 [2305.06231]. [70] H. Georgi, An Effective Field Theory for Heavy Quarks at Low-energies , Phys. Lett. B 240 (1990) 447. [71] P.A. Baikov, K.G. Chetyrkin and J.H. K¨ uhn,Quark Mass and Field Anomalous Dimensions to O(α5 s), JHEP 10 (2014) 076 [1402.6611]. [72] T. Luthe, A. Maier, P. Marquard and Y. Schr¨ oder,Five-loop quark mass and field anomalous dimensions for a general gauge group , JHEP 01 (2017) 081 [1612.05512]. [73] P.A. Baikov, K.G. Chetyrkin and J.H. K¨ uhn,Five-loop fermion anomalous dimension for a general gauge group from four-loop massless propagators , JHEP 04 (2017) 119 [1702.01458]. [74] P.A. Baikov, K.G. Chetyrkin and J.H. K¨ uhn,Five-Loop Running of the QCD coupling constant, Phys. Rev. Lett. 118 (2017) 082002 [1606.08659]. [75] F. Herzog, B. Ruijl, T. Ueda, J.A.M. Vermaseren and A. Vogt, The five-loop beta function of Yang-Mills theory with fermions , JHEP 02 (2017) 090 [1701.01404]. [76] T. Luthe, A. Maier, P. Marquard and Y. Schroder, Complete renormalization of QCD at five loops, JHEP 03 (2017) 020 [1701.07068]. [77] T. Liu and M. Steinhauser, Decoupling of heavy quarks at four loops and effective Higgs-fermion coupling, Phys. Lett. B 746 (2015) 330 [1502.04719]. [78] K.G. Chetyrkin, J.H. Kuhn and M. Steinhauser, RunDec: A Mathematica package for running and decoupling of the strong coupling and quark masses , Comput. Phys. Commun. 133 (2000) 43 [hep-ph/0004189]. [79] B. Schmidt and M. Steinhauser, CRunDec: a C++ package for running and decoupling of the strong coupling and quark masses , Comput. Phys. Commun. 183 (2012) 1845 [1201.6149]. [80] F. Herren and M. Steinhauser, Version 3 of RunDec and CRunDec , Comput. Phys. Commun. 224 (2018) 333 [1703.03751]. [81] ALPHA collaboration, QCD Coupling from a Nonperturbative Determination of the Three-Flavor Λ Parameter, Phys. Rev. Lett. 119 (2017) 102001 [1706.03821]. [82] HPQCD collaboration, High-Precision Charm-Quark Mass from Current-Current Correlators in Lattice and Continuum QCD , Phys. Rev. D 78 (2008) 054513 [0805.2999]. [83] Y. Maezawa and P. Petreczky, Quark masses and strong coupling constant in 2+1 flavor QCD, Phys. Rev. D 94 (2016) 034507 [1606.08798]. [84] B. Grinstein, E.E. Jenkins, A.V. Manohar, M.J. Savage and M.B. Wise, Chiral perturbation theory for f D(s) / f D and B B(s) / B B , Nucl. Phys. B 380 (1992) 369 [hep-ph/9204207]. 42[85] J.L. Goity, Chiral perturbation theory for SU(3) breaking in heavy meson systems , Phys. Rev. D 46 (1992) 3929 [hep-ph/9206230]. [86] A.V. Manohar and M.B. Wise, Heavy quark physics , Camb. Monogr. Part. Phys. Nucl. Phys. Cosmol. 10 (2000) 1. [87] X.-D. Ji and M.J. Musolf, Subleading logarithmic mass dependence in heavy meson form-factors, Phys. Lett. B 257 (1991) 409. [88] PACS-CS collaboration, Charm quark system at the physical point of 2+1 flavor lattice QCD, Phys. Rev. D 84 (2011) 074505 [1104.4600]. [89] χQCD collaboration, Charmed and ϕ meson decay constants from 2+1-flavor lattice QCD, Chin. Phys. C 45 (2021) 023109 [2008.05208]. [90] RBC/UKQCD collaboration, SU(3)-breaking ratios for D(s) and B(s) mesons, 1812.08791. [91] J. Frison, A. Bussone, G. Herdo´ ıza, C. Pena, J.´A. Romero and J. Ugarrio, Heavy semileptonics with a fully relativistic mixed action , PoS LATTICE2019 (2019) 234 [1911.02412]. [92] J. Frison, A. Conigli, G. Herdo´ ıza and C. Pena,A comparison of Wilson and twisted mass valence quarks for charmed semileptonic form factors , PoS LATTICE2022 (2023) 408. 43",
      "references": [
        "Hadronic physics from a Wilson fermion mixed-action approach: Setup and scale setting",
        "Non-renormalizability of the HMC algorithm",
        "Critical slowing down and error analysis in lattice QCD simulations",
        "Continuum Limit and Improved Action in Lattice Theories. 1. Principles and φ4 Theory",
        "Continuum Limit and Improved Action in Lattice Theories. 2. O(N) Nonlinear Sigma Model in Perturbation Theory",
        "On-Shell Improved Lattice Gauge Theories",
        "Chiral symmetry and O(a) improvement in lattice QCD",
        "FLAG Review 2019: Flavour Lattice Averaging Group (FLAG)",
        "FLAG Review 2021",
        "Simulation of QCD with N f = 2 + 1 flavors of non-perturbatively improved Wilson fermions",
        "Lattice QCD without topology barriers",
        "Lattice QCD with open boundary conditions and twisted-mass reweighting",
        "Lattice QCD with a chirally twisted mass term",
        "Twisted mass QCD and lattice approaches to the Delta I = 1/2 rule",
        "Chirally improving Wilson fermions. 1. O(a) improvement",
        "A tmQCD mixed-action approach to flavour physics",
        "First results for charm physics with a tmQCD valence action",
        "Heavy-quark physics with a tmQCD valence action",
        "Matching of Nf = 2 + 1 CLS ensembles to a tmQCD valence sector",
        "Charm physics with a tmQCD mixed action",
        "Light meson physics and scale setting from a mixed action with Wilson twisted mass valence quarks",
        "Scale Setting from a Mixed Action with Twisted Mass Valence Quarks",
        "Towards precision charm physics with a mixed action",
        "Bayesian model averaging for analysis of lattice field theory results",
        "Towards fully bayesian analyses in Lattice QCD",
        "On fits to correlated and auto-correlated data",
        "High-Precision c and b Masses, and QCD Coupling from Current-Current Correlators in Lattice and Continuum QCD",
        "Update: Precision Ds decay constant from full lattice QCD using very fine lattices",
        "B- and D-meson decay constants from three-flavor lattice QCD",
        "|Vcd| from D Meson Leptonic Decays",
        "B-physics from Nf = 2 tmQCD: the Standard Model and beyond",
        "High-precision quark masses and QCD coupling from nf = 4 lattice QCD",
        "Up, down, strange and charm quark masses with Nf = 2+1+1 twisted mass lattice QCD",
        "Baryon spectrum with Nf = 2 + 1 + 1twisted mass fermions",
        "Charm and strange quark masses and fDs from overlap fermions",
        "Leptonic decay constants fK, fD, and fDs with Nf = 2 + 1 + 1 twisted-mass lattice QCD",
        "Short-distance charmonium correlator on the lattice with M¨ obius domain-wall fermion and a determination of charm quark mass",
        "B- and D-meson leptonic decay constants from four-flavor lattice QCD",
        "The decay constants fD and fDs in the continuum limit of Nf = 2 + 1 domain wall lattice QCD",
        "Up-, down-, strange-, charm-, and bottom-quark masses from four-flavor lattice QCD",
        "Decay constant of Bs and B∗ s mesons from Nf = 2 lattice QCD",
        "Strong coupling constant and heavy quark masses in ( 2+1 )-flavor QCD",
        "Charmonium properties from lattice QCD+QED : Hyperfine splitting, J/ψ leptonic width, charm quark mass, and ac µ",
        "Quark masses using twisted-mass fermion gauge ensembles",
        "Determination of the charm quark mass in lattice QCD with 2 + 1 flavours on fine lattices",
        "Computation of the Action for On-Shell Improved Lattice Gauge Theories at Weak Coupling",
        "Improved Continuum Limit Lattice Action for QCD with Wilson Fermions",
        "Setting the scale for the CLS 2 + 1 flavor ensembles",
        "CLS 2+1 flavor simulations at physical light- and strange-quark masses",
        "Remarks on strange-quark simulations with Wilson fermions",
        "Confinement of Quarks",
        "Chirally improving Wilson fermions. II. Four-quark operators",
        "Non-perturbative quark mass renormalisation and running in Nf = 3 QCD",
        "Reducing cutoff effects in maximally twisted lattice QCD close to the chiral limit",
        "Quenched B(K)-parameter from Osterwalder-Seiler tmQCD quarks and mass-splitting discretization effects",
        "Twisted mass lattice QCD",
        "Properties and uses of the Wilson flow in lattice QCD",
        "Scale setting and the light baryon spectrum in N f = 2 + 1 QCD with Wilson fermions",
        "Scale setting for CLS 2+1 simulations",
        "Distance preconditioning for lattice Dirac operators",
        "Charmed pseudoscalar decay constants on three-flavour CLS ensembles with open boundaries",
        "Monte Carlo errors with less errors",
        "Automatic differentiation for error analysis of Monte Carlo data",
        "On the generalized eigenvalue method for energies and matrix elements in lattice field theory",
        "Review of Particle Physics",
        "Contribution of disconnected diagrams to the hyperfine splitting of charmonium",
        "Precision tests of the J/ψ from full lattice QCD: mass, leptonic width and radiative decay rate to ηc",
        "B-meson decay constants: a more complete picture from full lattice QCD",
        "Precise determination of decay rates for ηc → γγ, J/ψ → γηc, and J/ψ → ηce+e− from lattice QCD",
        "An Effective Field Theory for Heavy Quarks at Low-energies",
        "Quark Mass and Field Anomalous Dimensions to O(α5 s)",
        "Five-loop quark mass and field anomalous dimensions for a general gauge group",
        "Five-loop fermion anomalous dimension for a general gauge group from four-loop massless propagators",
        "Five-Loop Running of the QCD coupling constant",
        "The five-loop beta function of Yang-Mills theory with fermions",
        "Complete renormalization of QCD at five loops",
        "Decoupling of heavy quarks at four loops and effective Higgs-fermion coupling",
        "RunDec: A Mathematica package for running and decoupling of the strong coupling and quark masses",
        "CRunDec: a C++ package for running and decoupling of the strong coupling and quark masses",
        "Version 3 of RunDec and CRunDec",
        "QCD Coupling from a Nonperturbative Determination of the Three-Flavor Λ Parameter",
        "High-Precision Charm-Quark Mass from Current-Current Correlators in Lattice and Continuum QCD",
        "Quark masses and strong coupling constant in 2+1 flavor QCD",
        "Chiral perturbation theory for f D(s) / f D and B B(s) / B B",
        "Chiral perturbation theory for SU(3) breaking in heavy meson systems",
        "Heavy quark physics",
        "Subleading logarithmic mass dependence in heavy meson form-factors",
        "Charm quark system at the physical point of 2+1 flavor lattice QCD",
        "Charmed and ϕ meson decay constants from 2+1-flavor lattice QCD",
        "SU(3)-breaking ratios for D(s) and B(s) mesons",
        "Heavy semileptonics with a fully relativistic mixed action",
        "A comparison of Wilson and twisted mass valence quarks for charmed semileptonic form factors"
      ],
      "meta_data": {
        "arxiv_id": "2309.14154v1",
        "authors": [
          "Andrea Bussone",
          "Alessandro Conigli",
          "Julien Frison",
          "Gregorio Herdoíza",
          "Carlos Pena",
          "David Preti",
          "Alejandro Sáez",
          "Javier Ugarrio"
        ],
        "published_date": "2023-09-25T14:07:57Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Introduces and validates a Wilson-fermion mixed-action lattice QCD framework for precision charm physics, combining CLS Nf=2+1 non-perturbatively O(a)-improved Wilson sea quarks (with open boundaries) with maximally twisted-mass Wilson valence quarks. Demonstrates automatic O(a) improvement for charm observables (eliminating O(a m_c) effects without tuning improvement coefficients) and continuum scaling consistent with leading O(a^2) cutoff effects. Provides first high-precision determinations (from a subset of CLS ensembles) of: (i) the charm quark mass (RGI and MSbar), (ii) leptonic decay constants f_D and f_{D_s}, and (iii) the ratio f_{D_s}/f_D, with controlled chiral/continuum extrapolations and systematic-error estimation via information-criterion model averaging.",
        "methodology": "Mixed-action setup: sea sector uses tree-level improved gauge action plus non-perturbatively O(a)-improved Wilson fermions; valence sector uses fully-twisted tmQCD with clover term. Ensemble-by-ensemble matching of valence light/strange to sea via gradient-flow based variables φ2=8 t0 m_π^2 and φ4=8 t0(½ m_π^2+m_K^2), and maximal twist enforced by vanishing O(a)-improved valence PCAC mass. Two-point correlators computed with time-diluted U(1) stochastic sources and distance-preconditioned solvers; masses/matrix elements extracted with a 2×2 time-shifted pseudoscalar GEVP. Charm mass matched through global chiral-continuum fits using two alternative charm proxies: (1) flavor-averaged heavy-light mass \u001bm_D\u001d=2/3 m_D+1/3 m_{D_s}; (2) connected η_c mass (η_c^{conn}) corrected for missing disconnected+QED effects. Renormalized charm mass obtained from twisted bare μ_c with nonperturbative Z_P in the Schrödinger Functional scheme, converted to RGI using nonperturbative running; MSbar masses obtained via 5-loop anomalous dimension/beta function plus 4-loop decoupling (RunDec). Decay constants computed without Z_A using exact tmQCD Ward identity relating conserved point-split vector current divergence to pseudoscalar density; f_PS from pseudoscalar matrix element. Chiral-continuum extrapolations: charm mass uses polynomial in φ2 and φ_H with flexible O(a^2) and optional O(a^4) terms (linear or multiplicative discretization ansätze). For decay constants, fit Φ_{D(s)}=(8 t0)^{3/4} f_{D(s)} √m_{D(s)} using HMχPT (with chiral logs and free g coupling) plus heavy-mass interpolation and O(a^2) terms; also dedicated fits for f_{D_s}/f_D using HMχPT-difference formula and Taylor alternatives. Systematics estimated via Takeuchi Information Criterion (TIC) weights and model averaging; goodness-of-fit assessed with χ^2 expectation/p-values for correlated data.",
        "experimental_setup": "Gauge ensembles: subset of CLS Nf=2+1 ensembles on a line of (approx.) constant tr{m_q^{(s)}} / constant φ4, with open boundary conditions. 10 ensembles (H101, H102, H105, H400, N202, N203, N200, D200, N300, J303) spanning four lattice spacings (~0.087, 0.077, 0.065, 0.05 fm), pion masses ~200–420 MeV, and volumes with m_π L ≈ 3.9–6.4. Scale setting: gradient-flow t0 determined using √(8 t0) f_{πK} with isoQCD inputs for f_π and f_K; final √t0^{phys}=0.1445(5)(3) fm. Correlator measurement: zero-momentum two-point functions with source at T/2 (bulk, far from open boundaries), 10 time-diluted stochastic sources per configuration; Gamma-method autocorrelation analysis and ADerrors for error propagation. Valence parameters: matched μ_l, μ_s, and critical mass per ensemble; charm sector simulated at 2–3 μ_c values around physical charm for interpolation. Validation/benchmarks: checks of automatic O(a) improvement via observed O(a^2) scaling; consistency tests showing that D and D_s masses extrapolate to isoQCD targets under both charm-matching prescriptions; model-averaging stability assessed through weighted fit-histograms and comparisons between matching schemes. Reported final key results (subset): M_c^RGI(Nf=3)=1.485(8)(3)(14) GeV; f_D=211.3(1.9)(0.6) MeV; f_{D_s}=247.0(1.9)(0.7) MeV; f_{D_s}/f_D=1.177(15)(5).",
        "limitations": "Results use only a subset of available CLS ensembles; lack of physical-pion-mass ensembles in this analysis increases reliance on chiral extrapolation (though down to m_π≈200 MeV) and limits precision improvements. Charm is partially quenched (no dynamical charm), so matching relies on selected valence observables and assumes controlled continuum unitarity restoration. Correlated fits are avoided due to ill-conditioned covariance matrices; the approach uses uncorrelated χ^2 with corrected goodness-of-fit measures, which may be less optimal than fully correlated analyses. Heavy-light vector masses are noisy; a spin-flavor averaged charm-matching condition involving vector states is disfavored, indicating limited control over those channels. Dominant uncertainty in M_c^RGI comes from nonperturbative RG running factor; MSbar results additionally depend on Λ_MS and perturbative truncation/decoupling uncertainties (especially at μ=m_c). Systematics from mixed-action unitarity violations are assumed to vanish in the continuum; residual O(a) terms proportional to a·tr(m_q^{sea}) exist but are argued to be very small (O(10^-2) and α_s^2-suppressed). QED and strong isospin-breaking effects are not included; isoQCD inputs and ad hoc corrections (e.g., η_c^{conn} shift) approximate these effects.",
        "future_research_directions": "Extend to the full CLS ensemble set, including finer lattice spacings and physical-pion-mass ensembles, to reduce statistical/chiral uncertainties and better constrain continuum scaling (including assessing necessity of O(a^4) terms). Improve dominant charm-mass uncertainties by refining nonperturbative renormalization and RG running (e.g., more precise Z_P and step-scaling, alternative schemes, or higher-statistics running determinations). Incorporate QED and strong-isospin-breaking corrections for direct comparison to experiment, and compute disconnected contributions explicitly for charmonium where relevant. Apply the mixed-action framework to additional charm observables: semileptonic form factors (D→π/K), CKM extractions, charm hadron spectroscopy, and possibly B-physics via step-scaling/HQET strategies. Explore improved treatment of heavy-light vector states (better interpolators, larger GEVP bases, smearing) to enable robust spin-averaged matching and hyperfine studies. Develop more robust correlated-fitting or shrinkage/regularized covariance approaches to exploit full correlations while maintaining numerical stability; expand model-averaging to include data cuts and alternative EFT-informed chiral/heavy-mass ansätze.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "PRewrite: Prompt Rewriting with Reinforcement Learning",
      "full_text": "Solving Nonlinear Absolute Value Equations Aris Daniilidis, Mounir Haddou, Tr´ı Minh Lˆe, Olivier Ley Abstract. In this work, we show that several problems, that are naturally represented as Nonlinear Absolute Value Equation problems (in short, NAVE), can be reformulated as Nonlinear Comple- mentarity Problems (in short, NCP) and, under mild conditions, they can be efficiently solved using smoothing regularizing techniques. To the best of our knowledge, this is the first numerical approach that deals directly with Nonlinear Absolute Value Equations. We also identify a commonly used technical assumption in smoothing techniques and prove its equivalence to a classical  Lojasiewicz inequality at infinity, confirming in particular that this assumption is not restrictive. The effective- ness of our approach is illustrated in several problems, including asymmetric ridge optimization and nonlinear ordinary differential equations. Keywords: Nonlinear Absolute Value Equality, Complementarity problem, P0-map, Numerical methods AMS Classification: Primary: 90C33, 65K10 ; Secondary: 15B48, 65K15, 90C59. 1 Introduction The last two decades, absolute value equation problems (in short, AVE problems) have been ex- tensively studied in the literature. This interest is justified by the fact that this class of problems already covers a wide spectrum of applications: indeed, numerous problems stemming from real-life applications, as for instance all mixed integer linear programming problems, can be reformulated as AVE problems. It is also well-known, see [24, 32] e.g., that AVE problems admit an equivalent description as Linear Complementarity problems (in short, LCP). The exact definitions of an AVE problem and a LCP are recalled below. Dealing efficiently with these problems is thus paramount. The literature on this subject contains several theoretical results for existence as well as conditions guaranteeing uniqueness of the solution [22, 23, 34, 35]. Concurrently, there are also various nu- merical approaches to solve an AVE problem. Generally speaking, these methods can be divided into at least three categories [3] : iterative linear algebra based methods (also known as projective methods), semi-smooth Newton-like methods and smoothing methods. The aforementioned meth- ods generally require some assumption on the matrix involved in the AVE problem. In particular, the classes of P0-matrices and P-matrices (recalled below) turn out to be particularly relevant in this study [1]. In this work, we consider a natural generalization of (linear) AVE problems to nonlinear ones, known as Nonlinear Absolute Value Equations (in short, NAVE). This more general framework encompasses new applications including ridge regression models, bounded constrained nonlinear systems of equations, and stiff Ordinary Differential Equations (in short, stiff ODE). This approach to deal with the aforementioned problems, based on NAVE, is to the best of our knowledge, com- pletely new in the literature. Our main contribution is the following: we first show that similarly to the way that an AVE problem 1 arXiv:2402.16439v2  [math.OC]  2 Oct 2024is linked to a LCP, nonlinear absolute value equations can also be associated to nonlinear comple- mentarity problems (in short, NCP). Indeed, any NCP can be reformulated as NAVE. The converse is also true, but the association is generally given in an implicit way. Then, taking profit from the huge literature concerning existence, uniqueness and numerical resolution of NCP (see [9,23,35,37] e.g.), we propose a new method to solve a NAVE problem, based on the smoothing technique proposed in [10] and further developments in the follow-up work [30]. The proposed approach is explained in Section 2, while in Section 3 we discuss applications. To ease the reading we start with some definitions and settings. The Absolute Value Equality problem (in short, AVE) is defined as follows: find x ∈ Rd : Ax − |x| = b, (AVE) where A is a ( d × d)-matrix and b ∈ Rd. Throughout this work, given x = (x1, . . . , xd)T ∈ Rd, we use the notation |x| := (|x1|, . . . ,|xd|)T componentwise to denote a vector in Rd +. Denoting by I the identity matrix of Rd and assuming that either A − I or A + I is invertible, the (AVE) problem can be transformed to a Linear Complementarity Problem (in short, LCP). Indeed, setting (coordinate by coordinate) ( y = x+ = max { x, 0} z = x− = max {−x, 0} (1.1) and performing the transformation x = y − z and |x| = y + z we obtain: (A − I) y − (A + I) z = b. Therefore for ( M := (A + I)−1(A − I) q := (A + I)−1(−b) or respectively ( fM := (A − I)−1(A + I) eq := (A − I)−1b (1.2) we obtain ( z = My + q 0 ≤ y ⊥ z ≥ 0 or respectively ( y = fMz + eq 0 ≤ y ⊥ z ≥ 0 (LCP) The above problem can be solved providedM (respectively fM) is a P-matrix (see below for details). It is important to notice that this property can be traced back to the matrix A; in particular, the property is ensured whenever the singular values of A are all greater than 1. Notice that this condition guarantees invertibility of both A − I and A + I. Solving (LCP) under the assumption that M (respectively fM) is a P-matrix has been treated in several works (see [4]). In this case, it can be shown that the problem has a unique solution (¯ y, ¯z) yielding that ¯x := ¯y − ¯z is the (unique) solution of (AVE). Moreover, this solution can be obtained numerically, via smoothing regularization techniques (see [1,10,30] and Section 2.3 below). In this work, we propose a new method of solving a nonlinear generalization of (AVE), namely the following Nonlinear Absolute Value Equality problem (in short, NAVE) Find x ∈ Rd : F(x) − |x| = 0 (coordinatewise), (NAVE) 2where F : Rd → Rd is a (nonlinear) mapping. By introducing new variables y = x+ and z = x− (cf. (1.1)) so that x = y − z and |x| = y + z, (NAVE) becomes F(y − z) − (y + z) = 0. By setting z = H(y) (which is possible under regularity assumptions on F, see Lemma 2.5), we can transform (NAVE) to a Nonlinear Complementarity Problem (NCP): ( H(y) = F(y − H(y)) − y 0 ≤ y ⊥ H(y) ≥ 0. (1.3) As we shall see in Section 2.3, even though the functionH is only defined implicitly, it is still possible to solve (1.3) numerically provided we are able to guarantee thatH is a P0-map (see Definition 2.2), a notion which generalizes P0–matrices (c.f. Lemma 2.7). This is the first work that directly addresses nonlinear absolute value equations, besides the fact that a variety of real problems and applications can naturally assume this form. Another important contribution related to smoothing techniques for NCP is that we clearly charac- terize a technical assumption commonly employed in functions–smoothing and prove its equivalence with the classical  Lojasiewicz inequality at infinity (see Theorem 2.8). This clearly reveals that the former assumption is not at all restrictive. 2 Setting of the problem and description of the method 2.1 Definitions and preliminaries Given a ( d × d) matrix A and I ⊂ {1, 2, ··· , d}, we denote by AII the submatrix made up of the rows and columns of I. Definition 2.1 (P0-matrix and P-matrix). A matrix A is called a P0–matrix (respectively, P– matrix) if one of the following equivalent properties holds (i) for every I ⊂ {1, 2, ··· , d}, det(AII ) ≥ 0 (respectively det(AII ) > 0); (ii) for every x = (x1, ··· , xd)T ∈ Rd, x ̸= 0, max 1≤j≤d (Ax)jxj ≥ 0 \u0012 respectively max 1≤j≤d (Ax)jxj > 0 \u0013 ; (iii) for every I ⊂ {1, 2, ··· , d}, the real eigenvalues of AII are nonnegative (respectively strictly positive). The notion of P-matrix can be generalized to general nonlinear maps H : Rd → Rd as follows: Definition 2.2 (P0–map and P–map). A mapping H : Rd → Rd is called P0-map (respectively, P-map), if for every x, y∈ Rd, x ̸= y, it holds: max j∈{1,...,d} \u0000 H(y)j − H(x)j\u0001 (yj−xj) ≥ 0 \u0012 respectively, max j∈{1,...,d} \u0000 H(y)j − H(x)j\u0001 (yj − xj) > 0 \u0013 ; 3If H is of the form H(x) = Ax + b for some ( d × d) matrix A and vector b ∈ Rd, then it follows directly that H is a P0–map (respectively, a P–map) if and only if A is a P0–matrix (respectively, a P–matrix). More generally, we have the following result: Lemma 2.3 ([27, Corollary 5.3, Theorem 5.8]) . Let H : Rd → Rd be C1. Then H is a P0-map if and only if, for every x ∈ Rd, ∇H(x) is a P0-matrix. We refer the reader to [8,27] for further results about P0– and P–matrices and maps. We finish this section with the following useful lemma. Lemma 2.4 (A characterization of P0–matrices). Let A be a (d×d) matrix. Then A is a P0-matrix if and only if, for every diagonal matrix ∆1 with strictly positive entries and for every nonnegative diagonal matrix ∆2, the matrix ∆1 + ∆2A is invertible. Proof. Let A be a P0-matrix. Then for every diagonal matrix ∆ 2 with nonnegative entries, the matrix ∆2A is also P0, while for every diagonal matrix ∆ 1 with strictly positive entries, the matrix ∆1 + ∆2A is a P-matrix, therefore, in particular, it is invertible. Conversely, let us assume that A is not a P0–matrix. Then there exists v ∈ Rd, v ̸= 0 such that (Av)ivi < 0, for every i ∈ {1, ··· , d}. (2.1) Let ∆1 = diag(δ1 1, ··· , δd 1) and ∆2 = diag(1, ··· , 1). Then (∆1 + ∆2A)v = (δ1 1v1 + (Av)1, ··· , δd 1vd + (Av)d)T , and by setting, for every i, δi 1 := −(Av)i/vi (which is well-defined and strictly positive thanks to (2.1)) we deduce that (∆ 1 + ∆2A)v = 0 and therefore ∆ 1 + ∆2A is not invertible. 2 2.2 Transforming a (NAVE) problem to a (NCP) problem Given a nonlinear mapping F : Rd → Rd, we consider the (NAVE) problem Find x ∈ Rd : F(x) − |x| = 0. By introducing new variables y = x+ and z = x− so that x = y −z, |x| = y + z, y ⊥ z, the (NAVE) problem becomes F(y − z) − (y + z) = 0. (2.2) The following lemma gives conditions under which (2.2) may be written as a (NCP) problem by setting y = H(z) or z = eH(y) for some suitable maps H or eH. Lemma 2.5. Assume that the mapping F is C1 in a neighborhood of the point x∗ = y∗ − z∗ ∈ Rd which is assumed to be solution of (2.2). Then it holds: (i). If F −I is a P0-map, then there exists a C1 map H : Rd → Rd defined in a neighborhood of y∗ such that z∗ = H(y∗) and y∗ is a solution to the following (NCP) problem: ( H(y) = F(y − H(y)) − y 0 ≤ y ⊥ H(y) ≥ 0. (2.3) 4(ii). If −(F + I) is a P0-map, then there exists a C1 map eH : Rd → Rd defined in a neighborhood of z∗ such that y∗ = eH(z∗) and z∗ is a solution to the following (NCP) problem: ( eH(z) = F( eH(z) − z) − z 0 ≤ z ⊥ eH(z) ≥ 0. (2.4) Proof. We consider the C1 map F : R2d → R defined by F(y, z) = F(y − z) − (y + z). We are going to apply the implicit function theorem at the point ( y∗, z∗) ∈ R2d. Notice that (∇yF(y∗, z∗), ∇zF(y∗, z∗)) = (∇F(y∗ − z∗) − I, −∇F(y∗ − z∗) − I). If F − I is a P0-map, then ∇F(x∗) − I is a P0-matrix by Lemma 2.3. Applying Lemma 2.4, we obtain that 2 I + ∇F(x∗) − I = ∇F(x∗) + I is invertible. Therefore ∇zF(y∗, z∗) is invertible and, by the implicit function theorem, we obtain a map H such that (i) holds. Similarly, if −(F + I) is a P0-map, then 2I − (∇F(z∗) + I) = I − ∇F(z∗) = −∇yF(y∗, z∗) is invertible and we obtain a map eH such that (ii) holds. 2 Remark 2.6. (i) The condition F − I (respectively, −(F + I)) being a P0-map is actually quite natural since it is exactly the requested assumptions to solve the (NCP) problem, see Section 2.3. (ii) (NAVE vs AVE). At this stage, the reader may have already noticed an analogy with the (LCP) reformulation of (AVE). Indeed, if F(x) = Ax−b, then (NAVE) coincides with (AVE), and if either A −I or −(A + I) is a P0-matrix (which is automatically satisfied if, e.g., the singular values of the matrix A are greater than 1), then the functions H and eH are explicitly given by the formulae H(y) = My + q and ˜H(y) = fMz + eq, where M, fM, q and eq appear in (1.2). Consequently, in this case it is possible to solve (AVE) as explained in the introduction. 2.3 Smoothing techniques to solve (NCP) As already mentioned, even though the functions H and eH are only implicitly defined, we can still solve (2.3)–(2.4) numerically (we shall do so below), whenever it is guaranteed that H, eH are P0-maps. This is the aim of the following lemma, yielding a criterium based on F. Lemma 2.7 (Guaranteeing P0-property for H, eH). (i). If F − I is a P0-map, then so is H. (ii). If −(F + I) is a P0-map, then so is eH. Proof. We now focus on the case of (2.3), the case of (2.4) can be adapted accordingly. Let y1, y2 ∈ Rd, with y1 ̸= y2. We infer from (2.3) that 2H(yk) = F (yk − H(yk)) − (yk − H(yk)) , k ∈ {1, 2}. 5Setting tk := yk − H(yk), it follows 2H(yk) = (F − I)(tk). Using the fact that F −I is a P0-map, we deduce that for some coordinate j = j(t1, t2) ∈ {1, . . . , d}, we have 2 \u0000 H(y1)j − H(y2)j\u0001 (tj 1 − tj 2) ≥ 0, from which we infer \u0000 H(y1)j − H(y2)j\u0001 \u0010 (yj 1 − yj 2) − (H(y1)j − H(y2)j) \u0011 ≥ 0, yielding \u0000 H(y1)j − H(y2)j\u0001 (yj 1 − yj 2) ≥ \u0000 H(y1)j − H(y2)j\u00012 ≥ 0. This is the desired property for the map H. 2 To solve (2.3), we will apply the smoothing approach proposed in [10] and more precisely the non-parametric technique introduced in [30]. The overall approach of [10] is based on functions θ satisfying the following properties: • the function θ : R → (−∞, 1) is concave, continuous and nondecreasing; • θ(t) < 0, for all t ∈ (−∞, 0), θ(0) = 0 and lim t→+∞ θ(t) = 1. These functions are used as certificate of positivity, that is, they “detect” whether t = 0 or t >0 holds in a “continuous way”, in the sense of the following characterization: t >0 ⇐⇒ lim r→0 θ \u0012t r \u0013 = 1. The authors in [10] used these functions to regularize the (nonsmooth) (NCP) 0 ≤ x ⊥ H(x) ≥ 0, (2.5) by means of a sequence of smooth systems (indexed by r >0) of the form Gr(x, H(x)) = \u0010 Gr(x, H(x))1, ··· , Gr(x, H(x))d \u0011T = (0, ··· , 0)T , (2.6) where Gr(x, H(x))i := rψ−1 \u0014 ψ \u0012xi r \u0013 + ψ \u0012H(x)i r \u0013\u0015 with ψ := 1 − θ. Then they eventually take the limit as r tends to 0. Several convergence results have been established under the assumption that the problem has at least one solution and H is a P0–map. Although this approach is efficient numerically, it suffers from two drawbacks: • There is no clear or optimal strategy to drive the parameter r to 0. 6• The following ad hoc technical assumption on the function ψ has been used without rigorous explanation: there exist a ∈ (0, 1) and Ra > 0 such that: ψ(t) 2 ≥ ψ \u00121 at \u0013 , for all t ∈ (Ra, +∞). (2.7) The first drawback has been addressed in [30] by considering a larger system of equations ( Gr(x, H(x)) = (0, ··· , 0)T , 1 2 ∥x−∥2 + 1 2 ∥H(x)−∥2 + r2 + εr = 0, (2.8) where ε >0 is some positive parameter. The second drawback will be the subject of the following result which proves that this technical assumption (2.7) corresponds to a well-known property. Theorem 2.8 (asymptotic behavior) . Let ψ : (0 , ∞) → (0, ∞) be a convex decreasing function satisfying lim x→∞ ψ(x) = inf ψ = 0. The following assertions are equivalent: (i). ( Lojasiewicz inequality at infinity) There exists c >0 such that lim inf x→∞ x|ψ′(x)| ψ(x) > c >0. (ii). There exist m, n >1 and R >0 such that: ψ(x) m ≥ ψ(nx), for all x ∈ (R, +∞) (2.9) (iii). For every m >1 there exist n >1 and R >0 such that: ψ(x) m ≥ ψ(nx), for all x ∈ (R, +∞) Notice that the technical assumption (2.7) corresponds to (ii). Therefore, the above result shows that it is equivalent to assume that ψ satisfies the  Lojasiewicz inequality at infinity. This latter condition is always satisfied if the function ψ is semialgebraic: Indeed, in this case, the correspond- ing Hardy field (that is, the field of germs of real semialgebraic functions at infinity) has rank one, and consequently, for any non-ultimately zero semi-algebraic function ψ in the single variable x, the function x 7→ xψ′(x)/ψ(x) has a non-zero limit as x goes to infinity (see [7, Remark 2.9]). The same argument applies also for the more general case of functions ψ that are definable in some polynomially bounded o-minimal structure (we refer to [6] for the corresponding definitions). Proof. (i)⇒(ii). Let us assume that (ii) fails. We define inductively a sequence {yn}n ⊂ [1, +∞) such that lim n→∞ yn = +∞ and lim n→∞ yn|ψ′(yn)| ψ(yn) = 0. 7To this end, we set x1 = y1 = 1. Since (ii) fails, for every n ≥ 2, taking m = 1 + 1 n and R = yn−1 we obtain the existence of some xn > Rsuch that for yn := nxn it holds ψ(xn) m < ψ(yn) yielding ψ(xn) ψ(yn) − 1 < m− 1 = 1 n. (2.10) Using convexity we also deduce that \f\fψ′(yn) \f\f ≤ ψ(xn) − ψ(yn) yn − xn = \u0012 n n − 1 \u0013\u0012 ψ(xn) − ψ(yn) yn \u0013 , whence, from (2.10), 0 ≤ yn|ψ′(yn)| ψ(yn) ≤ \u0012 n n − 1 \u0013\u0012 ψ(xn) ψ(yn) − 1 \u0013 < 1 n − 1. Taking the limit as n → ∞we conclude that (i) also fails to hold, which establishes the desired implication. (ii)⇒(iii). Assume that (2.9) holds for some m0 > 1, n0 and R0 > 1, that is, for all x > R0 we have ψ(x) ≥ m0 ψ(n0x). Then since n0x > x > Rwe also have: ψ(n0x) ≥ m0 ψ(n2 0x) yielding ψ(x) ≥ m2 0 ψ(n2 0x). We conclude that (2.9) also holds for m1 = m2 0 (under the choice of n1 = n2 0 and R1 = R0). Repeating this argument we deduce that (2.9) holds for all mk = mk 0, k≥ 1 (taking nk = nk 0 and Rk = R0). Since mk → ∞, in order to establish (iii) it is sufficient to observe that if (2.9) holds for some ¯m >1 (together with some ¯n >1 and ¯R >0) then it also holds for all m ∈ (1, ¯m], since ψ(x) m ≥ ψ(x) ¯m . (iii)⇒(i). Fix m >1, n >1 and R >0 such that (2.9) holds and set c := \u0012m − 1 m \u0013\u0012 1 n − 1 \u0013 > 0. Using convexity of ψ and (2.9), we deduce that for all x > Rwe have: \f\fψ′(x) \f\f ≥ ψ(x) − ψ(nx) nx − x =⇒ x|ψ′(x)| ψ(x) ≥ \u0012 1 n − 1 \u0013\u0012 1 − ψ(nx) ψ(x) \u0013 ≥ c. This establishes (i) and finishes the proof. 2 Remark 2.9. As already mentioned, assertion (i) ( Lojasiewicz inequality at infinity) holds true whenever the function ψ is semialgebraic (or more generally, definable in a polynomially bounded o-minimal structure). This already provides a broad assembly of examples of functions satisfy- ing (i), together with straightforward criteria to detect easily whether the property holds, based on certificates of semialgebricity or o-minimality (see [6, Theorem 1.13] e.g.). This being said, let us draw reader’s attention to the fact that besides what is asserted in [7, Propo- sition 2.7], the assumption of polynomial boundedness is essential for the validity of (i). Indeed, as shown in [21, Remark 8], the convex function ψ(x) = (log(1 + x))−1 is definable in the log-exp o-minimal structure but fails to satisfy (i). 82.4 Algorithm and numerical results To solve the system of equation (2.8), we will apply the Newton-like method proposed in [30]. However, since H is defined implicitly, we first need to reformulate the problem as follows:    z − F(y − z) + y = 0 rψ−1 \u0010 ψ(yi r ) + ψ(zi r ) \u0011 = 0 i = 1 . . . d, 1 2 ∥y−∥2 + 1 2 ∥z−∥2 + r2 + εr = 0, (2.11) where the variable z plays the role of H(y). Remark 2.10. In this new system of equations we assume that case (i) of Lemma 2.5 holds. (One can proceed in a similar way if (ii) holds.) In the definition of the following algorithm, we set X := (y, z, r)T and H(X) :=    z − F(y − z) + y rψ−1 \u0010 ψ(yi r ) + ψ(zi r ) \u0011 i = 1 . . . d, 1 2 ∥y−∥2 + 1 2 ∥z−∥2 + r2 + εr (2.12) so that (2.11) is reduced to H(X) = 0. This algorithm corresponds to a Newton method under a standard Armijo line search. Algorithm 1. Chose X0 = (X0, r0), X0 ∈ Ξ, r0 = ⟨y0, z0⟩/n, τ ∈ (0, 1/2), ϱ∈ (0, 1). Set k = 0. 2. If H(Xk) = 0, stop. 3. Find a direction dk ∈ R2n+1 such that H(Xk) + ∇XH(Xk)dk = 0. 4. Choose ζk = ϱjk ∈ (0, 1), where jk ∈ N is the smallest integer such that Θ(Xk + ϱjk dk) − Θ(Xk) ≤ τϱjk ∇Θ(Xk)T dk. 5. Set Xk+1 = Xk + ζkdk and k ← k + 1. Go to step 2 . The merit function used in the line search corresponds to the square of the global error: Θ(X) = 1 2∥H(X)∥2. To get a well defined algorithm, the initial point ( y0, z0)T must be an interior point, and the initial value for r must be positive r0 > 0. 93 Applications In this section we show that several problems, which can be naturally restated as (NAVE) and can be solved efficiently thanks to the above transformation. We present in this section numerical experiments, in which the smoothing functions are restricted to two specific cases θ1(t) :=    t t + 1, t ≥ 0 t, t < 0 and θ2(t) := 1 − e−t. The numerical experiments are conducted in an ordinary computer. All program codes are written and executed in MATLAB R2023a. In Subsection 3.1 and 3.3, we employ a similar stopping criterion for every numerical method, by using a tolerance T ol= 1e−10 and fixing the maximum number of iterations to Nmax = 2000. Since the NAVE problems may have multiple solutions, in the following, the error will be computed by Error = ∥F(xapproximate) − |xapproximate|∥ in Subsection 3.1 and respectively by Error = ∥F(xapproximate) − |xapproximate| −b∥) in Subsection 3.3). 3.1 Ridge Regression Ridge regression adds to the loss function L(x), x∈ Rd a penalty term in order to avoid overfitting: historical development and the applications in data science of ridge regression can be found e.g. in [12, 14]. This penalty term usually consists of adding the squared magnitude of the coefficients (traditionally denoted by w). We hereby consider an asymmetric ridge regression of the form: min x∈Rd   L(x) + dX j=1 \u0000 λj max{xj, 0}2 + µj max{−xj, 0}2\u0001   , (3.1) where the penalization parameters λj and µj satisfy λj − µj ̸= 0 for all j ∈ {1, ··· , d}. The case λj = µj = λ for every j corresponds to the classical ridge regression, which will not be considered here. On the other hand, the case λj = 0 for all j and µj > 0, corresponds to a penalization of the negativity of the coefficients, promoting solutions with positive coefficients. The necessary condition for optimality reads as follows: ∇L(x) + 2λ max{x, 0} −2µ max{−x, 0} = 0, where the two vectorsλ max{x, 0} and µ max{−x, 0} are to be understood componentwise. Noticing that 2 max{x, 0} = |x|+x and 2 max{−x, 0} = |x|−x, we end up with the following (NAVE) problem F(x) − |x| = 0 with F(x) = \u0012 1 µ − λ \u0013 ∇L(x) + \u0012µ + λ µ − λ \u0013 x (coordinatewise) Therefore, one can solve the previous problem if either F − I or −(F + I) is a P0–map, that is either ( µ − λ)−1 (∇L + 2λI) or − (µ − λ)−1 (∇L + 2µI) is a P0–map. To illustrate for asymmetric ridge regression, we consider the loss function L(x) = 1 2∥Ax − b∥2, where A ∈ Rm×d and b ∈ Rd. (3.2) 10We performed numerical experiments, fixing λj = ¯λ and µj = ¯µ for every j ∈ {1, ··· , d}. These parameters, matrix A ∈ Rm×d and vector b ∈ Rd were randomly generated with values in [ −5, 5]. As shown in Table 1, considering the average number of iterations with similar tolerance, using the function θ2 is better, while in an exceptional case m = 20 > d = 10 and ( ¯λ, ¯µ) = (0 , 100), θ2–smoothing performs worse. On the other hand, while the parameters ¯λ and ¯µ become greater, which can be compared to the ascent of the (classical) ridge parameter, θ2–smoothing performs within a better tolerance in a small number of iterations. Table 1: Comparing (asymmetric) ridge regression with different smoothing functions Error Iterations Running time( ×e − 2(s)) (¯λ, ¯µ) ( m, d) θ1 θ2 θ1 θ2 θ1 θ2 (0, 100) (3 , 10) 1 .9e − 11 7 .3e − 15 18 21 4 .81 2 .96 (5, 10) 5 .8e − 11 1 .3e − 16 17 71 4 .78 6 .26 (10, 10) 5 .4e − 11 1 .9e − 16 18 24 5 .78 3 .45 (20, 10) 3 .8e − 11 3 .5e − 3 17 2000 4 .94 819 (200, 1000) (3 , 10) 8 .9e − 11 1 .4e − 17 18 23 5 .62 2 .99 (5, 10) 2 .7e − 11 3 .6e − 18 19 22 5 .12 2 .9 (10, 10) 6 e − 11 2 .7e − 17 18 33 4 .93 3 .87 (20, 10) 4 .18e − 11 1 .2e − 10 18 40 5 .41 4 .17 To end this part, we give a heuristic observation on a sparse optimization problem (see e.g. [13,36]). Let us consider the following problem min x∈Rd L(x) + λ∥x∥1. (3.3) The first order optimality condition for (3.3) has the form 0 ∈ ∇L(x) + λ ∂∥ · ∥1(x), (3.4) where the subdifferential of ℓ1 norm can be written explicitly as q ∈ ∂∥ · ∥1(x) if and only if ( qi = sign(xi), if xi ̸= 0, |qi| ≤1, if xi = 0. Using the fact that α sign(α) = |α| for every α ∈ R, the inclusion (3.4) can be transformed into x∇L(x) + λ|x| = 0 (coordinatewise). The above equation just provides a necessary condition for optimal solution. In the following, we give a short numerical observation to guarantee its potential utility in sparse optimization. In order to apply results from previous sections, it is necessary to ensure that one of the following maps is a P0-map −1 λ x ∇L(x) − I and 1 λ x ∇L(x) − I. 11In the following figures, we use the same quadratic loss function (3.2), where matrix A and vector b are randomly generated ranging from −1 to 1 and −0, 05 to 0, respectively. Figure 1 shows the behavior of each coefficient while increasing the tuning parameter λ >0. (a) θ1–smoothing  (b) θ2–smoothing. Figure 1: Problem in dimension m = 20 and d = 40. 3.2 Nonlinear ordinary differential equations A NAVE problem also naturally arises when we deal with a discretization of a nonlinear ordinary differential equation (ODE, for short) involving rough velocity, for example ˙ γ(t) = p |γ(t)| as well as an ODE of the form Φ(X(2k), X(2k−1), . . . ,˙X) = |X| In this subsection we provide two examples (one being a stiff ODE) to illustrate the effectiveness of smoothing techniques when using finite difference schemes for ODEs. Example 3.1. We consider a stiff ODE with initial value as follows ( ¨x + 1001 ˙x − 1000|x| = 0, t >0 x(0) = x0 < 0, ˙x(0) = 0, (3.5) whose exact solution is xexact(t) = x0 \u0012 − 1 999e−1000t + 1000 999 e−t \u0013 ≈ x0e−t. Let us consider problem (3.5) in time domain I = [0, T]. We use a uniform mesh ttt = (ti), where ti = ih for i ∈ {0, ··· , N} and h = T/N , and the approximation solution will be xxx = (xi) where xi ≈ x(ti). For the first and the second derivative, we use the 2nd–order approximation ¨x(ti) ≈ xi−2 − 2xi−1 + xi h2 and ˙x(ti) ≈ xi+1 − xi−1 2h . Remarkably, at the final time, the first derivative ˙x(tN ) will be computed via the 2nd–order backward formula ˙x(tN ) ≈ (xN−2 − 4xN−1 + 3xN )/2h. Since the initial velocity is zero, using 1st–order backward approximation, we note that x−1 = x0. The discretization of (3.5) can be written as 1 1000AAAxxx + 1001 1000BBBxxx − |xxx| = bbb, 12where AAA,BBB ∈ RN×N is determined by AAA = 1 h2   1 0 0 ··· 0 0 0 −2 1 0 ··· 0 0 0 1 −2 1 ··· 0 0 0 ··· ··· ··· ··· ··· ··· ··· 0 0 0 ··· 1 0 0 0 0 0 ··· − 2 1 0 0 0 0 ··· 1 −2 1   and BBB = 1 2h   0 1 0 ··· 0 0 0 −1 0 1 ··· 0 0 0 0 −1 0 ··· 0 0 0 ··· ··· ··· ··· ··· ··· ··· 0 0 0 ··· 0 1 0 0 0 0 ··· − 1 0 1 0 0 0 ··· 1 −4 3   (3.6) Here, vector bbb = (bi) ∈ RN is defined by bi = 0 for i ≥ 2, b1 = x0(1/(1000h2) + 1001/(2000h)) and b2 = −x0/(1000h2). In Figure 2.(a), we approximate the solution of equation (3.5) with initial condition x0 = −1 and time interval I = [0, 5]. The finite difference scheme was computed with mesh size h = 0.05 and the error is 9 .22e − 4 when applying θ1 and θ2 smoothing funtions. To get convergence rate in Figure 2.(b), we apply difference mesh sizes in the same time interval I = [0, 1] and intial condition x0 = −2. (a) Approximate solutions.  (b) Convergence rate O(h1/2). Figure 2: Solving equation 3.5 Let us now make some comments on the utility of NAVE for boundary value problems: we consider a boundary value problem related to (3.5) ( ¨x + 1001 ˙x − 1000|x| = 0, t∈ (0, T), x(0) = x0 < 0, x(T) = y0 ∈ R. (3.7) In order to illustrate this case, we consider the time interval I = [0, 2] and exact solution is deter- mined by xexact. Using similar time mesh as above, the first and second derivatives are approximate as follows ¨x(ti) ≈ xi−1 − 2xi + xi+1 h2 and ˙x(ti) ≈ xi − xi−1 h . Figure 3 shows the convergence rate for the boundary value problem (3.7). The smoothing technique used in this problem presents a better accuracy compared to the above initial value problem, which 13seems to be natural because of the stiffness of the problem (3.5). It is noteworthy that Figure 3 also depicts an expected convergence rate since we have used a first order approximation for ˙ x. Figure 3: Convergence rate O(h) for a boundary value problem Example 3.2. For a continuous function f : [0, +∞) → R, we consider an ODE ( ¨x + arctan(x) − |x| = f(t), t >0, x(0) = x0 ∈ R, ˙x(0) = 0. (3.8) Using a similar discretization as in Example 3.1, the unknown variable xxx ≈ x(ttt) solves a NAVE problem as follows AAAxxx + arctan(xxx) − |xxx| = bbb, (3.9) where the matrix AAA is determined as in Example 3.1 and the vector bbb ∈ RN is defined by bi = f(ti) for i ≥ 2 and b1 = f(t1) + x0 h2 and b2 = f(t2) − x0 h2 . To illustrate for this example, we consider problem (3.8) with source term f(t) = arctan(cos(πt)) − |cos(πt)| −π2 cos(πt), whose exact solution is xexact(t) = cos( πt). Figure 4.(a) shows the approximate solution on the time interval I = [0, 1] with mesh size h = 0.0125. The error between θ1 (resp. θ2) approximation and exact solution is 0 .06 (resp. 0 .0725). Besides, Figure 4.(b) displays convergence rate of the combination of finite difference scheme and the θ–smoothing applying for the associated NAVE problem. 14(a) Approximate solutions.  (b) Convergence rate O(h). Figure 4: Solving equation 3.8. 3.3 Comparison of methods for NAVE Instead of smoothing procedure considered in Section 2, one can solve a NCP via other numerical methods. In this subsection we give examples to compare the efficiency of four methods • Newton–like method with smoothing functions θ1 and θ2; • approximating by Soft–Max function, in which the main idea is to approximate the comple- mentarity condition via the limit max i∈{1,···,d} xi = lim r↘0 r log  dX i=1 exi/r ! . which have been widely used in many optimization problems, for example [31, Example 1.30], [19,20,28]; • using interior point method, for example, one can find the use of interior point method for complementarity problems in [11,15,17,29]. Now, in the following examples, we solve the system ˜F(x) −|x| = b, especially, Example 3.4 and 3.5 can be found in [2,18]. Example 3.3. We consider ˜F(x) = Ax, where A = tridiag(−1, 4, −1) ∈ Rd×d, x∗ ∈ Rd, b= Ax∗ − |x∗|. (3.10) Example 3.4. ˜F : R3 → R3 is defined by ˜F(x) :=   2x1 − 2 2x2 + x3 2 − x3 + 3 x2 + 2x3 + 2x3 3 − 3  . 15Example 3.5. ˜F : R4 → R4 is defined by ˜F(x) :=   3x2 1 + x1 + 2x1x2 + 2x2 2 + x3 + 3x4 2x2 1 + x1 + x2 2 + x2 + 10x3 + 2x4 3x2 1 + x1x2 + 2x2 2 + 3x3 + 9x4 x2 1 + 3x2 2 + 2x3 + 4x4  . Table 2 compares the four methods: smoothing method with θ1 and θ2, Soft Max (denoted SM) and Interior Point (denoted IP) method on the NAVE problem associated to Example 3.3–3.5. In the first example, the vector b is randomly generated with values in [−5, 5] and the problem is considered in dimensions d = 10, 50, 200. In Example 3.4 and 3.5, we respectively consider b1 = (−1, −5, 10)T , b2 = (9, −100, 10)T , b3 = (200, 0, 900)T , b∗ 1 = (10, 10, −12, 0)T , b∗ 2 = (20, −100, −12, 1)T and b∗ 3 = (200, 10, −5, −5)T . We observe that the smoothing method (especially with θ2–smoothing function) is the most robust among the considered methods. In connection with convergence speed, the interior point method performs much less competitively than the others, while it only reaches 1e−2 after N = 2000 iterations. Another point that can be recognized from Table 2 is that the Soft Max method could only solve problems with small size, for example for problems in dimension N = 50 and N = 200 the singularities appear after less than 100 iterations. Table 2: Several methods to solve NAVE Error Iterations Example Vector b θ 1 θ2 SM IP θ1 θ2 SM IP 3.3 d = 10 9 .3e − 11 1 .5e − 11 5 .5e − 12 1 .6e − 2 20 13 173 2000 d = 50 1 .4e − 11 5 .3e − 15 NaN 8.96e − 3 29 41 41 2000 d = 200 8 .39e − 11 1 .32e − 14 NaN 9.55e − 3 45 76 96 2000 3.4 b1 4.7e − 11 1 .7e − 11 7 e − 14 5 .5e − 2 14 9 8 2000 b2 1.7e − 11 3 .6e − 11 1 .8e − 12 4 e − 1 22 16 15 2000 b3 9.3e − 11 1 .4e − 13 1 .4e − 13 2 e + 2 211 205 205 2000 3.5 b∗ 1 5.5e − 11 1 .3e − 14 2 e − 14 7 e − 2 16 12 12 2000 b∗ 2 5.5e − 11 2 .2e − 14 4 .7e − 14 9 .1e − 1 26 22 16 2000 b∗ 3 6.1e − 11 1 .4e − 10 3 e + 1 3 .8e − 0 50 43 2000 2000 Figure 5(a) and 5(b) display the performance time between different methods for Example 3.3 with the size n = 20 and Example 3.4, respectively. We did the observation with 50 samples and the vector b is randomly generated with values in [ −10, 10]. At a first sight, the interior point method appears to be the slowest one in comparison with the other three methods. As shown in Figure 5(a), the θ1–smoothing performs the best choice among all the methods. If we look carefully, in lower dimension as Example 3.4, the Soft Max and θ2–smoothing performs slightly better than θ1–smoothing method. 16(a) Example 3.1 in dimension n = 20.  (b) Example 3.2. Figure 5: Performance time. 4 Conclusion and discussion In this work, we applied smoothing techniques commonly used for Nonlinear Complementarity Problems (NCP) to Nonlinear Absolute Value Equations (NAVE). We first showed that a NAVE can be formulated as a NCP with an implicitly known corresponding mapping. We then established that a NAVE can be effectively addressed under a mild direct assumption on the NAVE function. Additionally, we clarified a technical assumption used in some smoothing approaches, proving its equivalence to a  Lojasiewicz inequality, thus showing its broad applicability. Last but not least, we provided illustrative examples and applications that through numerical verification reveal effective- ness and potential of this approach. In a future work, we aim to establish error bounds or estimations and study the complexity of the proposed method in favorable situations, such as when the implicitly corresponding mapping in the NCP formulation is monotone or strongly monotone. Acknowledgement. This work was initiated during a research stay of Aris Daniilidis and Tr´ ı Minh Lˆ e to INSA Rennes (February 2023). These authors thank their hosts for hospitality. The first author acknowledges support from the Austrian Science Fund (FWF, P–36344-N). References [1] L. Abdallah, M. Haddou, T. Migot, Solving absolute value equation using complemen- tarity and smoothing functions. J. Comput. Appl. Math. 327 (2018), 196–207. [2] J. H. Alcantara, J-S. Chen. A new class of neural networks for NCPs using smooth per- turbations of the natural residual function, J. Comput. Appl. Math. 407 (2022), Paper No. 114092, 22 pp. [3] J. H. Alcantara, J.-S. Chen, M. K. Tam,Method of alternating projections for the general absolute value equation. J. Fixed Point Theory Appl. 25 (2023), Paper No. 39, 38 pp. 17[4] I. Ben Gharbia J.C. Gilbert,Nonconvergence of the plain Newton-min algorithm for linear complementarity problems with a P-matrix, Mathematical Programming 134 (2012), 349–364. [5] J. Y. Bello Cruz, O. P. Ferreira, L. F. Prudente, On the global convergence of the inexact semi-smooth Newton method for absolute value equation, Comput. Optim. Appl. 65 (2016), 93–108. [6] M. Coste, An Introduction to o-minimal Geometry , Instituti Editoriali e Poligrafici Inter- nazionali, Pisa (2000). [7] D. Dacunto, V. Grandjean, A gradient inequality at infinity for tame functions, Rev. Mat. Complut. 18 (2005), 493–501. [8] M. Fiedler, V. Pt ´ak, On matrices with nonpositive off-digagonal elements and positive principal minors, Czech. Math. J. 12 (1962), 382–400. [9] S.-L. Hu and Z.-H. Huang, A note on absolute value equations, Optim. Lett. 4 (2010), 417–424. [10] M. Haddou, P. Maheux, Smoothing methods for nonlinear complementarity problems, J. Optim. Theory Appl. 160 (2014), 711–729. [11] M. Haddou, T. Migot, J. Omer, A generalized direction in interior point method for monotone linear complementarity problems, Optim. Lett. 13 (2019), 35–53. [12] T. Hastie , Ridge regularization: An essential concept in data science, Technometrics 62 (2020), 426–433. [13] T. Hastie, R. Tibshirani, J. Friedman, The elements of statistical learning. Data mining, inference and prediction, Springer (New York, 2001). [14] R. W. Hoerl, Ridge regression: A historical context, Technometrics 62 (2020), 420–425. [15] A. N. Iusem, An interior point method for the nonlinear complementarity problem, Appl. Numer. Math. 24 (1997), 469–482. [16] C. Kanzow, A new approach to continuation methods for complementarity problems with uniform P-functions, Oper. Res. Lett. 20 (1997), 85–92. [17] M. Kojima, N. Megiddo, T. Noma, A. Yoshise, A unified approach to interior point algorithms for linear complementarity problems , Lecture Notes in Comput. Sci. 538, Springer (Berlin, 1991). [18] M. Kojimam S. Shindo, Extension of Newton and quasi-Newton methods to systems of PC 1 equations, J. Oper. Res. Soc. Jpn. 29 (1986), 352–374. [19] X. Li, An entropy–based aggregate method for minimax optimization, Eng. Opt. 18 (1992), 277–285. [20] Y. Li, T. Tan, X. Li, A log-exponential smoothing method for mathematical programs with complementarity constraints, Appl. Math. Comput. 218(2012), 5900—5909. 18[21] T. L. Loi ,  Lojasiewicz inequalities in o-minimal structures, Manuscripta Math. 150 (2016), 59–72. [22] T. Lotfi and H. Veiseh, A note on unique solvability of the absolute value equation, 2013. [23] O. L. Mangasarian, RR. Meyer, Absolute value equations, Linear Algebra Appl. 419 (2006), 359–367. [24] O. L. Mangasarian, Linear complementarity as absolute value equation solution, Optim. Lett. 8 (2014), 1529–1534. [25] MATLAB R2023A, Natick, Massachusetts: The MathWorks Inc., 2010. [26] J.-J. Mor´e, Global methods for nonlinear complementarity problems, Math. Oper. Res. 21 (1996), 589–614. [27] J.-J. Mor´e , W.C. Rheinboldt,On P- and S- functions and related classes ofn- dimensional nonlinear mappings, Linear Algebra Appl. 6 (1973), 45–68. [28] Y. Nesterov, Smooth minimization of nonsmooth functions, Math. Program. Ser. A 103 (2005), 127–152. [29] F. A. Potra, Y. Ye, Interior-point methods for nonlinear complementarity problems, J. Optim. Theory Appl. 88(1996), 617–642. [30] E. H. Osmani, M. Haddou, N. Bensalem, L. Abdallah, A new smoothing method for nonlinear complementarity problems involving P0-function. Stat. Optim. Inf. Comput. 10 (2022), 1267–1292. [31] R. T. Rockafellar, J.-B. R. Wets, Variational analysis, Grundlehren Math. Wiss. 317, Springer (Berlin, 1998). [32] O. Prokopyev, On equivalent reformulations for absolute value equations, Comput. Optim. Appl. 44 (2009), 363–372. [33] J. Rohn, A theorem of the alternatives for the equation ax + b|x| = b, Linear Mult. Algebra 52 (2004), 421–426. [34] J. Rohn, On unique solvability of the absolute value equation,Optim. Lett. 3 (2009), 603–606. [35] J. Rohn, V. Hooshyarbakhsh, and R. Farhadsefat, An iterative method for solving absolute value equations and sufficient conditions for unique solvability, Optim. Lett. 8 (2014), 35–44. [36] R. Tibshirani, Regression shrinkage and selection via the lasso, J. Roy. Statist. Soc. Ser. B 58 (1996), 267–288. [37] S.-L. Wu and C.-X. Li, A note on unique solvability of the absolute value equation, Optim. Lett. 14 (2020), 1957–1960. 19Aris Daniilidis, Tr´ ı Minh Lˆ e Institut f¨ ur Stochastik und Wirtschaftsmathematik, VADOR E105-04 TU Wien, Wiedner Hauptstraße 8, A-1040 Wien E-mail: {aris.daniilidis, minh.le}@tuwien.ac.at https://www.arisdaniilidis.at/ Research supported by the grants: Austrian Science Fund (FWF P-36344N) (Austria) Mounir Haddou, Olivier Ley Univ Rennes, INSA, CNRS, IRMAR - UMR 6625, F-35000 Rennes, France E-mail: {mounir.haddou, olivier.ley}@insa-rennes.fr http://{haddou, ley}.perso.math.cnrs.fr/ Research supported by the Centre Henri Lebesgue ANR-11-LABX-0020-01. 20",
      "references": [
        "Solving absolute value equation using complemen- tarity and smoothing functions.",
        "A new class of neural networks for NCPs using smooth per- turbations of the natural residual function",
        "Method of alternating projections for the general absolute value equation.",
        "Nonconvergence of the plain Newton-min algorithm for linear complementarity problems with a P-matrix",
        "On the global convergence of the inexact semi-smooth Newton method for absolute value equation",
        "An Introduction to o-minimal Geometry",
        "A gradient inequality at infinity for tame functions",
        "On matrices with nonpositive off-digagonal elements and positive principal minors",
        "A note on absolute value equations",
        "Smoothing methods for nonlinear complementarity problems",
        "A generalized direction in interior point method for monotone linear complementarity problems",
        "Ridge regularization: An essential concept in data science",
        "The elements of statistical learning. Data mining, inference and prediction",
        "Ridge regression: A historical context",
        "An interior point method for the nonlinear complementarity problem",
        "A new approach to continuation methods for complementarity problems with uniform P-functions",
        "A unified approach to interior point algorithms for linear complementarity problems",
        "Extension of Newton and quasi-Newton methods to systems of PC 1 equations",
        "An entropy–based aggregate method for minimax optimization",
        "A log-exponential smoothing method for mathematical programs with complementarity constraints",
        "Lojasiewicz inequalities in o-minimal structures",
        "A note on unique solvability of the absolute value equation",
        "Absolute value equations",
        "Linear complementarity as absolute value equation solution",
        "MATLAB R2023A",
        "Global methods for nonlinear complementarity problems",
        "On P- and S- functions and related classes ofn- dimensional nonlinear mappings",
        "Smooth minimization of nonsmooth functions",
        "Interior-point methods for nonlinear complementarity problems",
        "A new smoothing method for nonlinear complementarity problems involving P0-function.",
        "Variational analysis",
        "On equivalent reformulations for absolute value equations",
        "A theorem of the alternatives for the equation ax + b|x| = b",
        "On unique solvability of the absolute value equation",
        "An iterative method for solving absolute value equations and sufficient conditions for unique solvability",
        "Regression shrinkage and selection via the lasso"
      ],
      "meta_data": {
        "arxiv_id": "2402.16439v2",
        "authors": [
          "Aris Daniilidis",
          "Mounir Haddou",
          "Tri Minh Le",
          "Olivier Ley"
        ],
        "published_date": "2024-02-26T09:36:31Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "[Unavailable]",
        "methodology": "[Unavailable]",
        "experimental_setup": "[Unavailable]",
        "limitations": "[Unavailable]",
        "future_research_directions": "[Unavailable]",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "TEMPERA: Test-Time Prompt Editing via Reinforcement Learning",
      "full_text": "Elastic Analysis of Augmented Curves and Constrained Surfaces Esfandiar Nava-Yazdani[0000−0003−4895−739X] Zuse Institute Berlin, Berlin, Germany navayazdani@zib.de https://www.zib.de/members/navayazdani Abstract. The square root velocity transformation is crucial for efficiently em- ploying the elastic approach in functional and shape data analysis of curves. We study fundamental geometric properties of curves under this transformation. Moreover, utilizing natural geometric constructions, we employ the approach for intrinsic comparison within several classes of surfaces and augmented curves, which arise in the real world applications such as tubes, ruled surfaces spherical strips, protein molecules and hurricane tracks. Keywords: Elastic shape analysis · Tube · Manifold-valued · Ruled surface · Hurricane track. 1 Introduction Metric comparison of curves is a core task in a wide range of application areas such as morphology, image and shape analysis, computer vision, action recognition and signal processing. Thereby, a Riemannian structure is highly desirable, since it naturally provides powerful tools, beneficial for such applications. In the recent years, the use of Riemannian metrics for the study of sequential data, such as shapes of curves, trajectories given as longitudinal data or time series, has rapidly grown. In elastic analysis of curves, one considers deformations caused from both bending and stretching. A Riemannian metric, which quantifies the amount of those deformations is called elastic (cf. [18,19]). Therein, in contrast to landmark-based approaches (cf. [12,20,22]), one considers whole continuous curves instead of finite num- ber of curve-points. Consequently, the underlying spaces are infinite dimensional and computational cost becomes a significant issue. The square root velocity (SRV) frame- work provides a convenient and numerically efficient approach for analysing curves via elastic metrics and has been widely used in the recent years (cf. [14,11,4,3] and the comprehensive work [24]). In many applications the curves are naturally manifold-valued. For instance, Lie groups such as the Euclidean motion group, or more generally, symmetric spaces includ- ing the Grassmannian and the Hadamard-Cartan manifold of positive definite matrices are widely used in modelling of real world applications. Extensions of SRV framework from euclidean to general manifold-valued data can be found in [13,27,25,9,26]. arXiv:2402.04944v3  [math.DG]  28 Mar 20242 E. Nava-Yazdani Our contributions are the following. We expose for plane curves the behaviour of speed and curvature under the SRV transformation and geometric invariants. More- over, we apply the elastic approach to augmented curves, determining certain classes of surfaces, tubes, ruled surfaces and spherical strips, as well as hurricane tracks consid- ered with their intensities. We recall that with distance and geodesic at hand, signifi- cant ingredients of statistical analysis such as mean and principal geodesic components as well as approximation and modelling concepts such as splines can be computed. This paper is organized as follows. Section 2, presents the Riemannian setting and notations. Section 3 is devoted to applications. Therein, we consider time series, for which in addition to spatial data, auxiliary information give rise to augmented curves and some classes of surfaces generated by them. Thereby, we apply the elastic approach to both euclidean and spherical trajectories. Future prospects and concluding remarks are presented in 4. For the convenience of those readers primary interested in the applications, we mention that, advanced parts and details from differential geometry, presented in 2, can be skipped. Thereby, the essential point is the use of a framework (SRV) for computation of shortest paths on the spaces of curves and their shapes. 2 Riemannian Framework 2.1 Preliminaries For the background material on Riemannian geometry, we refer to [8] and [10]. Let (M, g) be a finite dimensional Riemannian manifold and M the Fr´ echet manifold of smooth immersed curves from D in M, where D denotes either the unit circle S1 or the unit interval I := [0, 1] for closed or open curves respectively. Moreover, we denote the group of orientation preserving diffeomorphisms on D by Diff +. The following reparametrization invariance is crucial for a Riemannian metric G on M: Gc◦φ(h ◦ φ, k◦ φ) = Gc(h, k), for any c ∈ M, h, k∈ TcM and φ ∈ Diff +. The above equivariance ensures that the induced distance function satisfies the following, which is often desirable in applica- tions: d(c0 ◦ φ, c1 ◦ φ) = d(c0, c1), for any two curves c0 and c1 in M. Similarly, denoting the isometry group of M by Isom(M) and the tangent map of F ∈ Isom(M) by T F, the invariance GF◦c(T F◦ h, TF◦ k) = Gc(h, k), ensures that d(F ◦ c0, F◦ c1) = d(c0, c1). With the above invariances, we can divide out the spaces Isom(M) and Diff +, and consider the natural induced distance dS on the quotient space S = M/(Diff + × Isom(M))Elastic Analysis of Augmented Curves and Constrained Surfaces 3 given by dS([c0], [c1]) = inf {d(c0, f◦ c1 ◦ φ) : φ ∈ Diff +, f∈ Isom(M)} = inf {d(f ◦ c0 ◦ φ, c1) : φ ∈ Diff +, f∈ Isom(M)}. In the context of shape analysis of curves, M and S are called the pre-shape and shape space, respectively. Note that the order of quotient operations does not matter, since the left action of Isom(M) and the right action of Diff + commute. M/Diff + is the space of unparametrized curves and its inherited distance reads inf {d(c0, c1 ◦ φ) : φ ∈ Diff +}. We remark that particular essential challenges are due to the fact that some basic concepts and results from finite dimensional differential geometry such as Hopf-Rinow theorem, do not carry over to the infinite dimensional case. Now, let ∇ be the Levi- Civita connection of M and denote the arc length parameter, speed and unit tangent of c by θ, ω and T respectively. Thus, we have ω = |˙c|, dθ = ωdt and T = ˙c ω , where dot stands for derivation with respect to the parameter t. Due to a remarkable result in [16] the geodesic distance induced by the simplest natural choice, the L2-metric GL2 c (h, k) = Z D gc(h, k) dθ, always vanishes. Consequently, some stronger Sobolev metrics have been considered in several works including [17,7,5]. They are given by Gc(h, k) = nX i=0 Z D aigc(∇i T h, ∇i T k) dθ, with a1 non-vanishing and all ai non-negative, distinguish the curves. We consider first order metrics with constant coefficients. We remark that the coefficients ai can be chosen such that the metric is scale invariant, which is a desired property for some applications in shape analysis. A family of certain weighted Sobolev-type metrics, the so-called elastic metrics, based on a decomposition of derivatives of the vector fields into normal and tangent components, has been introduced in [18,19]: Ga,b c (h, k) = Z D agc((∇T h)⊤, (∇T k)⊤) + bgc((∇T h)⊥, (∇T k)⊥) dθ, with 4b ≥ a >0. In this work, we use the square root velocity (SRV) framework, which allows for a convenient and computationally efficient elastic approach. The main tool in this framework is the square root velocity transformation, which for euclidean M reads q : c 7→ ˙cp |˙c| .4 E. Nava-Yazdani It isometrically maps curves modulo translations, with the metric G1,1/4 to M with the flat L2-metric given by G0(v, w) = Z D g(v(t), w(t))dt. This metric is frequently called (cf. [15,2,6]) flat, to emphasize its footpoint indepen- dence. Note that the elastic metric G1,1 corresponds to the first order Sobolev metric with a0 = 0 and a1 = 1. We remark, that for plane curves, the work [23] has extended the SRV transformation to general parameters a, b >0. For further reading on the SRV framework and applications in shape analysis, we refer to [14], [11] (numerical aspects), the survey [6] and particularly, the comprehensive work [24]. 2.2 Plane Curves A natural question that arises is, how essential geometric characteristics of a curve behave under the SRV transformation. In the following, we provide an answer for speed and curvature in the case of plane curves. Let M = R2, ˜c := q(c) and denote the curvature of c by κ. Note that ˜c does not need to be an immersion. Proposition 1. Denoting the speed of˜c by ˜ω, we have ˜ω = r ˙ω2 4ω + ω3κ2. (1) Moreover, ˜c is an immersion if and only ifκ and ˙ω have no common zeros. In this case, ˜κ˜ω = κω + ˙φ, (2) where ˜κ denotes the curvature of˜c and φ := arctan \u00122ω2κ ˙ω \u0013 . Proof. Let N denote the unit normal of c. With the shorthand notations α := √ω and β := α3κ, a straightforward application of the Frenet equations ˙T = ωκN and ˙N = −ωκT , yields ˙˜c = ˙αT + βN, ¨˜c = (¨α − β2 α )T + ( ˙β + ˙αβ α )N. Thus, we have ˜ω = p ˙α2 + β2, immediately implying (1). Obviously, zeros of ˜ω are common zeros of κ and ˙ω. Thus, ˜c is an immersion if and only if κ and ˙ω have no common zeros. In this case, ˜ κ and φ = arctan (β/ ˙α) = arctan \u0010 2ω2κ ˙ω \u0011 are well-defined and ˜κ˜ω3 = ˜ω2β/α + ˙α ˙β − ¨αβ, which immediately implies the curvature formula (2).Elastic Analysis of Augmented Curves and Constrained Surfaces 5 Next, we apply the proposition to study some geometric quantities, which are invariant under the SRV transformation. For closed curves, integrating the curvature formula above over D = S1 (note that in this case, ˜ω >0 almost everywhere), we see that the SRV transformation preserves the total curvature and particularly the turning number. Moreover, κω is preserved if and only if κ = a d dt \u00001 ω \u0001 with a constant a. Clearly, with κ and ω at hand, utilizing Frenet equations, we can compute c up to rigid motions. The following explicit solution is an immediate application of the above proposition. In light of the above proposition, immersed curves, which are mapped to straight lines, can easily be determined as follows. Example 1. Let a, b, Abe constants withab, A >0, ω(t) = A/ sin2(at+b) and κ = a/ω. A straightforward computation, utilizing the curvature formula (2), implies ˜κ = 0. 2.3 Curves in Homogeneous Spaces For the background material on Lie groups and homogeneous spaces, we refer to [10]. The works [13,27] provide extensions of the SRV framework for euclidean curves to the case of general manifolds. The former has high computational cost, while the latter, transported SRV, depends on a reference point and also suffers from distortion or bias caused by holonomy effects. We use the natural extension to homogeneous spaces exposed in [26,9]. For reader’s convenience, we sketch the core ingredients of the approach and refer to the mentioned works for details and some applications. Let M be a homogeneous space, i.e., M = H/K, where K is a closed Lie subgroup of a Lie Group H. Let ∥·∥ denote the induced norm by a left invariant metric on H, L the tangent map of the left translation, and Imm(D, H) the space of immersed curves from D to H. The SRV transformation is given by Q(α) = (α(0), q(α)), where q(α) = Lα−1 ˙αp ∥ ˙α∥ Here, α−1(t) denotes the inverse element of α(t) in H and H the Lie algebra of H. The map Q is a bijection from Imm(D, H) onto H × L2(D, H). Now, M can be equipped with the Riemannian metric given by the pullback of the product metric of H × L2(D, H) using the map Q and horizontal lifting. Let c1 and c2 be immersed curves in M with horizontal lifts α1 and α2 respectively. The induced distance on M reads d(c1, c2) = inf \u001aq d2 H(α1(0), α2(0)x) + ∥q(α1) − Adx−1 (q(α2)∥2 L2 : x ∈ K \u001b . 3 Applications Frequently, besides spatiotemporal data, represented by a curve γ in a manifold M, there are additional or auxiliary information associated with the curve, thus with the same time-correspondence. These can jointly with γ be comprised and represented as a6 E. Nava-Yazdani so-called augmented curve ˜γ in a higher dimensional manifold ˜M. In some applications, the curve ˜γ uniquely determines a submanifold N of M via a natural construction. An important example is provided, when ˜M is a submanifold of the tangent bundle of M, where the auxiliary information is represented as a vector field along γ and the con- struction is given by the Riemannian exponential map. Significant special cases occur, when M is R3 or the unit two-sphere S2 and N a surface. In the next two subsections, we consider certain classes of surfaces in R3, which often arise in applications and are determined by augmented curves in R4. In the last two subsections, we consider certain spherical regions as well as hurricane tracks together with their intensities. In both cases, we utilize the Riemannian distance from subsection 2.3 to S2 × R, which is a homogeneous space (recall that S2 can be identified with SO(3)/SO(2)). For our example applications, we present geodesic paths representing deformations, minimizing the elastic energy within the SRV framework. We remark, that in a Rie- mannian setting, distance and geodesics are essential Building blocks for many major issues in the morphology and shape analysis, such as computation of mean and test statistics as well as principal component or geodesic analysis. Moreover, besides sta- tistical analysis, also some methods for clustering and classification use Riemannian metrics and geodesics. For the code implementing our approach, which particularly includes Riemannian optimization for the computation of geodesic paths, we utilized our publicly available python package https://github.com/morphomatics, introduced in [1]. 3.1 Tubes A tube or canal surface c is a one-parameter family of circles, whose centers constitute a regular curve γ such that the circles are perpendicular to γ. More precisely, denoting the radii of the circles by r, c(s, .) = γ + r(N cos s + B sin s), 0 ≤ s ≤ 2π, where N and B are the normal and binormal of the curve γ = γ(t), t∈ D, resp. Due to the unique correspondence of c to (γ, r), comparison of tubes reduces to comparison of curves in R4. Figure 1 shows some examples of shortest paths of tubes. Real world applications include a variety of fields such as examination of vein, pipes, capsules and plant roots. Clearly, tubes include surfaces of revolution. 3.2 Ruled Surfaces A ruled surface is formed by moving a straight line segment (possibly with varying length) along a base curve. More precisely, let γ be a curve in R3 and v a unit vector field along γ. Then c(s, .) = γ + sv, s∈ I, parametrizes a ruled surface generated by ( γ, v). Figure 2 depicts an example, where each surface consists of straight line segments connecting the blue (for better visibility) curves γ and γ + v. The class of ruled surfaces includes many prominent surfacesElastic Analysis of Augmented Curves and Constrained Surfaces 7 Fig. 1.Two shortest paths of tubes such as cone, cylinder, helicoid (a minimal surface) and M¨ obius strip. They arise in manufacturing (construction by bending a flat sheet), cartography, architecture and biochemistry (secondary and tertiary structure of protein molecules). Fig. 2.Shortest path of ruled surfaces 3.3 Spherical Strips Let exp denote the exponential map of the unit two-sphere S2. We recall that for any non-zero tangent vector to S2 at a point x: expx(v) = cos(|v|)x + sin(|v|) v |v|8 E. Nava-Yazdani and expx(0) = x. Now, let γ be a curve in S2 with binormal B (cross product of γ and its unit tangent), and r a scalar function along γ. Then, the map c given by c(s, .) := expγ s(rB), s∈ I, parametrizes a spherical strip with bandwidth r. Figure 3 depicts an example of the shortest path between two spherical curves comprised with their bandwidth functions visualised as strips. Fig. 3.Shortest path of spherical strips 3.4 Hurricane Tracks Hurricanes belong to the most extreme natural phenomena and can cause major im- pacts regarding environment, economy, etc. Intensity of a hurricane is determined by the maximum sustained wind (maxwind), monotonically classifying the storms into categories (due to Saffir–Simpson wind scale; for instance, maxwind ≥ 137 knots cor- responds to category 5). Due to their major impacts on economy, human life and envi- ronment, as well as extreme variability and complexity, hurricanes have been studies in a large number of works. For our example, we used the HURDAT 2 database provided by the U.S. National Oceanic and Atmospheric Administration publicly available on https://www.nhc.noaa.gov/data/, supplying latitude, longitude, and maxwind on a 6 hours base of Atlantic hurricanes. Fig. 4.2010 Atlantic hurricane tracks (left) and the shortest path between two of them (right) with color-coded maximum sustained wind (in knots)Elastic Analysis of Augmented Curves and Constrained Surfaces 9 We represent the tracks as discrete trajectories in S2. For further details and com- parison with other approaches, we refer to [24,25] and the recent work [21]. The latter, also provides statistical analysis and a classification of hurricane tracks in terms of their intensities. Fig. 4 illustrates this data set with a visualization of the 2010 hurricane tracks and a shortest path, where the intensities, considered as auxiliary information, are color-marked. 4 Conclusion In this paper, we analysed the behaviour of speed and curvature under the square root velocity framework for elastic approach to plane curves. Moreover, we applied an extension of this framework to homogeneous Spaces, to metrically compare augmented curves and special surfaces, generated by those curves, using a natural construction via the Riemannian exponential map. Our approach, allows for computationally efficient determination of geodesic paths in the shape spaces of the respective classes of surfaces. Our example applications include tubes, ruled surfaces, spherical strips and hurricane tracks. Future work includes further real world applications, particularly concerning statistical analysis of longitudinal data such as comparison of group wise trends within a hierarchical model as well as classification and prediction. Acknowledgements This work was supported through the German Research Foun- dation (DFG) via individual funding (project ID 499571814). References 1. Ambellan, F., Hanik, M., von Tycowicz, C.: Morphomatics: Geometric morphomet- rics in non-Euclidean shape spaces (2021). https://doi.org/10.12752/8544, https:// morphomatics.github.io/ 2. Bauer, M., Bruveris, M., Marsland, S., Michor, P.: Constructing reparametrization in- variant metrics on spaces of plane curves. arXiv: Differential Geometry (2012), https: //arxiv.org/pdf/1207.5965.pdf 3. Bauer, M., Bruveris, M., Charon, N., Møller-Andersen, J.: A relaxed approach for curve matching with elastic metrics. ESAIM: Control, Optimisation and Calculus of Variations 25 (03 2018). https://doi.org/10.1051/cocv/2018053 4. Bauer, M., Bruveris, M., Harms, Philipp Michor, P.W.: Soliton solutions for the elastic metric on spaces of curves. Discrete & Continuous Dynamical Systems - A 38, 1161–1185 (2018). https://doi.org/10.3934/dcds.2018049 5. Bauer, M., Bruveris, M., Michor, P.W.: Overview of the geometries of shape spaces and diffeomorphism groups. Journal of Mathematical Imaging and Vision 50(1-2), 60–97 (2014) 6. Bauer, M., Charon, N., Klassen, E., Brigant, A.L.: Intrinsic riemannian metrics on spaces of curves: theory and computation. arXiv preprint (2020), https://arxiv.org/abs/ 2003.05590 7. Bauer, M., Harms, P., Michor, P.W., et al.: Sobolev metrics on the manifold of all rie- mannian metrics. Journal of Differential Geometry 94(2), 187–208 (2013)10 E. Nava-Yazdani 8. do Carmo, M.P.: Riemannian Geometry. Mathematics: Theory and Applications, Birkh¨ auser Boston, Cambridge, MA, USA, 2 edn. (1992) 9. Celledoni, E., Eidnes, S., Schmeding, A.: Shape analysis on homogeneous spaces: a gener- alised srvt framework. In: Computation and Combinatorics in Dynamics, Stochastics and Control: The Abel Symposium, Rosendal, Norway, August 2016. pp. 187–220. Springer (2018) 10. Gallot, S., Hullin, D., Lafontaine, J.: Riemannian Geometry. Universitext, Springer, Berlin, 3 edn. (2004) 11. Huang, W., Gallivan, K.A., Srivastava, A., Absil, P.A.: Riemannian optimization for registration of curves in elastic shape analysis. Journal of Mathematical Imaging and Vision 54(3), 320–343 (2016) 12. Kendall, D., Barden, D. Carne, T., Le, H.: Shape and Shape Theory. John Wiley & Sons (1999) 13. Le Brigant, A.: Computing distances and geodesics between manifold-valued curves in the srv framework. Journal of Geometric Mechanics 9(2) (2017) 14. Liu, W., Srivastava, A., Zhang, J.: Protein structure alignment using elastic shape anal- ysis. In: Proceedings of the First ACM International Conference on Bioinformatics and Computational Biology. pp. 62–70 (2010) 15. Michor, P., Mumford, D., Shah, J., Younes, L.: A metric on shape space with explicit geodesics. Atti Accad. Naz. Lincei Cl. Sci. Fis. Mat. Natur. Rend. Lincei (9) Mat. Appl. 19 (07 2007). https://doi.org/10.4171/RLM/506 16. Michor, P.W., Mumford, D.: Vanishing geodesic distance on spaces of submanifolds and diffeomorphisms. Documenta Mathematica 10, 217–245 (2005) 17. Michor, P.W., Mumford, D.: An overview of the riemannian metrics on spaces of curves using the hamiltonian approach. Applied and Computational Harmonic Analysis 23(1), 74–113 (2007) 18. Mio, W., Srivastava, A., Joshi, S.H.: On shape of plane elastic curves. International Journal of Computer Vision 73, 307–324 (2006) 19. Mio, W., Srivastava, A., Joshi, S.: On shape of plane elastic curves. International Journal of Computer Vision 73, 307–324 (07 2007). https://doi.org/10.1007/s11263-006-9968-0 20. Nava-XYazdani, E., Hege, H.C., Sullivan, T.J., von Tycowicz, C.: Geodesic analysis in kendall’s shape space with epidemiological applications. Journal of Mathematical Imaging and Vision pp. 1–11 (2020). https://doi.org/10.1007/s10851-020-00945-w 21. Nava-Yazdani, E., Ambellan, F., Hanik, M., von Tycowicz, C.: Sasaki metric for spline models of manifold-valued trajectories. Computer Aided Geometric Design 104, 102220 (2023). https://doi.org/10.1016/j.cagd.2023.102220 22. Nava-Yazdani, E., Hege, H.C., von Tycowicz, C.: A hierarchical geodesic model for longi- tudinal analysis on manif olds. Journal of Mathematical Imaging and Vision 64(4), 395 – 407 (2022). https://doi.org/10.1007/s10851-022-01079-x 23. Needham, T., Kurtek, S.: Simplifying transforms for general elastic metrics on the space of plane curves. SIAM Journal on Imaging Sciences 13(1), 445–473 (2020). https://doi.org/10.1137/19M1265132 24. Srivastava, A., Klassen, E.P.: Functional and shape data analysis, vol. 1. Springer (2016) 25. Su, Z., Klassen, E., Bauer, M.: The square root velocity framework for curves in a ho- mogeneous space. 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW) pp. 680–689 (2017) 26. Su, Z., Klassen, E., Bauer, M.: Comparing curves in homogeneous spaces. Differential Geometry and its Applications 60, 9–32 (2018) 27. Zhang, Z., Su, J., Klassen, E., Le, H., Srivastava, A.: Rate-invariant analysis of covariance trajectories. Journal of Mathematical Imaging and Vision 60, 1306–1323 (2018)",
      "references": [
        "Morphomatics: Geometric morphometrics in non-Euclidean shape spaces",
        "Constructing reparametrization invariant metrics on spaces of plane curves.",
        "A relaxed approach for curve matching with elastic metrics.",
        "Soliton solutions for the elastic metric on spaces of curves.",
        "Overview of the geometries of shape spaces and diffeomorphism groups.",
        "Intrinsic riemannian metrics on spaces of curves: theory and computation.",
        "Sobolev metrics on the manifold of all riemannian metrics.",
        "Riemannian Geometry.",
        "Shape analysis on homogeneous spaces: a generalised srvt framework.",
        "Riemannian optimization for registration of curves in elastic shape analysis.",
        "Shape and Shape Theory.",
        "Computing distances and geodesics between manifold-valued curves in the srv framework.",
        "Protein structure alignment using elastic shape analysis.",
        "A metric on shape space with explicit geodesics.",
        "Vanishing geodesic distance on spaces of submanifolds and diffeomorphisms.",
        "An overview of the riemannian metrics on spaces of curves using the hamiltonian approach.",
        "On shape of plane elastic curves.",
        "Geodesic analysis in kendall’s shape space with epidemiological applications.",
        "Sasaki metric for spline models of manifold-valued trajectories.",
        "A hierarchical geodesic model for longitudinal analysis on manifolds.",
        "Simplifying transforms for general elastic metrics on the space of plane curves.",
        "Functional and shape data analysis",
        "The square root velocity framework for curves in a homogeneous space.",
        "Comparing curves in homogeneous spaces.",
        "Rate-invariant analysis of covariance trajectories."
      ],
      "meta_data": {
        "arxiv_id": "2402.04944v3",
        "authors": [
          "Esfandiar Nava-Yazdani"
        ],
        "published_date": "2024-02-07T15:25:20Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Studies geometric properties of the square root velocity (SRV) transform for elastic curve analysis, deriving explicit formulas for how speed and curvature of plane curves change under SRV and identifying SRV-preserved invariants (e.g., total curvature/turning number for closed curves). Extends SRV-based elastic comparison to manifold-valued/augmented curves in homogeneous spaces and uses these to intrinsically compare constrained surface classes generated by augmented curves (tubes/canal surfaces, ruled surfaces, spherical strips) and real-world trajectories with auxiliary signals (hurricane tracks with intensity).",
        "methodology": "Uses reparameterization- and isometry-invariant Riemannian metrics on spaces of immersed curves and their quotient (shape) spaces. Employs first-order elastic metrics Ga,b and the SRV transform q(c)=ċ/|ċ|^{1/2} to obtain an L2-flat representation enabling efficient geodesic/distance computation. For plane curves, applies Frenet formulas to derive SRV speed/curvature relations and invariants. For homogeneous spaces M=H/K, uses the generalized SRV transform Q(α)=(α(0), L_{α^{-1}}α̇/||α̇||^{1/2}) with horizontal lifting and optimization over K to compute distances. Represents constrained surfaces via augmented curves (e.g., (γ,r) in R^4 for tubes; (γ,v) in R^3×S^2 for ruled surfaces; (γ,r) on S^2×R for spherical strips) and computes shortest paths/geodesics using Riemannian optimization (Morphomatics).",
        "experimental_setup": "Primarily demonstration-style experiments/visualizations of computed shortest paths (geodesic deformations) in the induced shape spaces for: (i) tubes (curves in R^4 via centerline γ and radius function r), (ii) ruled surfaces (base curve γ and unit direction field v), (iii) spherical strips (spherical curve γ with bandwidth r using the S^2 exponential map), and (iv) hurricane tracks as discrete trajectories on S^2 augmented with maximum sustained wind, using NOAA HURDAT2 Atlantic dataset sampled every 6 hours (example year 2010). Geodesics/distances are computed numerically via the SRV framework, quotienting by reparameterizations (Diff+) and isometries; implementation uses the Morphomatics Python package with Riemannian optimization.",
        "limitations": "Largely qualitative evaluation (visual examples) with no systematic quantitative benchmarks, error analysis, or comparison to alternative metrics/registrations. Theoretical results on SRV speed/curvature are restricted to plane curves; immersion assumptions are needed (SRV image may fail to be an immersion; conditions given via common zeros of κ and ω̇). Homogeneous-space SRV requires choices of group model, left-invariant metric, and horizontal lifts; optimization over K and discretization may introduce numerical issues. Applications assume specific generative constructions (tubes/ruled/spherical strips) so results may not generalize to arbitrary surfaces or noisy/irregularly sampled data without additional modeling.",
        "future_research_directions": "Develop full statistical pipelines on the resulting shape spaces (Fréchet means, PGA/PGC, hypothesis tests) for augmented trajectories and generated surfaces; extend to hierarchical longitudinal models for group-wise trend comparison. Explore classification and prediction for hurricane tracks using intensity-augmented geodesic features. Broaden to additional constrained-surface families and other manifold-valued auxiliary fields (e.g., tangent-bundle valued data), and study robustness to noise, missing data, and irregular sampling. Provide quantitative benchmarks, computational scaling studies, and comparisons to competing registration/shape-analysis approaches; investigate handling of SRV singularities/non-immersed cases and effects of holonomy/metric choices in homogeneous-space implementations.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "RLPrompt: Optimizing Discrete Text Prompts with Reinforcement Learning",
      "full_text": "Simulations of laser-driven strong-field QED with Ptarmigan: Resolving wavelength-scale interference and 𝛾-ray polarization T. G. Blackburn,1, a) B. King,2 and S. Tang3 1)Department of Physics, University of Gothenburg, SE-41296 Gothenburg, Sweden 2)Centre for Mathematical Sciences, University of Plymouth, Plymouth, PL4 8AA, United Kingdom 3)College of Physics and Optoelectronic Engineering, Ocean University of China, Qingdao, Shandong, 266100, China (Dated: 6 September 2023) Accurate modelling is necessary to support precision experiments investigating strong-field QED phenomena. This modelling is particularly challenging in the transition between the perturbative and nonperturbative regimes, where the normalized laser amplitude 𝑎0 is comparable to unity and wavelength-scale interference is significant. Here we describe how to simulate nonlinear Compton scattering, Breit-Wheeler pair creation, and trident pair creation in this regime, using the Monte Carlo particle-tracking code Ptarmigan. This code simulates collisions between high-intensity lasers and beams of electrons or 𝛾rays, primarily in the framework of the locally monochromatic approximation (LMA). We benchmark our simulation results against full QED calculations for pulsed plane waves and show that they are accurate at the level of a few per cent, across the full range of particle energies and laser intensities. This work extends our previous results to linearly polarized lasers and arbitrarily polarized 𝛾rays. I. INTRODUCTION Experiments are planned or underway to measure nonlinear Compton and Breit-Wheeler pair-creation in the strong-field QED regime1–3, where the quantum parameter 𝜒is of order unity. For plane waves, 𝜒is the product of the (classical) dimensionless laser amplitude, 𝑎0, and the (quantum, linear) energy parameter 𝜂. One promising experimental strategy is to collide a multi- terawatt laser pulse with a beam of >10-GeV electrons, achieving 𝑎0 ∼𝑂(1)and 𝜂 ∼𝑂(0.1), and thereby reaching the strong-field regime4–8. In this case, quantum interference has a significant effect, because the formation lengths of all strong-field QED processes can be comparable to the laser wavelength. The polarization of the 𝛾rays, i.e. high-energy photons, that drive electron-positron pair creation also has an important role to play9–11. Indeed, earlier theory work suggests that 𝛾-ray polarization has a larger effect on the total probability than fermion spin, as seen when comparing the nonlinear trident process 12,13 with double nonlinear Compton scattering14,15. Accounting for both interference and polarization effects is necessary for numerical simulations to achieve an accuracy that matches the expected precision of experimental investigations of the transition regime (𝑎0 ∼1), e.g. LUXE7,8. In this work we present the Ptarmigan simulation framework, which resolves wavelength-scale interference effects by means of the locally monochromatic approximation (LMA)16–20 and how photon emission and electron-positron pair creation depend on the polarization of the high-energy photons. As we have already demonstrated that LMA-based simulations accurately model these processes in circularly polarized electromagnetic waves21,22, we consider here the interaction with linearly polarized electromagnetic waves. This is a richer and more physically relevant problem because of the broken symmetry of linear polarization. The electric and magnetic fields oscillate along fixed directions, instead of rotating around the propagation axis, thereby defining a preferred direction in space. As a result, the angular profile of the radiation emitted by electrons travelling through an linearly polarized wave lacks rotational symmetry: it is dipolar (along the 𝐵-field direction) for small 𝑎0, before becoming increasing elliptical (along the 𝐸-field direction) at larger𝑎023,24. Furthermore, the emitted 𝛾-ray photons are strongly polarized parallel to the laser 𝐸-field25–29. Inducing secondary processes with this high-energy radiation provides opportunities to test how strong-field QED processes depend on 𝛾-ray polarization. Finally, from a practical perspective, high-power laser systems are naturally linearly polarized to begin with. Converting to circular polarization at fixed laser energy leads to a peak electric field at focus that is reduced by a factor of √ 2. It follows that the classical and quantum nonlinearity parameters, 𝑎0 and 𝜒, are larger for linear polarization and hence nonlinear and quantum effects are also larger. We begin by reviewing the probability rates for photon emission (or nonlinear Compton scattering, 𝑒 →𝑒𝛾) and electron- positron pair creation (𝛾→𝑒+𝑒−) in monochromatic, linearly polarized electromagnetic waves in section II. We then discuss how the Ptarmigan simulation framework combines Monte Carlo sampling of these probability rates with tracking of cycle-averaged classical trajectories to generate predictions for final-state particle spectra, as well as the alternative models that are available, in section III. Comparisons with full QED results are presented in section IV, which validate the accuracy of the underlying approach. We also present some examples of the physics that can be explored with Ptarmigan in section V. a)Electronic mail: tom.blackburn@physics.gu.se arXiv:2305.13061v2  [hep-ph]  5 Sep 20232 II. PROBABILITY RATES The probability rates for QED processes in monochromatic electromagnetic waves are controlled by two Lorentz-invariant parameters: the normalized root-mean-square (r.m.s.) amplitude of the wave 30, 𝑎rms = 𝑒𝐸rms/(𝑚𝜔), and a (quantum) energy parameter 𝜂=𝑘.𝑞/𝑚2. Here 𝑒is the elementary charge, 𝑚is the electron mass, 𝐸rms the r.m.s. electric field, 𝑘is the wavevector of the background,𝜔=|k|is its frequency, and𝑞is the cycle-averaged momentum (orquasimomentum) of the incoming particle. We set ℏand 𝑐to unity throughout unless otherwise stated. In a plane EM wave, the quasimomentum of an electron or positron with asymptotic momentum 𝑝is 𝑞= 𝑝+𝑚2𝑎2 rms 𝑘/(2𝑘.𝑝), where 𝑞2 =𝑚2 (1 +𝑎2 rms). The peak and r.m.s. amplitudes are related by 𝑎0 = √ 2𝑎rms for linear polarization and 𝑎0 =𝑎rms for circular polarization. The quasimomentum of a photon, 𝑞𝛾, coincides with the asymptotic momentum, 𝑘′, as photons are uncharged and massless. The differential rates depend on the properties of the particle in the initial state through the two parameters 𝑎rms and 𝜂. As is characteristic of QED processes in plane EM waves with a well-defined carrier frequency, they can be written as a sum over an integral harmonic index 𝑛. The dependence on the momenta of the daughter particles is parametrized in terms of 𝑠, the fraction of the parent lightfront momentum transferred to one of the daughters, and the polar and azimuthal angles 𝜃and 𝜙. For photon emission, 𝑠=𝑘.𝑘′/𝑘.𝑞, where 𝑘′is the momentum of the emitted photon and 𝑞the quasimomentum of the electron or positron. For pair creation, 𝑠=𝑘.𝑞′/𝑘.𝑘′, where 𝑞′is the quasimomentum of the produced positron and𝑘′the momentum of the decaying photon. The two angles are conveniently defined in the zero-momentum frame (ZMF) of the scattering event. The polar angle 𝜃 is determined by kinematics if 𝑠and 𝑛are known; thus it does not appear explicitly in the theoretical results we will show. The azimuthal scattering angle 𝜙is the angle between e, one of the two vectors defining the background field’s polarization, and the projection of the emitted photon (or created positron) momentum k′(q) on the e-(e ×k) plane. In this work we use rates that are fully resolved in the polarization of the high-energy photons, whether those photons are in the initial or final state. This ensures that we capture how the positron yield in trident pair creation (𝑒→𝑒𝛾 →𝑒𝑒+𝑒−) depends on the polarization of the intermediate photon9. At the same time, the rates are averaged over the spin of the initial-state electron (or positron) and summed over the spin states of any final-state electrons or positrons. We make this assumption despite the fact that, at high intensity 𝑎0 ≫1, the radiation power of electrons that are polarized parallel or antiparallel to the magnetic field differs by 30%; or that as they continue to radiate, electrons spin-polarize antiparallel to the local magnetic field, in an analogy of the Sokolov-Ternov effect31. This is because we consider the interaction with plane-wave-like fields, where the electric and magnetic fields change sign every half-cycle (linear polarization) or rotate around the propagation axis (circular polarization). Thus initially unpolarized electrons do not accumulate significant polarization over the course of the interaction, particularly if the laser pulse is more than a few cycles in duration15. There are interaction scenarios where this does occur, but generally some symmetry-breaking mechanism is required, such as a laser field with two colours32 or a small degree of elliptical polarization33 (the latter combined with an angular cut). Here it is sufficient to treat electrons and positrons as always being unpolarized; we will return to this point in future work. The polarization of the emitted (or decaying) photon is defined in terms of the three Stokes parameters𝑆1, 𝑆2 and 𝑆3. The two basis vectors used in deriving the theoretical results are34 ε1 = ˆE + 𝑘′ 𝑥 𝜔′−𝑘′𝑧 (n −n′), ε2 = ˆB + 𝑘′ 𝑦 𝜔′−𝑘′𝑧 (n −n′), (1) where n and n′are unit vectors along the laser wavevector (k) and photon momentum (k′), respectively. The basis vectors ε1 and ε2 form an orthonormal triad with n′=k′/𝜔′. Here 𝜔′=|k′|and the coordinate system is defined by unit vectors parallel to the laser electric field (ˆE), magnetic field ( ˆB) and wavevector.𝑆1, 𝑆2 and 𝑆3 are: the degree of linear polarization with respect to the basis given in eq. (1); the degree of linear polarization with respect to the same basis, but rotated by 45 ° around k′; and the degree of circular polarization. We will refer to photons with 𝑆1 =−1, which are polarized parallel to the laser electric field, as “𝐸-polarized”, and to photons with 𝑆1 =+1, which are polarized parallel to the laser magnetic field, as “𝐵-polarized”. In what follows, we give the double-differential emission rates per unit proper time in azimuthal angle 𝜙 and lightfront momentum fraction 𝑠for the nonlinear Compton and Breit-Wheeler processes. If integrated over a plane-wave background, they are related to the probability P of the process via: P = ∫ d𝜏𝑊, (2) 𝑊 = ∞∑︁ 𝑛=𝑛★ ∫ 𝑠+ 𝑛 𝑠−𝑛 d𝑠 ∫ 2𝜋 0 d𝜙d2𝑊𝑛 d𝑠d𝜙 (3) where 𝑛★ is the threshold harmonic, the lightfront momentum fraction limits 𝑠− 𝑛 and 𝑠+ 𝑛 can depend on harmonic order, and 𝜏is the proper time. Strictly, eq. (2) is valid only when the integral is small compared to one; it may be interpreted as the limiting case of P = 1 −exp(− ∫ d𝜏𝑊), the probability for at least one event to occur. We focus here on the case that the background field is a linearly polarized electromagnetic wave; for completeness, the equivalent results for a circularly polarized wave and a constant, crossed field are presented in appendices A and B respectively.3 A. Photon emission Emission of a single, high-energy photon is often called nonlinear Compton scattering in the context of laser interactions. The double-differential emission rate per unit proper time, 𝑊𝛾, at a particular harmonic index 𝑛, in the LMA is given by19 (see also35): d2𝑊𝛾 𝑛 d𝑠d𝜙 =−𝛼𝑚 2𝜋 \u001a 𝐴2 0 (𝑛,𝑥,𝑦 )+𝑎2 rms \u0014 2 + 𝑠2 1 −𝑠 \u0015 \u0002 𝐴0 (𝑛,𝑥,𝑦 )𝐴2 (𝑛,𝑥,𝑦 )− 𝐴2 1 (𝑛,𝑥,𝑦 ) \u0003\u001b (4) where the arguments of the 𝐴functions (defined in section II C) are 𝑥=−2𝑛cos 𝜙 √︄ 2𝑎2rms𝑤𝑛(1 −𝑤𝑛) 1 +𝑎2rms , 𝑦 = 𝑎2 rms 4𝜂𝑒 𝑠 1 −𝑠, 𝑤 𝑛 = 𝑠 𝑠𝑛(1 −𝑠), 𝑠 𝑛 = 2𝑛𝜂𝑒 1 +𝑎2rms , (5) and 0 ≤𝑠≤𝑠𝑛/(1 +𝑠𝑛). The energy parameter of the electron 𝜂𝑒 =𝑘.𝑞/𝑚2. The total rate 𝑊𝛾 is obtained by integrating eq. (4) over 𝑠and 𝜙and summing the resulting partial rates 𝑊𝛾 𝑛 over all 𝑛≥1. The Stokes parameters of the emitted photon are given by27,36: 𝑆1 = 1 𝑆0 \" 2(𝐴2 1 −𝐴0 𝐴2)−( 1 +2𝑟2 𝑛 sin2 𝜙) 𝐴2 0 𝑎2rms # , 𝑆2 = 1 𝑆0 \" 𝑟2 𝑛 𝐴2 0 𝑎2rms sin 2𝜙+4𝑟𝑛 𝐴0 𝐴1√ 2𝑎rms sin 𝜙 # , 𝑆3 =0, (6) where 𝑆0 = \u0012 1 −𝑠+ 1 1 −𝑠 \u0013 (𝐴2 1 −𝐴0 𝐴2)− 𝐴2 0 𝑎2rms , 𝑟 2 𝑛 = 2𝑛𝜂𝑒 (1 −𝑠) 𝑠 −(1 +𝑎2 rms). (7) These are defined with respect to the basis given in eq. (1). In the limit𝑠→0, where emission is dominated by the first harmonic 𝑛=1, 𝑆1 =cos 4𝜙and 𝑆2 =sin 4𝜙. In general, the radiation is only partially polarized. a. Classical limit (nonlinear Thomson scattering). In the limit that 𝜂𝑒 →0, 𝑥and 𝑦have universal shapes as functions of 𝑣=𝑠/𝑠𝑛. d2𝑊𝛾,cl 𝑛 d𝑣d𝜙 =−𝛼𝑚𝑠𝑛 2𝜋 \b 𝐴2 0 (𝑛,𝑥,𝑦 )+2𝑎2 rms \u0002 𝐴0 (𝑛,𝑥,𝑦 )𝐴2 (𝑛,𝑥,𝑦 )− 𝐴2 1 (𝑛,𝑥,𝑦 ) \u0003\t . (8) The arguments of the 𝐴functions are 𝑥=−2𝑛cos 𝜙 √︄ 2𝑎2rms𝑣(1 −𝑣) 1 +𝑎2rms , 𝑦 = 𝑛𝑎2 rms𝑣 2(1 +𝑎2rms), (9) and 0 ≤𝑣 ≤1. The total rate is proportional to 𝜂𝑒. The Stokes parameters are obtained by using 𝑟2 𝑛 = (1 +𝑎2 rms)(1 −𝑣)/𝑣and replacing 1 −𝑠+1/(1 −𝑠)→ 2 in eq. (6). b. Low-intensity limit (linear Compton scattering). The arguments of the Bessel functions𝑥and 𝑦reach their largest values when 𝑠 = 𝑠𝑛/(2 +𝑠𝑛)and 𝜃 =0, where 𝑥 = √ 2𝑛𝑎rms/ √︁ 1 +𝑎2rms and 𝑦 =𝑛𝑎2 rms/[4(1 +𝑎2 rms)]. In the linear regime, 𝑎rms →0, we may therefore expand 𝐽𝑛(𝑥,𝑦)around 𝑥 =𝑦 =0. As 𝑥is linear in 𝑎rms, and 𝑦quadratic, this expansion is done to order 𝑥2𝑘 and 𝑦𝑘. For the first harmonic, we find (expanding to 𝑘 =1): d2𝑊𝛾 1 d𝑠d𝜙 = 𝛼𝑚𝑎2 rms 8𝜋 \u001a 1 −𝑠+ 1 1 −𝑠 + \u0014 2𝑠2 𝜂2𝑒 (1 −𝑠)2 − 4𝑠 𝜂𝑒 (1 −𝑠) \u0015 cos2 𝜙 \u001b , (10) where 0 ≤𝑠≤2𝜂𝑒/(1 +2𝜂𝑒), and 𝑊𝛾 1 = 𝛼𝑚𝑎2 rms 4 \u00141 2 + 4 𝜂𝑒 − 1 2(1 +2𝜂𝑒)2 + \u0012 1 − 2 𝜂𝑒 − 2 𝜂2𝑒 \u0013 ln(1 +2𝜂𝑒) \u0015 . (11) At the first Compton edge 𝑠 = 2𝜂𝑒/(1 +2𝜂𝑒), the Stokes parameters take the limiting values 𝑆1 = \u0010 1 + 2𝜂2 𝑒 1+2𝜂𝑒 \u0011−2 ≤1 and 𝑆2 =𝑆3 =0. The maximum attainable polarization of the radiation is limited by recoil effects, which generate motion in the 1/𝛾 cone; for 𝜂𝑒 ≪1, corrections to 𝑆1 =1 are proportional to ℏand powers thereof.4 B. Pair creation Production of an electron-positron pair is often called the nonlinear Breit-Wheeler processin the context of laser interactions. The double-differential pair-creation rate per unit “proper” time, 𝑊±, at a particular harmonic index 𝑛, is given by37 (see also Nikishov and Ritus35 ) d2𝑊± 𝑛 d𝑠d𝜙 = 𝛼𝑚 2𝜋 \u001a 𝐴2 0 +𝑎2 rms \u0014 1 𝑠(1 −𝑠)−2 \u0015 \u0010 𝐴2 1 −𝐴0 𝐴2 \u0011 −𝑆1 h (𝐴0 𝑟𝑛 cos 𝜙− √ 2𝑎rms 𝐴1)2 −(𝐴0 𝑟𝑛 sin 𝜙)2 i −𝑆2 h 2𝐴0 𝑟𝑛 sin 𝜙 \u0010 𝐴0 𝑟𝑛 cos 𝜙− √ 2𝑎rms 𝐴1 \u0011io (12) where the arguments of the functions 𝐴𝑖 ≡𝐴𝑖 (𝑛,𝑥,𝑦 )are 𝑥= √ 2𝑎rms𝑟𝑛 cos 𝜙 𝜂𝛾 𝑠(1 −𝑠) , 𝑦 = 𝑎2 rms 4𝜂𝛾 𝑠(1 −𝑠), 𝑟 2 𝑛 =2𝑛𝜂𝛾 𝑠(1 −𝑠)−( 1 +𝑎2 rms), (13) and the range of 𝑠is bound by |𝑠−1/2|≤ √︁ 1/4 −1/𝑠𝑛. The photon energy parameter 𝜂𝛾 = 𝑘.𝑘′/𝑚2. The Stokes parameters appearing in this expression are defined with respect to the basis given in eq. (1). The total rate is obtained by integrating eq. (12) over all 𝑠and 𝜙, then summing over all harmonic orders 𝑛 ≥𝑛★ = ⌈2(1 +𝑎2 rms)/𝜂𝛾⌉. It depends on the normalised amplitude 𝑎rms, energy parameter 𝜂𝛾 and only the first Stokes parameter 𝑆1, i.e. 𝑊±=𝑊±(𝑎,𝜂𝛾,𝑆1). C. Double Bessel functions The 𝐴functions are defined in terms of the ‘double Bessel functions’ 𝐽𝑛(𝑥,𝑦): 𝐴0 (𝑛,𝑥,𝑦 )=𝐽𝑛(𝑥,𝑦), 𝐴1 (𝑛,𝑥,𝑦 )= 𝐽𝑛−1 (𝑥,𝑦)+ 𝐽𝑛+1 (𝑥,𝑦) 2 , 𝐴2 (𝑛,𝑥,𝑦 )= 𝐽𝑛−2 (𝑥,𝑦)+2𝐽𝑛(𝑥,𝑦)+ 𝐽𝑛+2 (𝑥,𝑦) 4 . (14) The double Bessel functions extend the usual Bessel functions 𝐽𝑛(𝑥)in the following way: 𝐽𝑛(𝑥,𝑦)= 1 2𝜋 ∫ 𝜋 −𝜋 exp[−𝑖𝑛𝜃 +𝑖𝑥sin 𝜃−𝑖𝑦sin 2𝜃]𝑑𝜃 = ∞∑︁ 𝑟=−∞ 𝐽𝑛+2𝑟 (𝑥)𝐽𝑛(𝑦). (15) The 𝐴functions satisfy the following equality, when evaluated at the same𝑛,𝑥,𝑦 : (𝑛−2𝑦)𝐴0 −𝑥𝐴1 +4𝑦𝐴2 =0. (16) We have implemented the recurrence algorithm introduced by L¨otstedt and Jentschura38 in order to evaluate these functions with the necessary speed and accuracy. The kinematics of Compton scattering and pair creation mean that our implementation needs to consider only the domain 0 ≤𝑥 <𝑛 √ 2 and 0 ≤𝑦 <𝑛/2 at fixed 𝑛. III. SIMULATION FRAMEWORK By default, the Ptarmigan simulation code models particle and photon dynamics in QED by means of the LMA. This mode will be discussed in detail here and will be the focus of the benchmarking presented in section IV. However, the code also includes modes based on classical electrodynamics and the locally constant field approximation (LCFA). Ptarmigan’s coverage of polarization dependence in the two basic strong-field QED processes, as well as the theoretical models by which they are implemented, is summarized in table I. The LMA is available in the parameter region 𝜂𝑒,𝛾 ≤1 and 𝑎0 ≤20, whereas the LCFA can be used for arbitrary values of the same. In both cases, the laser may be linearly (LP) or circularly polarized (CP).5 Process Polarization Available modes 𝑒± 𝛾 laser QED classical mod. clas. 𝑒±→𝑒±𝛾 averaged (initial), summed (final) arbitrary LP / CP LMA / LCFA LMA / LCFA LCFA 𝛾→𝑒+𝑒− summed arbitrary LP / CP LMA / LCFA n/a n/a TABLE I. Polarization dependence and available models of photon emission and electron-positron pair creation in Ptarmigan. A. Default mode In the locally monochromatic approximation, the background electromagnetic field is modelled as a wave with a cycle-averaged amplitude 𝑎rms and wavevector 𝑘 that vary slowly in space and time. Charged particles move through this field on classical trajectories defined by the quasimomentum𝑞and cycle-averaged position𝑋, i.e. the slowly varying component of the worldline. These satisfy the following equations of motion39 𝑑𝑞𝜇 𝑑𝜏 = 1 2 𝑚𝜕𝜇𝑎2 rms (𝑋), 𝑑𝑋𝜇 𝑑𝜏 = 𝑞𝜇 𝑚, (17) where 𝑚is the electron mass and 𝜏is the proper time.40 The code solves eq. (17) numerically by means of a leapfrog method, thereby tracking electrons and positrons as they travel through the laser pulse. Given the particle position and momentum at a particular proper time, 𝑋(𝜏)and 𝑞(𝜏), the local energy parameter follows from 𝜂𝑒 = 𝑘.𝑞/𝑚2 and the intensity parameter from 𝑎rms = √︁ 𝑞2/𝑚2 −1. Each particle has a weight, which determines the statistical importance of that particular particle. When a simulation is used to model a real experiment, and true particle counts are required as output, the weight is used to indicate the number of physical particles represented by that individual particle. Ptarmigan simulates interactions with plane-wave or focussed laser pulses by taking the normalized squared potential to be21: 𝑎2 rms (𝑋)= [𝑎0 𝑔(𝜑)]2 1 +(𝑧/𝑧𝑅)2 exp \" − 2r2 ⊥ 𝑤2 0 +(𝜗𝑧)2 # × ( 1/2 LP 1 CP , (18) where 𝑋𝜇 = (𝑡,r⊥,𝑧), 𝑤0 is the beam waist (the radius at which the intensity falls to 1 /𝑒2 of its central value), 𝑧𝑅 =𝜋𝑤2 0/𝜆is the Rayleigh range, 𝜗=𝑤0/𝑧𝑅 is the diffraction angle, and the pulse envelope 𝑔(𝜑)is a function of phase 𝜑=𝜔(𝑡−𝑧). 1. Photon emission At each timestep of size Δ𝑡, the probability of photon emission 𝑃𝛾 = 𝑊𝛾 (𝑎rms,𝜂𝑒)Δ𝑡/(𝑞0/𝑚)is evaluated and a photon generated if 𝑟1 < 𝑃𝛾, where 𝑟1 is a pseudorandomly generated number in the unit interval. The total rate of emission𝑊(𝑎rms,𝜂𝑒) is precalculated and tabulated as a function of 𝑎rms and 𝜂𝑒. We do this by numerically integrating the double-differential rate, eq. (4) (LP) or eq. (A1) (CP), over all 𝑠and 𝜙then summing the resulting partial rates all over harmonic orders 1 ≤𝑛 ≤𝑛max. The cutoff 𝑛max is automatically determined to ensure convergence. On emission, the harmonic index 𝑛is sampled by inverting 𝑟2 =cdf(𝑛). Here 𝑟2 is another pseudorandom number and cdf (𝑛)=Í𝑛 𝑖=1 𝑊𝛾 𝑖 /𝑊𝛾 is the cumulative density function, which is also precalculated and tabulated as a function of 𝑎rms, 𝜂𝑒 and 𝑛. Once the harmonic index has been determined, the lightfront momentum transfer fraction 𝑠and azimuthal angle 𝜙are obtained by rejection sampling of the double-differential rate. In the ZMF, the photon momentum and polar scattering angle are given by \f\fk′ ZMF \f\f = 𝑚𝑛𝜂𝑒√︁ 1 +𝑎2rms +2𝑛𝜂𝑒 , cos 𝜃ZMF =1 −𝑠(1 +𝑎2 rms +2𝑛𝜂𝑒) 𝑛𝜂𝑒 . (19) These quantities, with 𝜙, allow us to construct the photon’s four-momentum𝑘′, which is then transformed back to the laboratory frame. The four-velocity of the ZMF with respect to the laboratory frame is 𝑈 = (𝑞+𝑛𝑘)/|𝑞+𝑛𝑘|. The electron (positron) quasimomentum after the scattering, 𝑞′, is fixed by momentum conservation 𝑞+𝑛𝑘 =𝑞′+𝑘′. (20) The weight of the photon is identical to the weight of the emitting electron. We then assign the newly emitted photon a set of Stokes parameters, using eq. (6) (LP) or eq. (A3) (CP). Note that we do not perform an additional sampling step to project the photon polarization onto a particular eigenstate (as is done in Li et al. 41 , for example). In fact, no projection takes place unless the photon reaches a synthetic detector, at which point we are free to choose an arbitrary basis. Once the Stokes parameters are chosen, they are transformed such that they are defined with respect to a global basis, which is illustrated in fig. 1. The code uses this global basis to unify the tracking procedures under the LMA and LCFA, which otherwise define different preferred bases. All output is defined with respect to the global basis.6 FIG. 1. In Ptarmigan, the polarization of a high-energy photon (in yellow) emitted by an electron (with quasimomentum 𝑞, in blue) is defined with respect to the basis illustrated here. The first basis vector lies in the plane defined by the laser polarizationE and wavevectork (illustrated by the grey circle), and is perpendicular to the photon momentum k′. The second basis vector is perpendicular to the first and to k′. The Stokes parameters are transformed to a local basis at each timestep so that the pair-creation probability can be evaluated. Under the locally monochromatic approximation (LMA), this local basis is defined by eq. (1); under the locally constant field approximation (LCFA) it is defined by the instantanous acceleration (see appendix B). 2. Pair creation Photons, whether externally injected or emitted during the interaction, are tracked by the simulation as they travel through the electromagnetic field on ballistic trajectories. In the current version of Ptarmigan, the only physical process that affects photon propagation is decay to an electron-positron pair. As each photon represents many real particles, this means that the (complex) polarization vector, as defined by the Stokes parameters, becomes a dynamical quantity: it is reduced in magnitude and it rotates. The change in magnitude is handled by pseudorandomly generating pair-creation events that reduce the photon weight. At each timestep of size Δ𝑡, the Stokes parameters of the photon are transformed to the basis defined by eq. (1) and the probability of pair creation 𝑊±(𝑎,𝜂𝛾,𝑆𝑗)Δ𝜏 is evaluated. An electron-positron pair is generated if 𝑟1 < 𝑊±(𝑎,𝜂𝛾,𝑆𝑗)Δ𝜏, where 𝑟1 is a pseudorandomly generated number in the unit interval, Δ𝜏 = Δ𝑡/(𝜔′/𝑚), and 𝑗 = 1 (LP) or 3 (CP). The pair creation rate is precalculated and tabulated as a function of 𝑎 and 𝜂𝛾 for 𝑆𝑗 = ±1; the value for arbitrary 𝑆𝑗 is fixed by linear interpolation between the two extreme cases, because the rate is a linear function of the Stokes parameters. If pair creation occurs, the harmonic index 𝑛is obtained by inverting 𝑟2 =cdf(𝑛), where 𝑟2 is a pseudorandom number and cdf(𝑛)=Í𝑛 𝑖=𝑛★ 𝑊± 𝑖 /𝑊±is the cumulative density function, precalculated and tabulated as a function of𝑎, 𝜂𝛾 and 𝑛for 𝑆𝑗 =±1. The fraction of lightfront momentum transferred to the positron 𝑠and azimuthal angle 𝜙are obtained by rejection sampling of the double-differential rate, eq. (12) (LP) or eq. (A9) (CP). The positron four-momentum 𝑞′is constructed in the ZMF using 𝑞0 ZMF =𝑚\u0000𝑛𝜂𝛾/2\u00011/2 , \f\fq′ ZMF \f\f =𝑚 \u0002 𝑛𝜂𝛾/2 −(1 +𝑎2 rms) \u00031/2 , cos 𝜃ZMF = (1 −2𝑠)𝑞0 |q′| (21) and then transformed back to the laboratory frame. The electron four-momentum 𝑞follows from 𝑘′+𝑛𝑘 =𝑞+𝑞′. (22) As pair creation is much rarer than photon emission, Ptarmigan incorporates a form of event biasing, where we artificially increase the pair-creation rate 𝑊±by a factor 𝑅↑, while reducing the weight of the daughter particles by the same factor: see Section 3.3 in Blackburn and King 22 . Thus a photon with initial weight 𝑤𝛾, creating an electron and positron with weights 𝑤𝛾/𝑅↑, becomes a photon with weight𝑤𝛾 (1−1/𝑅↑). This accelerates the convergence of the statistical properties of pair-creation events, at the expense of generating many low-weight particles that must subsequently be tracked. The photon polarization must rotate because it is a mixed state and the pair-creation rate is polarization-dependent. Consider, for example, a photon with weight 𝑤𝛾 and arbitrary −1 ≤ 𝑆1 ≤1 (taking the background to be LP). The weight in each eigenstate 𝑆1 = ±1 is 𝑤± = 𝑤𝛾 (1 ±𝑆1)/2. Decay to electron-positron pairs means that these individual weights are reduced to 𝑤′ ± = 𝑤𝛾 (1 ±𝑆1)[1 −𝑊±(𝑎rms,𝜂𝛾,±1)Δ𝜏]/2 after a time interval Δ𝜏. The decrease in the total weight 𝑤𝛾 is therefore accompanied by a change in the Stokes parameters, as 𝑆′ 1 =(𝑤′ +−𝑤′ −)/(𝑤′ ++𝑤′ −)≠𝑆1. To account for this, the photon Stokes parameters are modified after each timestep to: 𝑆′ 𝑖 =𝑆𝑖 1 −𝑊±(𝑎,𝜂𝛾,0)Δ𝜏 1 −𝑊±(𝑎,𝜂𝛾,𝑆𝑗)Δ𝜏 −𝛿𝑖 𝑗 𝑊±(𝑎,𝜂𝛾,1)−𝑊±(𝑎,𝜂𝛾,−1) 2 Δ𝜏, (23)7 where 𝛿𝑖 𝑗is the Kronecker delta, 𝑖 ∈{1,2,3}, and 𝑗 = 1 (LP) or 3 (CP). This correction becomes significant only when pair creation itself becomes likely.42 The Stokes parameters of the surviving photon 𝑆′ 𝑖 are then transformed back to the global basis (see fig. 1). B. Classical electrodynamics It is possible to use the same simulation framework to model the interaction entirely classically. In this case, the electron (positron) does not recoil at individual emission events, according to eq. (20). Instead, energy loss is accounted for by a radiation reaction force, here in the Landau-Lifshitz prescription 43. Under the additional assumption that radiative losses are weak, i.e. that 𝑞does not change significantly over a single cycle, we can define the following equation of motion for the quasimomentum 𝑞44: 𝑑𝑞𝜇 𝑑𝜏 = 1 2 𝑚𝜕𝜇𝑎2 rms (𝑋)− 2𝛼 3 𝑚(𝑎rms𝜂𝑒)2𝑞𝜇. (24) We solve eq. (24) numerically to track charged particles as they travel through the EM field, emitting radiation. This radiation is modelled by pseudorandomly generating ‘photons’, even though these do not exist classically: this works because the relevant physical observable in classical electrodynamics, the energy per unit frequency 𝑑E 𝑑𝜔 ′, can be used to define a ‘number’ spectrum via 𝑑𝑁 𝑑𝜔 ′ =(ℏ𝜔′)−1 𝑑E 𝑑𝜔 ′ (temporarily restoring factors of ℏ). The emission algorithm is then adapted so that it uses the nonlinear Thomson spectrum, eq. (8), rather than the nonlinear Compton spectrum, eq. (4), as follows. At each timestep, a photon is generated if a pseudorandom number 𝑟1 < 𝑃, where 𝑃=𝑊𝛾,cl (𝑎,𝜂𝑒)Δ𝑡/(𝑞0/𝑚). The total rate is precalculated by integrating eq. (8) over all𝑣and 𝜙, then summing over all𝑛, and is tabulated as a function of𝑎. On emission, a harmonic index 𝑛is obtained by inverting 𝑟2 =cdf(𝑛), where the cumulative density function is itself tabulated as a function of 𝑎and 𝑛. We then sample 𝑣and 𝜙from eq. (8), which define the photon momentum and polar scattering angle in the electron’s instantaneous rest frame (IRF): \f\fk′ IRF \f\f = 𝑚𝑛𝜂𝑒√︁ 1 +𝑎2rms , cos 𝜃IRF =1 −2𝑣. (25) The four-momentum is then transformed back to the laboratory frame, using that the IRF travels at four-velocity𝑈 =𝑞/|𝑞|. Pair creation has no classical equivalent and is therefore automatically disabled. C. LCFA-based dynamics The the locally constant field approximation (LCFA) 45–48 is the basis of the standard method by which strong-field QED processes are included in numerical simulations 49,50. The essential difference to the LMA is that photon emission and pair creation are assumed to occur instantaneously, i.e. that their formation lengths are much smaller than the laser wavelength. The rates are therefore functions of the locally defined quantum parameter 𝜒𝑒,𝛾 , which is defined in terms of the instantaneous (kinetic) momentum 𝜋𝜇; the particle worldline (𝑥𝜇) is defined at all, arbitrarily small, timescales. In an LCFA-based simulation, charged particle trajectories follow from the Lorentz force equation: 𝑑𝜋𝜇 𝑑𝜏 = 𝑞𝐹𝜇𝜈 𝜋𝜈 𝑚 𝑑𝑥𝜇 𝑑𝜏 = 𝜋𝜇 𝑚, (26) where 𝐹𝜇𝜈 is the electromagnetic field tensor and 𝑞 =±𝑒is the charge. The electric field of a linearly polarized laser pulse is given by E =Re(˜E)𝑔(𝜑)+ Im(˜E)𝑑𝑔 𝑑𝜑 , where ˜E is the paraxial solution for the complex electric field of a focussed Gaussian beam51 and 𝑔(𝜑)is the pulse envelope: we include terms up to fourth-order in the diffraction angle 𝜗 = 𝑤0/𝑧𝑅. A circularly polarized pulse is defined by adding together two linearly polarized pulses that have a phase difference of 𝜋/2 and orthogonal polarization vectors. The trajectory is partitioned into timesteps of size Δ𝑡; photon emission occurs in a given timestep if a pseudorandom number 𝑟 <𝑊𝛾 (𝜒𝑒)Δ𝑡/(𝜋0/𝑚), where 𝑊𝛾 is the LCFA photon emission rate. The total rate is precalculated using eq. (B5) and stored as a lookup table in 𝜒𝑒. When emission occurs, the photon momentum k′is constructed by first sampling the energy from the single-differential rate [eq. (B4)], and then sampling the two angles from the triple-differential spectrum [eq. (B1)]. The photon is then assigned a set of Stokes parameters using eq. (B2). The electron momentum after the scattering is fixed by conserving three-momentum, π′ = π −k′, which leads to a small, 𝑂(1/𝛾2), error in energy conservation 52. Pair creation is modelled in an analogous way, by tracking the photons along ballistic trajectories and sampling the LCFA pair-creation rate [eq. (B9)]. An additional subtlety is that the Stokes parameters entering the rates are defined with respect to a basis that depends on the8 instantaneous electric and magnetic fields (see appendix B); thus the Stokes parameters must be transformed at every timestep to match. We can define a classical LCFA framework in much the same way that we defined a classical LMA, by accounting for continuous radiative energy losses in the equations of motion themselves. Charged particle trajectories are therefore obtained by numerically solving the Landau-Lifshitz equation 53,54. Photons are pseudorandomly generated along these trajectories by sampling the classical emission spectrum [eq. (B6)]. We have also implemented a phenomenologically motivated modified classical model which incorporates quantum corrections to the emission spectrum, but does not include stochastic recoil52,55. In this case, the radiation-reaction force is reduced in magnitude by the Gaunt factor 2, 0 < G(𝜒𝑒)< 1, and photons are sampled from the QED emission spectrum [eq. (B1)]. D. Applicability The LMA is built on the assumption that the laser amplitude and wavevector are slowly varying functions of space and time. Investigations in this and previous work support LMA-based simulations being accurate in practice if 𝑁, the number of cycles equivalent to the pulse duration, and 𝑤0, the laser focal spot size, satisfy 𝑁 ≳ 4 and 𝑤0 ≳ 2𝜆(see Supplementary Materal in Ref. 21 and also Ref. 22). Generally, there is no condition on the size of 𝑎rms or 𝜂. However, if classical RR is modelled using the LMA, we have the additional requirement that the energy loss per cycle is relatively small (see section III B) and therefore that 𝑎2 rms𝜂𝑒 ≲ 30. The LCFA requires that the formation lengths of all QED processes be much smaller than the laser wavelength, or equivalently, 𝑎rms ≫1 and 𝑎2 rms/𝜂 ≫1. Note that even if these are satisfied, photon spectra are only accurate above the first Compton edge, 𝑠 >2𝜂/(1 +𝑎2 rms +2𝜂). As a rule of thumb, for reasonable electron energies (up to 10s of GeV), the LCFA begins to be reliable above 𝑎0 ≳ 5. Provided that 𝑎0 is large enough, there are in principle no restrictions on the pulse duration or focusing strength. Nevertheless, the high-order paraxial approximation thatPtarmigan uses to generate the laser fields in LCFA mode requires that 𝑤0 ≳ 2𝜆. In LCFA mode, Ptarmigan works in a similar way to the particle-in-cell (PIC) codes that include strong-field QED processes, with the exception that the fields in Ptarmigan are prescribed, not self-consistently evolved. PIC codes that have been extended to include strong-field QED, as well as spin and polarization dependence, include EPOCH (see Supplemental Material in Gong, Hatsagortsyan, and Keitel 56 ), OSIRIS (see Qian et al. 57 ) and YUNIC (see references in Song, Wang, and Li 58 ). These are mainly used to simulate laser-beam or laser-matter interactions2. Beam-beam interactions can be simulated using the dedicated code CAIN17, which also includes self-consistent field evolution and strong-field QED processes under the LCFA. It further implements an equivalent of the LMA for the modelling of laser-beam interactions; however, if the laser is linearly polarized, its coverage is limited to nonlinear Compton scattering and to 𝑎0 < 3 (unlike Ptarmigan, which has coverage up to 𝑎0 < 20 for both nonlinear Compton and nonlinear Breit-Wheeler). Other codes that use an equivalent of the LMA to simulate laser-beam interactions are NI16 and IPstrong59. Under certain conditions, it is not necessary to make approximations like the LMA or LCFA. For example, if multiple emission effects are negligible, one can integrate the nonlinear Compton cross section over the collision phase space directly 60. This captures subharmonic structure in the radiation spectrum, but not secondary events like further photon emission or pair creation. In the classical regime, the radiation spectrum can be obtained directly from the Li ´enard-Wiechert integrals: see, for example, RDTX61 and its predictions of classical RR in laser-beam interactions62. IV. BENCHMARKING We begin by comparing the polarization-resolved spectra of photons emitted when a high-energy electron collides with an intense laser pulse, which we model as a 1D pulsed plane wave. Its vector potential 𝑒𝐴𝜇 (𝜑) = 𝑚𝑎0 sin 𝜑𝑔(𝜑)𝜖𝜇 1 , where 𝜖𝜇 1 = (0,1,0,0)and the temporal envelope, 𝑔(𝜑)=cos2 [𝜑/(2𝑁)], is non-zero for phases 𝜑that satisfy |𝜑|< 𝑁𝜋; the number of cycles corresponding to the total duration of the pulse 𝑁 = 16. (The full-width-at-half-maximum duration of the intensity profile is 𝑇[fs]≃ 0.97𝑁𝜆[0.8 𝜇m]≃ 15.5.) We choose three values of 𝑎0 ∈{0.5,2.5,10}to illustrate the transition from the perturbative to nonperturbative regimes. The energy parameter of the electrons is fixed at 𝜂𝑒 = 0.1, which corresponds to an energy of 8.4 GeV for a head-on collision with a laser of wavelength of 0.8 𝜇m. Furthermore, in order to make comparisons with theory calculations that are first-order (single-vertex) in nature, we assume that this energy parameter does not change during the interaction with the pulse, i.e. we neglect quantum radiation reaction effects 63. Our simulation and theory results are shown in fig. 2. Harmonic structure, which is clearly visible for 𝑎0 = 0.5, is washed out as the laser intensity increases. The first Compton edge is redshifted to smaller energies and the spectrum becomes increasingly synchrotron-like as the number of contributing harmonics increases. The radiation is mainly polarized along the laser electric field, though the exact polarization purity is also photon-energy dependent: at very small 𝑠, 𝐸- and 𝐵-polarized photons are equally likely, whereas at the Compton edges,9 0.0 0.1 0.2 0.3 0.4 10-4 10-3 0.01 0.1 1.0 s dNγ/ds E-pol B-pol (a) a0 =0.5 -2.0 -1.0 0.0 1.0 2.0 -2.0 -1.0 0.0 1.0 2.0 rx ry (b) dNγ/d2r⟂ (E-pol) QEDsimulation 6×10-3 0 2 4 -2.0 0.0 2.0 -2.0 0.0 2.0 rx ry (c) dNγ/d2r⟂ (B-pol) QEDsimulation 6×10-4 0 2 4 0 π/4 π/2 3π/4 π 0.0 0.5 1.0 1.5 2.0 2.5 φ dNγ/d2r⟂ ×10-3 (d) r⟂ 2 =1 0.0 0.2 0.4 0.6 10-3 0.01 0.1 1.0 s dNγ/ds E-pol B-pol (e) a0 =2.5 -4.0 -2.0 0.0 2.0 4.0 -4.0 -2.0 0.0 2.0 4.0 rx ry (f) dNγ/d2r⟂ (E-pol) QEDsimulation 4×10-2 0 2 -4.0 -2.0 0.0 2.0 4.0 -4.0 -2.0 0.0 2.0 4.0 rx ry (g) dNγ/d2r⟂ (B-pol) QEDsimulation 4×10-3 0 2 0 π/4 π/2 3π/4 π 0.0 2.0 4.0 6.0 8.0 φ dNγ/d2r⟂ ×10-3 (h) r⟂ 2 =2 0.0 0.2 0.4 0.6 0.8 1.0 10-3 0.01 0.1 1.0 10 100 s dNγ/ds E-pol B-pol (i) a0 =10.0 -10.0 -5.0 0.0 5.0 10.0 -10.0 -5.0 0.0 5.0 10.0 rx ry (j) dNγ/d2r⟂ (E-pol) QEDsimulation 4×10-2 0 2 -10.0 -5.0 0.0 5.0 10.0 -10.0 -5.0 0.0 5.0 10.0 rx ry (k) dNγ/d2r⟂ (B-pol) QEDsimulation 6×10-3 0 2 4 0 π/4 π/2 3π/4 π 0.0 1.0 2.0 3.0 φ dNγ/d2r⟂ ×10-2 (m) r⟂ 2 =2 FIG. 2. Photon spectra predicted by LMA-based simulations and by QED, for electrons with energy parameter𝜂𝑒 =0.1 colliding with linearly polarized, pulsed plane waves that have peak amplitude 𝑎0 =0.5 (top row), 2.5 (middle row) and 10.0 (bottom row): (first column) 𝑑𝑁𝛾/𝑑𝑠 from simulations (solid lines) and QED (black, dashed lines); (second and third columns) the polarization-resolved angular profiles of the emitted radiation; (fourth column) lineouts through the angular profiles at fixed 𝑟⊥, from simulations (solid lines) and QED (black, dashed lines). 𝐸-polarization dominates. The angular profile of the 𝐸-polarized radiation changes from a dipole, at small 𝑎0, to an ellipse elongated along the laser electric field, at large 𝑎0. The same compression in the 𝐵-direction may be seen in the angular profile of the 𝐵-polarized radiation, which changes from a quadrupole to a double ellipse. The presence of an extinction line along 𝑟𝑦 =0 may be understood classically: for observers in this plane, which is also the plane of the trajectory, there is no vertical component of the electron current. Our results demonstrate that simulations accurately reproduce what is expected from theory, both in terms of the absolute numbers and shapes of the spectra. However, note that simulations have a finite cutoff for the largest harmonic order, so the high-energy tail of the spectrum will be underestimated. Close examination of the first Compton edge, particularly in fig. 2(a), reveals that the theory prediction is somewhat smoother than the simulation result. This softening occurs because of interference at the scale of the pulse duration64,65, which the LMA neglects: the longer the pulse, the less significant this becomes. We now consider positron production by high-energy photons colliding with an intense laser pulse. In this comparison the laser temporal envelope is Gaussian, 𝑔(𝜑)=exp[−𝜑2/(4𝑁2)], and 𝑁 =16. (The full-width-at-half-maximum duration of the intensity profile is 𝑇[fs]≃ 𝑁𝜆[0.8 𝜇m]≃ 16.0.) We choose three values of 𝑎0 ∈{0.5,1.0,2.5}to illustrate the transition from the perturbative to quasistatic (tunnelling) regimes. The energy parameter of the photons is fixed at𝜂𝛾 =0.2, which corresponds to an energy of 16.8 GeV for a laser wavelength of 0.8 𝜇m. The pair-creation probability is much smaller than one for all cases considered, so we use a rate biasing factor of𝑅↑∈{1015,2×108,105}to be able to resolve the positron spectrum. Our simulation and theory results are shown in fig. 3. As is the case in photon emission, the harmonic structure that is visible in the multiphoton regime, 𝑎0 ≲ 1, is washed out as10 0.2 0.4 0.6 0.8 0.0 2.0 4.0 6.0 s dN+/ds ×10-12 (a) a0=0.5 0.2 0.4 0.6 0.8 0.0 0.2 0.4 0.6 0.8 1.0 s p⟂/m (b) simulation QED m d2N+ dsdp⟂ 4×10-13 4×10-10 0.2 0.4 0.6 0.8 0.0 0.5 1.0 1.5 2.0 s dN+/ds ×10-7 (c) a0=1.0 0.2 0.4 0.6 0.8 0.0 0.2 0.4 0.6 0.8 1.0 1.2 s p⟂/m (d) simulation QED m d2N+ dsdp⟂ 3×10-9 3×10-6 0.2 0.4 0.6 0.8 0.0 2.0 4.0 6.0 s dN+/ds ×10-4 (e) a0=2.5 0.2 0.4 0.6 0.8 0.0 0.5 1.0 1.5 2.0 2.5 s p⟂/m (f) simulation QED m d2N+ dsdp⟂ 7×10-6 7×10-4 FIG. 3. Positron spectra for high-energy photons with energy parameter 𝜂𝛾 = 0.2 colliding with linearly polarized, pulsed plane waves that have peak amplitude 𝑎0, wavelength 0.8 𝜇m, and Gaussian temporal envelope. Left panels give the single-differential spectra for photons that are linearly polarized parallel (blue) or perpendicular (orange) to the laser electric field, as predicted by LMA-based simulations (solid lines) and by QED (dashed lines). Right panels give the log-scaled double-differential spectra for unpolarized photons. The 𝑎0s have been chosen to illustrate the transition from the perturbative (multiphoton) regime in (a, b) to the quasistatic (tunnelling) regime in (e, f). 𝑎0 increases. The theory prediction is generally smoother than the simulation results because of pulse-envelope effects, which are more significant for a threshold process like pair creation. The pulse contains a (small) range of frequency components and therefore, at fixed 𝑠, there is a range of threshold harmonic orders, spread around the LMA-predicted threshold order, 𝑛★. This may be seen in the double-differential spectra, where the simulation results have clearly defined harmonics [observe the rings in the left-hand side of fig. 3(b)] and the theory results contain substructure between these harmonics. Nevertheless, there is generally good agreement between the theory and simulations: notice that the pair creation probability increases by five orders of magnitude between 𝑎0 =0.5 and 𝑎0 =1.0. Finally, we provide a benchmark forPtarmigan’s classical electrodynamics mode. We compare the simulation results against a direct calculation of the 𝑠-weighted spectrum, given the classical electron trajectory for two cases: i) where radiation reaction is ignored, so the trajectory satisfies the Lorentz force equation; and ii) where radiation reaction is accounted for, so the trajectory satisfies the Landau-Lifshitz equation. In the latter case, the electron’s energy parameter decreases as it propagates through the laser pulse. Solving the Landau-Lifshitz equation for a circularly polarized, pulsed plane wave with envelope𝑔(𝜑), we find:66 𝜂(𝜑)= 𝜂𝑒 1 +2 3 𝛼𝑎2 0𝜂𝑒I(𝜑) , I(𝜑)= ∫ 𝜑 −∞ \" 𝑔2 (𝜓)+ \u0012 𝑑𝑔 𝑑𝜓 \u00132# d𝜓, (27) where 𝜂𝑒 is the initial energy parameter. Using the LMA equations of motion [eq. (24)] for a plane wave would give the same11 classical RR no RR 10-3 10-2 0.1 1 10 0.0 0.1 0.2 0.3 0.4 0.5 s sdN γ/ds (a) classical RR 10-6 10-5 10-4 10-3 10-2 0.1 1 10 10-4 10-3 10-2 0.1 1 s sdN γ/ds (b) classical RR no RR 0.5 1 5 10 50 100 0.000 0.005 0.010 0.015 0.020 0.025 0.030 s sdN γ/ds (c) FIG. 4. (a, b) Radiation spectra predicted by LMA-based simulations (solid lines) and classical theory (dashed lines), for electrons with energy parameter 𝜂𝑒 =0.4 colliding with circularly polarized, pulsed plane waves that have peak amplitude𝑎0 =2.5, wavelength 0.8 𝜇m, and duration equivalent to 𝑁 =32. (c) As in (a) and (b), but for electrons with 𝜂𝑒 =100 colliding with CP pulsed plane waves with 𝑎0 =1.0 and 𝑁 =4. Vertical, dot-dashed lines in (a) and (c) give the positions of the first nonlinear Thomson edges predicted by eq. (28). The horizontal, dot-dashed lines in (b) and (c) gives eq. (29), the IR limit expected from theory. There is no cutoff in classical electrodynamics so the spectrum extends beyond 𝑠=1. result, except that the derivative term in I(𝜑)would be absent; this is because LMA locally approximates variations in the envelope19. Let us consider the case that 𝑎0 =2.5, 𝜂𝑒 =0.4 and 𝑔(𝜑)=cos2 [𝜑/(2𝑁)], where 𝑁 =32. (The full-width-at-half-maximum duration of the intensity profile is 31.0 fs.) We find that the final energy parameter 𝜂′ 𝑒 =𝜂𝑒/(1 +1 2 𝜋𝛼𝑎2 0𝜂𝑒𝑁)≃ 0.209, i.e. that the electron loses almost half its energy. An energy loss of this magnitude manifests itself in a significant redshift of the first nonlinear Thomson edge, which is located at: 𝑠edge =min 𝜑 2𝜂2 (𝜑) 𝜂𝑒 \u0002 1 +𝑎2 0𝑔2 (𝜑) \u0003 . (28) We obtain 𝑠edge ≃0.0454 with radiation reaction and 𝑠edge =2𝜂𝑒/(1 +𝑎2 0)≃ 0.110 without. Both are in good agreement with the results of LMA-based simulations and direct calculations of the Lienard-Wiechert integrals from classical electrodynamics: see fig. 4(a). The photon spectrum, in the presence and absence of classical radiation reaction, is reproduced very well by the simulations. There is, however, a discrepancy at very small 𝑠, shown in fig. 4(b). The LMA predicts that 𝑑𝑁𝛾/𝑑𝑠 tends to a constant in the infrared limit,67 i.e., that lim𝑠→0 𝑠(𝑑𝑁𝛾/𝑑𝑠)∝ 𝑠, whether radiation reaction is present or absent. Naturally, the simulations obtain the same result. However, it can be shown by regularising the plane wave result 68 that in the exact plane wave result, 𝑑𝑁𝛾/𝑑𝑠diverges as 𝑠→0, and69: lim 𝑠→0 𝑠𝑑𝑁𝛾 𝑑𝑠 = 𝑒2 4𝜋2 \u0014\u0012 2 +Δ Δ \u0013 ln(1 +Δ)−2 \u0015 , Δ= lim 𝜑→∞ \u0014 𝜂𝑒 𝜂(𝜑) \u00152 −1. (29) In the limit Δ→0, the exact plane-wave result for the IR limit becomes 𝑠(𝑑𝑁𝛾/𝑑𝑠)→( 𝑒2/24𝜋2)Δ2. We note that logarithmic form of eq. (29) is similar in structure to the low-𝜔′limit of the energy spectrum 𝑑E/𝑑𝜔′derived by Di Piazza70 . The non-zero IR limit for the energy spectrum originates from the fact the electron moves with reduced velocity after the collision; as it is12 theory simulation : without /with QRR 1.0 1.5 2.0 2.5 3.0 10-20 10-17 10-14 10-11 10-8 a0 N+ (a) 1.0 1.5 2.0 2.5 3.0 - 30 - 25 - 20 - 15 - 10 - 5 0 a0 Δ N+ (%) (b) pol pol + QRR FIG. 5. (a) Positron yield per incident electron for electrons with energy 𝐸𝑒 =16.5 GeV (𝜂𝑒 ≃0.197) colliding with linearly polarized laser pulses with peak amplitude 𝑎0 and duration equivalent to 𝑁 =16 cycles: simulation results, accounting for photon-polarization effects, with (blue points) and without (green points) quantum radiation reaction. (b) The change in the positron yield when taking into account: the polarization of the intermediate photon (green points) and additionally radiative energy losses (blue points). The coloured bands indicate the uncertainty in the simulation results at the single standard deviation level. Our results are crosschecked against theory data from Tang and King 13 (black, dashed lines). associated with timescales much longer the laser period, the discrepancy is found at very small 𝑠, where envelope effects are important and the LMA is not accurate. Finally, we present a more extreme example, in which LMA-based simulations are expected to fail. We set the electron energy parameter to 𝜂𝑒 =100 and the laser amplitude and duration to 𝑎0 =1.0 and 𝑁 =4. In this case 𝑎2 0𝜂𝑒 =100, which means that the electron loses a significant fraction of its energy in a single cycle and therefore we cannot assume that its quasimomentum is slowly varying (see section III D). We see from fig. 4(c) that, while the simulations are accurate in the no-RR case, they reproduce only the gross structure and redshifting of the spectrum when classical RR is included. Interference effects not captured by the LMA mean that a distinct Compton edge, expected to be located at 𝑠 ≃5 according to eq. (28), does not emerge; the broad spectral feature appearing at 𝑠≃50 is completely missed for the same reasons. V. EXAMPLES Here we present two examples of the physics that can be explored with polarization-resolved simulations. A. Trident pair creation Electron-laser collisions can produce a large flux of photons with energies comparable to that of the incident electron. The probability that these photons create pairs, and therefore fail to escape the pulse, depends not only the photon’s momentum but also its polarization. At large 𝑎0 and small 𝜒𝛾, for example, 𝐵-polarized photons are twice as likely to pair create as𝐸-polarized photons. The fact that electron-laser collisions produce mainly𝐸-polarized photons means that the positron yield is overestimated by simulations that use spin-averaged and summed probability rates 9. As Ptarmigan incorporates polarization dependence in both photon emission and pair creation, we consider how the yield of positrons changes when the polarization of the intermediate photon is taken into account, as an example. The rapid growth of the pair-creation probability with increasing photon energy means that the dominant contribution to the trident positron yield comes from the tail of the photon spectrum. Resolving this tail with Monte Carlo simulations requires large statistics: the results presented in fig. 5 are the mean and standard deviation obtained from an ensemble of𝑁𝑐 simulated collisions, where each collision includes 10 7 primary electrons and 𝑁𝑐 = {200,100,20,5,5,5}for 𝑎0 = {0.75,1.0,1.5,2.0,2.5,3.0} respectively. To resolve the pair creation itself we set the rate biasing factor to𝑅↑={1021,1017,1013,1011,1010,108}, respectively. We set the electron energy parameter to 𝜂𝑒 =0.197, which is equivalent to an energy of 16.5 GeV for a head-on collision, the laser pulse envelope to 𝑔(𝜑)=cos2 [𝜑/(2𝑁)], where 𝑁 =16, and vary 𝑎0 between 0.75 and 3.0. In order to compare our results with a direct numerical calculation of the two-step trident yield13, we initially disable the recoil associated with photon emission, i.e. quantum radiation reaction. Simulations show that taking the polarization of the intermediate photon into account reduces the yield by ∼16% across the full range of 𝑎0 we have considered, which is consistent with the theoretical result. The yields themselves are consistent with the theory results at the 2% level (for 𝑎0 ≥1) and 5% level (for 𝑎0 =0.75), albeit that simulations predict fewer positrons than13 - 20 - 10 0 10 20 - 20 - 10 0 10 20 γ θx γ θy 2 4 0 (a) dNγ/d2θ (mrad-2) LMA LCFAalongx - 20 - 10 0 10 20 - 20 - 10 0 10 20 γ θx γ θy 2 4 0 (b) dNγ/d2θ (mrad-2) LMA LCFA along y p⟂ |p⟂|=ma0 -eE θx θy (c) FIG. 6. Angular profile of the radiation emitted when an energy with energy parameter𝜂𝑒 =0.2 collides with a circularly polarized laser pulse of amplitude 𝑎0 = 10, as predicted by LMA- and LCFA-based simulations: (a) horizontally and (b) vertically polarized components. The origin of the angular structure is illustrated in (c): the electron is accelerated by the laser electric field (in green) on a circular orbit (in blue), emitting radiation (in orange) that is mostly polarized along the instantaneous acceleration. expected. This is because the nonlinear Compton rate is only summed up to a finite cutoff harmonic 𝑛max. When quantum radiation reaction is enabled, the positron yield is further reduced. This is because radiative energy losses reduce the electron energy parameter 𝜂𝑒 and therefore the energy parameters of the photons that go on to produce electron-positron pairs. This correction becomes increasingly important as 𝑎0 rises. B. Polarization-resolved angular profiles Measuring the angular profile of the radiation emitted in an electron-laser collision has been proposed as means of inferring the laser amplitude 𝑎0, because the profile effectively carries information about the electron transverse momentum23,71. Consider a high-energy electron colliding head-on with a circularly polarized laser pulse with envelope 𝑔(𝜑). The transverse momentum as a function of phase is 𝑝⊥(𝜑)= 𝑚𝑎0𝑔(𝜑); assuming that the longitudinal momentum is sufficiently large that the Lorentz factor 𝛾 ≫𝑎2 0, the angle between the electron momentum and the collision axis is𝜃(𝜑)≃ 𝑎0𝑔(𝜑)/𝛾. As the radiation is strongly beamed along the instantaneous momentum, the angular size of the profile ∼𝑎0/𝛾. Let us consider the angular profile of the radiation emitted when an electron with initial energy parameter 𝜂𝑒 =0.2 collides with a circularly polarized laser pulse of amplitude 𝑎0 =10 (the pulse envelope 𝑔(𝜑)=cos2 [𝜑/(2𝑁)]and 𝑁 =16), assuming that we also resolve the polarization of emitted radiation. Figure 6 gives the angular distributions expected if we select photons that are linearly polarized along the horizontal or vertical axes. We see that a clear “batwing” structure emerges, with extinction regions lined up along the polarization axis. This relationship may be understood classically with the help of the diagram in fig. 6(c), which shows the electron trajectory in a monochromatic, circularly polarized plane wave, as viewed along the collision axis (or laser wavevector). The crucial point is that in the transverse plane the electron’s instantaneous momentum, p⊥, and acceleration, −𝑒E/𝑚, are perpendicular to each other. In a constant, crossed field, radiation is polarized along the direction of the instantaneous acceleration. Thus a photon, emitted by a electron with 𝑝𝑥 = 𝑚𝑎0 and 𝑝𝑦 = 0 (as shown in the diagram), travels horizontally (i.e. in 𝑥) and is polarized vertically (i.e. in 𝑦). Selecting the horizontally polarized component, for example, then leads to an extinction region along the horizontal axis: see fig. 6(a). The same structure emerges in both LMA- and LCFA-based simulations because 𝑎0 =10 and the photon formation length is14 small compared to the laser wavelength. In our previous work21, we observed that the LMA effectively moves the fast oscillation of the trajectory into the QED rates. The same phenomenon occurs here: the structure in the polarization-resolved angular profile comes from the azimuthal angle dependence in the Stokes parameters, eq. (A3), rather than the trajectory itself. We conclude that, in the same way that the polarization-summed angular profile contains information about the transverse momentum, the polarization-resolved angular profile additionally contains information about the transverse acceleration. VI. SUMMARY We have presented a simulation framework based on the locally monochromatic approximation (LMA), which enables us to predict strong-field QED interactions in the perturbative ( 𝑎0 ≪1), transition (0 .5 ≲ 𝑎0 ≲ 2), and nonperturbative regimes (𝑎0 ≫1). The limitations of this approach are that i) the background field must be sufficiently ‘plane-wave-like’, which sets bounds on the duration of the laser pulse and ii) the computational cost of evaluating very high-order harmonics restricts our implementation of the LMA to normalized amplitudes 𝑎0 ≤20. Nevertheless, this goes considerably further than any previous code and ensures good overlap with the region in which the LCFA is accurate. In contrast to our previous work 21,22, where we examined circularly polarized lasers and unpolarized 𝛾 rays, we have considered here the physically richer problem of linearly polarized lasers. The broken symmetry of this case makes the numerical implementation more challenging, because the loss of azimuthal symmetry means that an additional integral must be performed to evaluate the probability rates. It also makes it necessary to account for𝛾-ray polarization effects, because the radiation emitted by an electron in a linearly polarized background is preferentially polarized along the direction of the laser electric field. The open-source Ptarmigan code72 can now be used to simulate strong-field QED interactions: in linearly or circularly polarized, plane-wave or focussed, laser pulses; using QED, classical or modified-classical models of the particle dynamics; with either LMA- or LCFA-based probability rates. Fine-grained control of the physics under consideration can be achieved by enabling (or disabling) radiation reaction, electron-positron pair creation, or the polarization dependence thereof. Our benchmarking against theoretical calculations of nonlinear Compton scattering, nonlinear Breit-Wheeler pair creation, and trident pair creation shows that the code achieves per cent level accuracy across the whole transition regime (0 .5 ≲ 𝑎0 ≲ 2). This accuracy is also maintained at higher 𝑎0, where the LMA automatically recovers the LCFA where it should do so 19. The Ptarmigan simulation framework is designed to be extensible and additional physics can be included, as motivated by experimental needs. Future work will include the role of fermion spin and higher order processes, such as vacuum polarization. ACKNOWLEDGMENTS T.G.B. thanks Kyle Fleck (Queen’s University Belfast) for contributions to thePtarmigan source code. The computations were enabled by resources provided by the Swedish National Infrastructure for Computing (SNIC) at the High Performance Computing Centre North (HPC2N), partially funded by the Swedish Research Council through grant agreement no. 2018-05973. DATA AVAILABILITY The Ptarmigan source code can be obtained from its Github repository 72. The version used in this work (v1.3.1) is archived at https://doi.org/10.5281/zenodo.7957000 (Ref. 73). 1A. Di Piazza, C. M¨ uller, K. Z. Hatsagortsyan, and C. H. Keitel, “Extremely high-intensity laser interactions with fundamental quantum systems,” Rev. Mod. Phys. 84, 1177–1228 (2012), arXiv:1111.3886 [hep-ph]. 2A. Gonoskov, T. G. Blackburn, M. Marklund, and S. S. Bulanov, “Charged particle motion and radiation in strong electromagnetic fields,” Rev. Mod. Phys.94, 045001 (2022), arXiv:2107.02161 [physics.plasm-ph]. 3A. Fedotov, A. Ilderton, F. Karbstein, B. King, D. Seipt, H. Taya, and G. Torgrimsson, “Advances in QED with intense background fields,” Phys. Rep. 1010, 1–138 (2023), arXiv:2203.00019 [hep-ph]. 4C. Bula, K. T. McDonald, E. J. Prebys, C. Bamber, S. Boege, T. Kotseroglou, A. C. Melissinos, D. D. Meyerhofer, W. Ragg, D. L. Burke, R. C. Field, G. Horton-Smith, A. C. Odian, J. E. Spencer, D. Walz, S. C. Berridge, W. M. Bugg, K. Shmakov, and A. W. Weidemann, “Observation of nonlinear effects in Compton scattering,” Phys. Rev. Lett.76, 3116–3119 (1996). 5D. L. Burke, R. C. Field, G. Horton-Smith, J. E. Spencer, D. Walz, S. C. Berridge, W. M. Bugg, K. Shmakov, A. W. Weidemann, C. Bula, K. T. McDonald, E. J. Prebys, C. Bamber, S. J. Boege, T. Koffas, T. Kotseroglou, A. C. Melissinos, D. D. Meyerhofer, D. A. Reis, and W. Ragg, “Positron production in multiphoton light-by-light scattering,” Phys. Rev. Lett.79, 1626–1629 (1997). 6S. Meuren, “Probing strong-field QED at FACET-II (SLAC E-320),” (2019), ExHILP talk. 7H. Abramowicz, U. H. Acosta, M. Altarelli, R. Assmann, Z. Bai, T. Behnke, Y. Benhammou, T. Blackburn, S. Boogert, O. Borysov,et al., “Conceptual design report for the LUXE experiment,” Eur. Phys. J. Spec. Top.230, 2445–2560 (2021), arXiv:2102.02032 [hep-ex]. 8H. Abramowicz, M. A. Soto, M. Altarelli, R. Aßmann, A. Athanassiadis, G. Avoni, T. Behnke, M. Benettoni, Y. Benhammou, J. Bhatt, et al., “Technical Design Report for the LUXE Experiment,” (2023), arXiv:2308.00515 [hep-ex]. 9B. King, N. Elkina, and H. Ruhl, “Photon polarization in electron-seeded pair-creation cascades,” Phys. Rev. A87, 042117 (2013), arXiv:1301.7001 [hep-ph].15 10D. Seipt, C. P. Ridgers, D. D. Sorbo, and A. G. R. Thomas, “Polarized QED cascades,” New J. Phys.23, 053025 (2021), arXiv:2010.04078 [hep-ph]. 11Y.-N. Dai, B.-F. Shen, J.-X. Li, R. Shaisultanov, K. Z. Hatsagortsyan, C. H. Keitel, and Y.-Y. Chen, “Photon polarization effects in polarized electron–positron pair production in a strong laser field,” Matter and Radiation at Extremes 7, 014401 (2021), arXiv:2107.04996 [physics.plasm-ph]. 12B. King and H. Ruhl, “Trident pair production in a constant crossed field,” Phys. Rev. D88, 013005 (2013), arXiv:1303.1356 [hep-ph]. 13S. Tang and B. King, “Locally monochromatic two-step nonlinear trident process in a plane wave,” Phys. Rev. D 107, 096004 (2023), arXiv:2211.13299 [hep-ph]. 14B. King, “Double Compton scattering in a constant crossed field,” Phys. Rev. A91, 033415 (2015), arXiv:1410.5478 [hep-ph]. 15D. Seipt, D. Del Sorbo, C. P. Ridgers, and A. G. R. Thomas, “Theory of radiative electron polarization in strong laser fields,” Phys. Rev. A98, 023417 (2018), arXiv:1805.02027 [hep-ph]. 16C. Bamber, S. J. Boege, T. Koffas, T. Kotseroglou, A. C. Melissinos, D. D. Meyerhofer, D. A. Reis, W. Ragg, C. Bula, K. T. McDonald, E. J. Prebys, D. L. Burke, R. C. Field, G. Horton-Smith, J. E. Spencer, D. Walz, S. C. Berridge, W. M. Bugg, K. Shmakov, and A. W. Weidemann, “Studies of nonlinear QED in collisions of 46.6 GeV electrons with intense laser pulses,” Phys. Rev. D60, 092004 (1999). 17P. Chen, G. Horton-Smith, T. Ohgaki, A. W. Weidemann, and K. Yokoya, “CAIN: Conglom ´erat d’ ABEL et d’Interactions Non-lin´eaires,” Nucl. Instrum. Methods Phys. Res. A 355, 107–110 (1995). 18A. Hartin, “Strong field QED in lepton colliders and electron/laser interactions,” Int. J. Mod. Phys. A 33, 1830011 (2018), arXiv:1804.02934 [hep-ph]. 19T. Heinzl, B. King, and A. J. MacLeod, “Locally monochromatic approximation to QED in intense laser fields,” Phys. Rev. A 102, 063110 (2020), arXiv:2004.13035 [hep-ph]. 20G. Torgrimsson, “Loops and polarization in strong-field QED,” New J. Phys.23, 065001 (2021), arXiv:2012.12701 [hep-ph]. 21T. G. Blackburn, A. J. Macleod, and B. King, “From local to nonlocal: higher fidelity simulations of photon emission in intense laser pulses,” New J. Phys.23, 085008 (2021), arXiv:2103.06673 [hep-ph]. 22T. G. Blackburn and B. King, “Higher fidelity simulations of nonlinear Breit–Wheeler pair creation in intense laser pulses,” Eur. Phys. J. C 82, 44 (2022), arXiv:2108.10883 [hep-ph]. 23O. Har-Shemesh and A. Di Piazza, “Peak intensity measurement of relativistic lasers via nonlinear Thomson scattering,” Opt. Lett. 37, 1352 (2012), arXiv:1111.6002 [physics.optics]. 24D. Seipt and B. K ¨ampfer, “Asymmetries of azimuthal photon distributions in nonlinear Compton scattering in ultrashort intense laser pulses,” Phys. Rev. A 88, 012127 (2013), arXiv:1305.3837. 25R. H. Milburn, “Electron scattering by an intense polarized photon field,” Phys. Rev. Lett.10, 75–77 (1963). 26F. R. Arutyunian and V. A. Tumanian, “The Compton effect on relativistic electrons and the possibility of obtaining high energy beams,” Phys. Lett.4, 176–178 (1963). 27D. Y. Ivanov, G. L. Kotkin, and V. G. Serbo, “Complete description of polarization effects in emission of a photon by an electron in the field of a strong laser wave,” Eur. Phys. J. C36, 127–145 (2004), arXiv:hep-ph/0402139. 28B. King and S. Tang, “Nonlinear Compton scattering of polarized photons in plane-wave backgrounds,” Phys. Rev. A102, 022809 (2020), arXiv:2003.01749 [hep-ph]. 29S. Tang, B. King, and H. Hu, “Highly polarised gamma photons from electron-laser collisions,” Phys. Lett. B809, 135701 (2020), arXiv:2003.03246 [hep-ph]. 30In this section we assume that 𝑎rms is a constant. In a simulation framework based on the locally monochromatic approximation, it becomes a slowly varying function of space and time, 𝑎rms (𝑋). 31D. Del Sorbo, D. Seipt, T. G. Blackburn, A. G. R. Thomas, C. D. Murphy, J. G. Kirk, and C. P. Ridgers, “Spin polarization of electrons by ultraintense lasers,” Phys. Rev. A96, 043407 (2017), arXiv:1702.03203 [physics.plasm-ph]. 32D. Seipt, D. Del Sorbo, C. P. Ridgers, and A. G. R. Thomas, “Ultrafast polarization of an electron beam in an intense bichromatic laser field,” Phys. Rev. A 100, 061402 (2019), arXiv:1904.12037 [physics.plasm-ph]. 33Y.-F. Li, R. Shaisultanov, K. Z. Hatsagortsyan, F. Wan, C. H. Keitel, and J.-X. Li, “Ultrarelativistic electron-beam polarization in single-shot interaction with an ultraintense laser pulse,” Phys. Rev. Lett.122, 154801 (2019), arXiv:1812.07229 [physics.plasm-ph]. 34These are equivalent to the manifestly covariant basis vectors 𝜀1,2 = 𝜖1,2 −𝛿1,2 𝑘, where 𝛿1,2 = (𝜖1,2 ·𝑘′)/(𝑘 ·𝑘′), 𝜖 𝜇 1 = (0, 1, 0, 0), 𝜖 𝜇 2 = (0, 0, 1, 0) and 𝑘𝜇 = 𝜔(1, 0, 0, 1)28, which are orthonormal in the four-dimensional sense: 𝜀1 ·𝜀2 =0, 𝜀1,2 ·𝑘′=0 and 𝜀𝑗 ·𝜀𝑗 =−1 for 𝑗 ∈{1, 2}. We make these orthonormal in the three-dimensional sense, at the expense of explicit covariance, by transforming 𝜀1,2 →𝜀1,2 +𝛿1,2 𝜔𝑘′/𝜔′. 35A. I. Nikishov and V. I. Ritus, “Quantum processes in the field of a plane electromagnetic wave and in a constant field. I,” Soviet Physics JETP19, 529 (1964). 36D. Li, K. Yokoya, T. Hirose, and R. Hamatsu, “Transition probability and polarization of final photons in nonlinear Compton scattering for linearly polarized laser,” Jpn. J. Appl. Phys.42, 5376–5382 (2003). 37S. Tang, “Fully polarized nonlinear Breit-Wheeler pair production in pulsed plane waves,” Phys. Rev. D105, 056018 (2022), arXiv:2203.05721 [hep-ph]. 38E. L¨otstedt and U. D. Jentschura, “Recursive algorithm for arrays of generalized Bessel functions: Numerical access to Dirac-Volkov solutions,” Phys. Rev. E 79, 026707 (2009), arXiv:0902.1099 [physics.comp-ph]. 39B. Quesnel and P. Mora, “Theory and simulation of the interaction of ultraintense laser pulses with electrons in vacuum,” Phys. Rev. E58, 3719–3732 (1998). 40Ptarmigan uses proper time, rather than phase, to parameterize the trajectory of a particle. For context, the proper time is related to phase 𝜑 by the energy parameter: 𝑑𝜑 𝑑𝜏 =𝑚𝜂𝑒. 41Y. F. Li, R. Shaisultanov, Y. Y. Chen, F. Wan, K. Z. Hatsagortsyan, C. H. Keitel, and J. X. Li, “Polarized Ultrashort Brilliant Multi-GeV𝛾 Rays via Single-Shot Laser-Electron Interaction,” Phys. Rev. Lett.124, 14801 (2020), arXiv:1907.08877 [physics.plasm-ph]. 42Equivalent logic applies to electron polarization, which changes due to the emission of radiation in a spin-dependent way.? . 43L. D. Landau and E. M. Lifshitz, The Classical Theory of Fields, The Course of Theoretical Physics, Vol. 2 (Butterworth-Heinemann, Oxford, 1987). 44P. W. Smorenburg, L. P. J. Kamp, G. A. Geloni, and O. J. Luiten, “Coherently enhanced radiation reaction effects in laser-vacuum acceleration of electron bunches,” Laser Part. Beams 28, 553–562 (2010), arXiv:1004.0499 [physics.acc-ph]. 45A. Di Piazza, M. Tamburini, S. Meuren, and C. H. Keitel, “Implementing nonlinear Compton scattering beyond the local constant field approximation,” Phys. Rev. A 98, 012134 (2018), arXiv:1708.08276 [hep-ph]. 46A. Ilderton, B. King, and D. Seipt, “Extended locally constant field approximation for nonlinear Compton scattering,” Phys. Rev. A 99, 042121 (2019), arXiv:1808.10339 [hep-ph]. 47A. Di Piazza, M. Tamburini, S. Meuren, and C. H. Keitel, “Improved local-constant-field approximation for strong-field QED codes,” Phys. Rev. A99, 022125 (2019), arXiv:1811.05834 [hep-ph]. 48D. Seipt and B. King, “Spin- and polarization-dependent locally-constant-field-approximation rates for nonlinear Compton and Breit-Wheeler processes,” Phys. Rev. A 102, 052805 (2020), arXiv:2007.11837 [physics.plasm-ph]. 49C. P. Ridgers, J. G. Kirk, R. Duclous, T. G. Blackburn, C. S. Brady, K. Bennett, T. D. Arber, and A. R. Bell, “Modelling gamma-ray photon emission and pair production in high-intensity laser-matter interactions,” J. Comput. Phys.260, 273–285 (2014), arXiv:1311.5551 [physics.plasm-ph].16 50A. Gonoskov, S. Bastrakov, E. Efimenko, A. Ilderton, M. Marklund, I. Meyerov, A. Muraviev, A. Sergeev, I. Surmin, and E. Wallin, “Extended particle-in-cell schemes for physics in ultrastrong laser fields: Review and developments,” Phys. Rev. E92, 023305 (2015), arXiv:1412.6426 [physics.plasm-ph]. 51Y. I. Salamin, “Fields of a Gaussian beam beyond the paraxial approximation,” Appl. Phys. B86, 319–326 (2007). 52R. Duclous, J. G. Kirk, and A. R. Bell, “Monte Carlo calculations of pair production in high-intensity laser-plasma interactions,” Plasma Phys. Control. Fusion 53, 015009 (2011), arXiv:1010.4584 [hep-ph]. 53M. Tamburini, F. Pegoraro, A. Di Piazza, C. H. Keitel, and A. Macchi, “Radiation reaction effects on radiation pressure acceleration,” New J. Phys.12, 123005 (2010), arXiv:1008.1685 [physics.plasm-ph]. 54M. Vranic, J. L. Martins, R. A. Fonseca, and L. O. Silva, “Classical radiation reaction in particle-in-cell simulations,” Comput. Phys. Commun.204, 141–151 (2016), arXiv:1502.02432 [physics.plasm-ph]. 55T. G. Blackburn, C. P. Ridgers, J. G. Kirk, and A. R. Bell, “Quantum radiation reaction in laser-electron-beam collisions,” Phys. Rev. Lett.112, 015001 (2014), arXiv:1503.01009 [physics.plasm-ph]. 56Z. Gong, K. Z. Hatsagortsyan, and C. H. Keitel, “Retrieving transient magnetic fields of ultrarelativistic laser plasma via ejected electron polarization,” Phys. Rev. Lett. 127, 165002 (2021), arXiv:2103.12164 [physics.plasm-ph]. 57Q. Qian, D. Seipt, M. Vranic, T. E. Grismayer, T. G. Blackburn, C. P. Ridgers, and A. G. R. Thomas, “Parametric study of the polarization dependence of nonlinear Breit-Wheeler pair creation process using two laser pulses,” (2023), arXiv:2306.16706 [hep-ph]. 58H.-H. Song, W.-M. Wang, and Y.-T. Li, “Dense polarized positrons from laser-irradiated foil targets in the QED regime,” Phys. Rev. Lett.129, 035001 (2022), arXiv:2112.07451 [physics.plasm-ph]. 59A. Hartin, “Strong field QED in lepton colliders and electron/laser interactions,” Int. J. Mod. Phys. A 33, 1830011 (2018), arXiv:1804.02934 [hep-ph]. 60G. A. Krafft, B. Terzi´c, E. Johnson, and G. Wilson, “Scattered spectra from inverse Compton sources operating at high laser fields and high electron energies,” Phys. Rev. Accel. Beams26, 034401 (2023). 61A. G. R. Thomas, “Algorithm for calculating spectral intensity due to charged particles in arbitrary motion,” Phys. Rev. ST Accel. Beams13, 020702 (2010), arXiv:0906.0758 [physics.comp-ph]. 62A. G. R. Thomas, C. P. Ridgers, S. S. Bulanov, B. J. Griffin, and S. P. D. Mangles, “Strong radiation-damping effects in a gamma-ray source generated by the interaction of a high-intensity laser with a wakefield-accelerated electron beam,” Phys. Rev. X2, 041004 (2012), arXiv:1204.5259 [physics.acc-ph]. 63T. G. Blackburn, “Radiation reaction in electron-beam interactions with high-intensity lasers,” Rev. Mod. Plasma Phys. 4, 5 (2020), arXiv:1910.13377 [physics.plasm-ph]. 64B. King, “Interference effects in nonlinear Compton scattering due to pulse envelope,” Phys. Rev. D103, 036018 (2021), arXiv:2012.05920 [hep-ph]. 65S. Tang and B. King, “Pulse envelope effects in nonlinear Breit-Wheeler pair creation,” Phys. Rev. D104, 096019 (2021), arXiv:2109.00555 [physics.optics]. 66A. Di Piazza, “Exact solution of the Landau-Lifshitz equation in a plane wave,” Lett. Math. Phys.83, 305–313 (2008), arXiv:0801.1751 [physics.optics]. 67The LCFA spectrum, by contrast, contains an integrable singularity in this limit35: lim 𝑠→0 (𝑑𝑁𝛾/𝑑𝑠)∝ 𝑠−1/3. 68V. Dinu, T. Heinzl, and A. Ilderton, “Infrared divergences in plane wave backgrounds,” Phys. Rev. D86, 085037 (2012), arXiv:1206.3957 [hep-ph]. 69B. King, “Classical radiation reaction in red-shifted harmonics,” (2023), arXiv:2305.14429 [hep-ph]. 70A. Di Piazza, “Analytical infrared limit of nonlinear Thomson scattering including radiation reaction,” Phys. Lett. B 782, 559–565 (2018), arXiv:1804.01160 [physics.plasm-ph]. 71T. G. Blackburn, E. Gerstmayr, S. P. D. Mangles, and M. Marklund, “Model-independent inference of laser intensity,” Phys. Rev. Accel. Beams 23, 064001 (2020), arXiv:1911.02349 [physics.plasm-ph]. 72T. G. Blackburn, “Ptarmigan,” Github repository (2023). 73T. G. Blackburn, “Ptarmigan: Version 1.3.1,” Zenodo (2023). 74V. N. Baier, V. M. Katkov, and V. M. Strakhovenko, Electromagnetic Processes at High Energies in Oriented Single Crystals (World Scientific, Singapore, 1998). Appendix A: Circularly polarized plane waves The laser polarization is defined by the Stokes parameter 𝑆laser 3 = ±1. 𝑆laser 3 = 1 denotes right-circular polarization, which means that the electric field rotates anticlockwise around the direction of propagation, or equivalently that the laser photons have positive helicity (right handedness). 𝑆laser 3 =−1 denotes left-circular polarization or that the laser photons have negative helicity (left handedness): this is the default setting for circularly polarized lasers in Ptarmigan. 1. Photon emission The single-differential emission rate per unit proper time, at a particular harmonic index 𝑛, in the LMA is given by19 d𝑊𝛾 𝑛 d𝑠 =−𝛼𝑚 \u001a 𝐽2 𝑛(𝑧)+ 𝑎2 rms 2 \u0014 1 + 𝑠2 2(1 −𝑠) \u0015 \u0002 2𝐽2 𝑛(𝑧)− 𝐽2 𝑛−1 (𝑧)− 𝐽2 𝑛+1 (𝑧) \u0003\u001b (A1) where the argument of the Bessel functions 𝑧and auxiliary variables are 𝑧2 = 4𝑛2𝑎2 rms 1 +𝑎2rms 𝑠 𝑠𝑛(1 −𝑠) \u0014 1 − 𝑠 𝑠𝑛(1 −𝑠) \u0015 , 𝑠 𝑛 = 2𝑛𝜂 1 +𝑎2rms (A2) and 0 < 𝑠 < 𝑠𝑛/(1 +𝑠𝑛). The azimuthal angle 𝜙is uniformly distributed in [0,2𝜋).17 The Stokes parameters of the emitted photon are given by27 S =©­ « −cos 2𝜙 −sin 2𝜙 0 sin 2𝜙 −cos 2𝜙 0 0 0 1 ª® ¬ ©­ « 𝑆′ 1 𝑆′ 2 𝑆′ 3 ª® ¬ (A3) where 𝑆′ 1 = 2 𝑆′ 0 \u001a\u0002 𝐽2 𝑛−1 (𝑧)+ 𝐽2 𝑛+1 (𝑧)−2𝐽2 𝑛(𝑧) \u0003 +4 \u0012 1 −𝑛2 𝑧2 + 1 2𝑎2rms \u0013 𝐽2 𝑛(𝑧) \u001b , (A4) 𝑆′ 2 =0, (A5) 𝑆′ 3 = 𝑆laser 3 𝑆′ 0 \u0012 1 −𝑠+ 1 1 −𝑠 \u0013 \u0014 1 − 2𝑠 𝑠𝑛(1 −𝑠) \u0015 \u0002 𝐽2 𝑛−1 (𝑧)− 𝐽2 𝑛+1 (𝑧) \u0003 (A6) and 𝑆′ 0 = \u0012 1 −𝑠+ 1 1 −𝑠 \u0013 \u0002 𝐽2 𝑛−1 (𝑧)+ 𝐽2 𝑛+1 (𝑧)−2𝐽2 𝑛(𝑧) \u0003 − 4 𝑎2rms 𝐽2 𝑛(𝑧). (A7) The rotation matrix ensures that the Stokes parameters are defined with respect to the basis given in eq. (1). Note that this is the only place that the azimuthal angle 𝜙appears explicitly. a. Classical limit. In the limit that 𝜂≪1 (for arbitrary 𝑎), the partial rates take the form d𝑊𝛾,cl 𝑛 d𝑣 = 𝛼𝑚𝑛𝜂 1 +𝑎2rms [𝑎2 rms𝐽2 𝑛−1 (𝑧)+𝑎2 rms𝐽2 𝑛+1 (𝑧)−2(1 +𝑎2 rms)𝐽2 𝑛(𝑧)], 𝑧 2 = 4𝑎2 rms𝑛2𝑣(1 −𝑣) 1 +𝑎2rms , (A8) where 0 ≤𝑣=𝑠/𝑠𝑛 ≤1. The Stokes parameters are obtained by replacing 1−𝑠+1/(1−𝑠)→ 2 and 1−2𝑠/[𝑠𝑛(1−𝑠)]→ 1−2𝑣. 2. Pair creation The double-differential pair-creation rate per unit time, 𝑊±, at a particular harmonic index 𝑛, is given by37 (see also Nikishov and Ritus 35 ) d2𝑊± 𝑛 d𝑠d𝜙 = 𝛼𝑚 2𝜋 \u001a 𝐽2 𝑛 +𝑎2 rms 4 𝑠2 +(1 −𝑠)2 𝑠(1 −𝑠) \u0010 𝐽2 𝑛−1 +𝐽2 𝑛+1 −2𝐽2 𝑛 \u0011 −(𝑆1 cos 2𝜙+𝑆2 sin 2𝜙) \u0014\u0012 2𝑛2𝑎2 rms 𝑧2 −1 −𝑎2 rms \u0013 𝐽2 𝑛 −𝑎2 rms 2 \u0010 𝐽2 𝑛−1 +𝐽2 𝑛+1 \u0011\u0015 −𝑆laser 3 𝑆3 \u0014𝑎2 rms 4 𝑠2 +(1 −𝑠)2 𝑠(1 −𝑠) \u0012 1 − 2 𝑠𝑛𝑠(1 −𝑠) \u0013 \u0010 𝐽2 𝑛−1 −𝐽2 𝑛+1 \u0011\u0015\u001b , (A9) where 𝑧2 = 4𝑛2𝑎2 rms 1 +𝑎2rms 1 𝑠𝑛𝑠(1 −𝑠) \u0014 1 − 1 𝑠𝑛𝑠(1 −𝑠) \u0015 , 𝑠 𝑛 = 2𝑛𝜂 1 +𝑎2rms , (A10) and 1 2 h 1 − √︁ 1 −4/𝑠𝑛 i < 𝑠 <1 2 h 1 + √︁ 1 −4/𝑠𝑛 i . (A11) The second term, which depends on 𝑆1 and 𝑆2, disappears on integration over the azimuthal angle 𝜙. Pair creation is likelier for photons with 𝑆3 =𝑆laser 3 , i.e. that have the same helicity as the background, than for photons with 𝑆3 =−𝑆laser 3 . The total rate is obtained by integrating eq. (A9) over all 𝑠and 𝜙, then summing over all harmonic orders 𝑛≥𝑛★ = ⌈2(1 +𝑎2 rms)/𝜂⌉. It depends on the normalised amplitude 𝑎rms, energy parameter 𝜂𝛾 and only the third Stokes parameter 𝑆3, i.e. 𝑊±=𝑊±(𝑎,𝜂𝛾,𝑆3). Appendix B: Constant crossed fields Rates calculated for constant, crossed fields form the basis of the locally constant, crossed fields approximation, which has become the standard method by which strong-field QED processes are included in numerical simulations49,50.18 1. Photon emission The double-differential emission rate per unit proper time is given by74 d2𝑊𝛾 d𝑢d𝜁 = 2𝛼𝑚 3 √ 3𝜋𝜒𝑒 𝑢 (1 +𝑢)3 n 𝜁2/3 [1 +(1 +𝑢)2]−( 1 +𝑢) o 𝐾1/3 \u00122𝑢𝜁 3𝜒𝑒 \u0013 , (B1) where 𝑢=𝜔′/(𝛾𝑚−𝜔′)and 𝜁 =[2𝛾2 (1 −|v|cos 𝜃)]3/2. Here 𝜃is the polar angle in thelab frame, measured with respect to the electron’s instantaneous velocity v, and 𝛾is the electron Lorentz factor. The domain of eq. (B1) is 0 ≤𝑢 <∞and 1 ≤𝜁 <∞; the azimuthal angle is uniformly distributed in [0,2𝜋). A useful approximation for ultrarelativistic particles is 𝜁 ≃(1 +𝛾2𝜃2)3/2 as 𝜃 ∼𝑂(1/𝛾). The polarization of the emitted photon is fixed with respect to the orthonormal basis e1 = a −(n ·a)n and e2 = a ×n, where a is the unit vector along the electron’s instantaneous accelerationE +n ×B and n is the unit vector along the photon’s 3-momentum. Let 𝛽 be the angle between n and the plane defined by the electron’s instantaneous velocity and acceleration: 𝛽=n ·(v ×a)∼ 𝑂(1/𝛾). Then the Stokes parameters of the emitted photon are74: 𝑆1 = 1 𝑆0 h 𝜇2𝐾2 2/3 (𝜈)− 𝛽2𝐾2 1/3 (𝜈) i 𝑆2 =0 𝑆3 = 2 𝑆0 \u0014 1 + 𝑢2 2(1 +𝑢) \u0015 𝛽𝜇𝐾1/3 (𝜈)𝐾2/3 (𝜈) (B2) where 𝑆0 =𝜇2𝐾2 2/3 (𝜈)+ 𝛽2𝐾2 1/3 (𝜈)+ 𝑢2 𝜇2 2(1 +𝑢) h 𝐾2 1/3 (𝜈)+𝐾2 2/3 (𝜈) i (B3) and 𝜇2 =𝛽2 +1/𝛾2 and 𝜈=𝑢𝛾3 𝜇3/(3𝜒𝑒). As 𝑆2 1 +𝑆2 2 +𝑆2 3 <1, the photon is only partially polarized. Integrating over all 𝜁, we obtain d𝑊𝛾 d𝑢 = 𝛼𝑚√ 3𝜋 1 (1 +𝑢)3 \u001a [1 +(1 +𝑢)2]𝐾2/3 (𝜉)−( 1 +𝑢) ∫ ∞ 𝜉 𝐾1/3 (𝑡)d𝑡 \u001b . (B4) where 𝜉 =2𝑢/(3𝜒𝑒). Finally we obtain the total rate by integrating over all 𝑢: 𝑊𝛾 = √ 3𝛼𝑚 2𝜋 𝜒𝑒H(𝜒𝑒), H(𝜒𝑒)= 2 9𝜒𝑒 ∫ ∞ 0 5𝑢2 +7𝑢+5 (1 +𝑢)3 𝐾2/3 \u0012 2𝑢 3𝜒𝑒 \u0013 𝑑𝑢 (B5) a. Classical limit. The classical limit of eq. (B1) is given by: d2𝑊𝛾,cl d𝑢d𝜁 = 2𝛼𝑚 3 √ 3𝜋𝜒𝑒 𝑢 \u0010 2𝜁2/3 −1 \u0011 𝐾1/3 \u00122𝑢𝜁 3𝜒𝑒 \u0013 . (B6) As there is no cutoff in photon frequency, 𝑢 = 𝜔′/(𝛾𝑚). The Stokes parameters are obtained by setting 𝑢2/(1 +𝑢)→ 0 in eqs. (B2) and (B3); as a result, 𝑆2 1 +𝑆2 2 +𝑆2 3 =1. The classical total rate is obtained by setting H(𝜒𝑒)=5𝜋/3 in eq. (B5). 2. Pair creation The pair creation probability rate is used to determine the positron’s energy (as a fraction 𝑓 of the photon energy 𝜔′), polar angle 𝜃 and azimuthal angle 𝜙 (as defined with respect to n, the unit vector along the photon’s 3-momentum). The rate is a function of the photon’s quantum parameter 𝜒𝛾 and its polarization. We define the latter in terms of the three Stokes parameters 𝑆1,2,3 and the orthonormal basis e1 =a −(n ·a)n, e2 =a ×n, where a is the unit vector along the instantaneous acceleration E +n ×B. The triple-differential rate per unit time (resolved in energy and polar and azimuthal angle) is given by d3𝑊± d 𝑓d𝜁d𝜙 = 𝛼𝑚 2 √ 3𝜋2 \u001a\u0014 1 +𝜁2/3 𝑓2 +(1 −𝑓)2 𝑓(1 −𝑓) \u0015 𝐾1/3 (𝛿𝜁) +𝑆1 h cos 2𝜙−𝜁2/3 (1 +cos 2𝜙) i 𝐾1/3 (𝛿𝜁) −𝑆2 sin 2𝜙(𝜁2/3 −1)𝐾1/3 (𝛿𝜁) +𝑆3 sin 𝜙 \u0014 𝜁1/3 (𝜁2/3 −1)𝑓2 +(1 −𝑓)2 𝑓(1 −𝑓) \u0015 𝐾2/3 (𝛿𝜁) \u001b , (B7)19 where 𝜁 = \u0002 2𝛾2 (1 −|v|cos 𝜃) \u00033/2 ≥1 is a transformed polar angle that depends on the positron Lorentz factor 𝛾and velocity v and the auxiliary variable 𝛿= 2 3𝜒𝛾 𝑓(1 −𝑓). (B8) The 𝑆2- and 𝑆3-dependent terms vanish on integration over the azimuthal angle74: d2𝑊± d 𝑓d𝜁 = 𝛼𝑚√ 3𝜋 𝛿 \u0014 1 +𝜁2/3 \u0012 𝑓 1 −𝑓 +1 −𝑓 𝑓 −𝑆1 \u0013\u0015 𝐾1/3 (𝛿𝜁). (B9) Photon-polarization dependence appears in the form of the Stokes parameter 𝑆1, which gives the degree of linear polarization with respect to e1 and e2. The sign indicates that pair creation is more probable for photons polarized perpendicular to the acceleration, 𝑆1 =−1, than for photons polarized parallel to the acceleration, 𝑆1 =1. As expected, the spectrum is symmetric around 𝑓 =1/2.",
      "references": [
        "Extremely high-intensity laser interactions with fundamental quantum systems,",
        "Charged particle motion and radiation in strong electromagnetic fields,",
        "Advances in QED with intense background fields,",
        "Observation of nonlinear effects in Compton scattering,",
        "Positron production in multiphoton light-by-light scattering,",
        "Probing strong-field QED at FACET-II (SLAC E-320),",
        "Conceptual design report for the LUXE experiment,",
        "Technical Design Report for the LUXE Experiment,",
        "Photon polarization in electron-seeded pair-creation cascades,",
        "Polarized QED cascades,",
        "Photon polarization effects in polarized electron–positron pair production in a strong laser field,",
        "Trident pair production in a constant crossed field,",
        "Locally monochromatic two-step nonlinear trident process in a plane wave,",
        "Double Compton scattering in a constant crossed field,",
        "Theory of radiative electron polarization in strong laser fields,",
        "Studies of nonlinear QED in collisions of 46.6 GeV electrons with intense laser pulses,",
        "CAIN: Conglom ´erat d’ ABEL et d’Interactions Non-lin´eaires,",
        "Strong field QED in lepton colliders and electron/laser interactions,",
        "Locally monochromatic approximation to QED in intense laser fields,",
        "Loops and polarization in strong-field QED,",
        "From local to nonlocal: higher fidelity simulations of photon emission in intense laser pulses,",
        "Higher fidelity simulations of nonlinear Breit–Wheeler pair creation in intense laser pulses,",
        "Peak intensity measurement of relativistic lasers via nonlinear Thomson scattering,",
        "Asymmetries of azimuthal photon distributions in nonlinear Compton scattering in ultrashort intense laser pulses,",
        "Electron scattering by an intense polarized photon field,",
        "The Compton effect on relativistic electrons and the possibility of obtaining high energy beams,",
        "Complete description of polarization effects in emission of a photon by an electron in the field of a strong laser wave,",
        "Nonlinear Compton scattering of polarized photons in plane-wave backgrounds,",
        "Highly polarised gamma photons from electron-laser collisions,",
        "Spin polarization of electrons by ultraintense lasers,",
        "Ultrafast polarization of an electron beam in an intense bichromatic laser field,",
        "Ultrarelativistic electron-beam polarization in single-shot interaction with an ultraintense laser pulse,",
        "Polarized Ultrashort Brilliant Multi-GeV𝛾 Rays via Single-Shot Laser-Electron Interaction,",
        "Quantum processes in the field of a plane electromagnetic wave and in a constant field. I,",
        "Transition probability and polarization of final photons in nonlinear Compton scattering for linearly polarized laser,",
        "Fully polarized nonlinear Breit-Wheeler pair production in pulsed plane waves,",
        "Recursive algorithm for arrays of generalized Bessel functions: Numerical access to Dirac-Volkov solutions,",
        "Theory and simulation of the interaction of ultraintense laser pulses with electrons in vacuum,",
        "Coherently enhanced radiation reaction effects in laser-vacuum acceleration of electron bunches,",
        "Implementing nonlinear Compton scattering beyond the local constant field approximation,",
        "Extended locally constant field approximation for nonlinear Compton scattering,",
        "Improved local-constant-field approximation for strong-field QED codes,",
        "Spin- and polarization-dependent locally-constant-field-approximation rates for nonlinear Compton and Breit-Wheeler processes,",
        "Modelling gamma-ray photon emission and pair production in high-intensity laser-matter interactions,",
        "Extended particle-in-cell schemes for physics in ultrastrong laser fields: Review and developments,",
        "Fields of a Gaussian beam beyond the paraxial approximation,",
        "Monte Carlo calculations of pair production in high-intensity laser-plasma interactions,",
        "Radiation reaction effects on radiation pressure acceleration,",
        "Classical radiation reaction in particle-in-cell simulations,",
        "Quantum radiation reaction in laser-electron-beam collisions,",
        "Retrieving transient magnetic fields of ultrarelativistic laser plasma via ejected electron polarization,",
        "Parametric study of the polarization dependence of nonlinear Breit-Wheeler pair creation process using two laser pulses,",
        "Dense polarized positrons from laser-irradiated foil targets in the QED regime,",
        "Scattered spectra from inverse Compton sources operating at high laser fields and high electron energies,",
        "Algorithm for calculating spectral intensity due to charged particles in arbitrary motion,",
        "Strong radiation-damping effects in a gamma-ray source generated by the interaction of a high-intensity laser with a wakefield-accelerated electron beam,",
        "Radiation reaction in electron-beam interactions with high-intensity lasers,",
        "Interference effects in nonlinear Compton scattering due to pulse envelope,",
        "Pulse envelope effects in nonlinear Breit-Wheeler pair creation,",
        "Exact solution of the Landau-Lifshitz equation in a plane wave,",
        "Infrared divergences in plane wave backgrounds,",
        "Classical radiation reaction in red-shifted harmonics,",
        "Analytical infrared limit of nonlinear Thomson scattering including radiation reaction,",
        "Model-independent inference of laser intensity,",
        "Ptarmigan,",
        "Ptarmigan: Version 1.3.1,"
      ],
      "meta_data": {
        "arxiv_id": "2305.13061v2",
        "doi": "10.1063/5.0159963",
        "authors": [
          "T. G. Blackburn",
          "B. King",
          "S. Tang"
        ],
        "published_date": "2023-05-22T14:20:41Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Addresses the need for percent-level, polarization- and interference-resolving simulations of strong-field QED in the transition regime (a0~1) where formation lengths are comparable to the laser wavelength and locally-constant-field (LCFA) models lose accuracy. Extends the open-source Monte Carlo particle-tracking code Ptarmigan to accurately simulate nonlinear Compton scattering (e→eγ), nonlinear Breit–Wheeler pair creation (γ→e+e−), and two-step trident (e→eγ→ee+e−) in linearly polarized laser pulses while tracking arbitrary γ-ray polarization via Stokes parameters. Demonstrates that an LMA (locally monochromatic approximation) implementation reproduces full QED plane-wave results to within a few percent across broad ranges of particle energies and laser intensities, and shows how intermediate-photon polarization reduces predicted trident positron yields (~16%).",
        "methodology": "Uses the locally monochromatic approximation (LMA): the background is treated as a slowly varying monochromatic wave characterized locally by cycle-averaged amplitude a_rms(X) and wavevector k. Charged particles are advanced on cycle-averaged classical trajectories (quasimomentum q and averaged position X) via leapfrog integration of dqμ/dτ=(m/2)∂μ a_rms^2 and dXμ/dτ=qμ/m. QED events are generated by Monte Carlo sampling: precompute/tabulate total and harmonic-resolved rates W(a_rms,η) from monochromatic plane-wave QED, sample harmonic index n via tabulated CDFs, then sample (s,φ) by rejection sampling of double-differential rates. Photon polarization is assigned and propagated using Stokes parameters (S1,S2,S3) in a process-dependent local basis; surviving-photon Stokes vectors are updated each step to account for polarization-dependent depletion during pair creation. Pair creation uses event biasing (rate multiplied by R↑, daughter weights reduced accordingly) to efficiently sample rare events. Implements efficient evaluation of double Bessel functions J_n(x,y) and derived A0,A1,A2 via a recurrence algorithm (Lötstedt–Jentschura). Provides alternative dynamics: (i) classical LMA with Landau–Lifshitz radiation reaction and nonlinear Thomson spectra; (ii) LCFA-based mode using Lorentz-force trajectories with constant-crossed-field rates and polarization basis tied to instantaneous acceleration; (iii) modified-classical models with Gaunt-factor corrections without stochastic recoil.",
        "experimental_setup": "Validation is via benchmarking against full QED calculations in 1D pulsed plane waves (first-principles S-matrix results) and against known classical results.\n• Nonlinear Compton benchmark: linearly polarized pulsed plane wave with vector potential A∝a0 sinφ g(φ), envelope g(φ)=cos^2[φ/(2N)], N=16 cycles; head-on electron with fixed η_e=0.1 (≈8.4 GeV for λ=0.8 μm); a0∈{0.5,2.5,10}. Compare polarization-resolved photon energy spectra dNγ/ds and angular distributions for E- vs B-polarized photons.\n• Nonlinear Breit–Wheeler benchmark: linearly polarized pulsed plane wave with Gaussian envelope g(φ)=exp[-φ^2/(4N^2)], N=16; photon with η_γ=0.2 (≈16.8 GeV for λ=0.8 μm); a0∈{0.5,1.0,2.5}. Compare single- and double-differential positron spectra; use large bias factors R↑ to resolve tiny probabilities.\n• Classical benchmark: circularly polarized pulsed plane wave (λ=0.8 μm, N=32, a0=2.5, η_e=0.4) with and without Landau–Lifshitz radiation reaction; compare s-weighted spectra against direct classical (Lienard–Wiechert) predictions and analytic edge/IR-limit formulas; also show a stress-test case (η_e=100, a0=1, N=4) where LMA+RR breaks down.\n• Two-step trident example/benchmark: simulate e→eγ→ee+e− yields for LP pulses (N=16, η_e≈0.197 ≈16.5 GeV, a0 from 0.75 to 3.0) and compare to dedicated LMA trident theory; ensembles of many simulated collisions and large primary-electron counts to converge rare high-energy tail contributions.",
        "limitations": "LMA accuracy requires a sufficiently plane-wave-like, slowly varying field: empirically N≳4 cycles and focal spot w0≳2λ; it neglects pulse-duration/envelope-scale interference that smooths Compton edges and fills subharmonic structure—effects are more pronounced for threshold processes like Breit–Wheeler. Computational cost of summing many harmonics limits practical LMA implementation to a0≲20 and introduces a finite harmonic cutoff n_max that can underestimate high-energy spectral tails (impacting secondary yields such as trident). Electron/positron spin is neglected (spin-averaged initial, spin-summed final), justified for symmetric multi-cycle plane-wave-like fields but not for symmetry-broken scenarios; photon dynamics include only propagation and decay (no other interactions). Classical LMA with radiation reaction assumes small energy loss per cycle (roughly a_rms^2 η_e≲30); it fails when radiative losses vary rapidly within a cycle (demonstrated by η_e=100, N=4). LCFA mode inherits LCFA validity constraints (a0≫1, a0^2/η≫1) and is inaccurate near/under the first Compton edge; its implementation conserves three-momentum at emission, producing O(1/γ^2) energy errors. Fields are prescribed (not self-consistent as in PIC), and high-order paraxial focusing model still requires w0≳2λ.",
        "future_research_directions": "Incorporate fermion spin dynamics and spin-resolved rates (including spin precession and spin-dependent radiation reaction) to address regimes with symmetry breaking (e.g., bichromatic or weakly elliptic pulses) and precision polarimetry. Extend beyond first-order/independent-emission physics to higher-order processes and correlations (double Compton, one-step trident contributions, interference between channels) and include vacuum-polarization/loop effects as suggested by the authors. Improve treatment of finite-pulse effects within LMA (envelope/inter-pulse interference, subharmonics) and reduce sensitivity to harmonic cutoffs (adaptive/analytic asymptotics for high-n tails) to better capture rare high-energy photons that dominate secondary yields. Broaden applicability to more complex/realistic fields: tighter focusing, nonparaxial and spatiotemporally structured pulses, multi-beam geometries, and potentially coupling to external field solvers for partially self-consistent scenarios. Develop/benchmark hybrid schemes that smoothly interpolate LMA↔LCFA and quantify uncertainties for experimental analyses (e.g., LUXE/FACET-II) using polarization-resolved observables.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Reward-Guided Prompt Evolving in Reinforcement Learning for LLMs",
      "full_text": "Elastic Analysis of Augmented Curves and Constrained Surfaces Esfandiar Nava-Yazdani[0000−0003−4895−739X] Zuse Institute Berlin, Berlin, Germany navayazdani@zib.de https://www.zib.de/members/navayazdani Abstract. The square root velocity transformation is crucial for efficiently em- ploying the elastic approach in functional and shape data analysis of curves. We study fundamental geometric properties of curves under this transformation. Moreover, utilizing natural geometric constructions, we employ the approach for intrinsic comparison within several classes of surfaces and augmented curves, which arise in the real world applications such as tubes, ruled surfaces spherical strips, protein molecules and hurricane tracks. Keywords: Elastic shape analysis · Tube · Manifold-valued · Ruled surface · Hurricane track. 1 Introduction Metric comparison of curves is a core task in a wide range of application areas such as morphology, image and shape analysis, computer vision, action recognition and signal processing. Thereby, a Riemannian structure is highly desirable, since it naturally provides powerful tools, beneficial for such applications. In the recent years, the use of Riemannian metrics for the study of sequential data, such as shapes of curves, trajectories given as longitudinal data or time series, has rapidly grown. In elastic analysis of curves, one considers deformations caused from both bending and stretching. A Riemannian metric, which quantifies the amount of those deformations is called elastic (cf. [18,19]). Therein, in contrast to landmark-based approaches (cf. [12,20,22]), one considers whole continuous curves instead of finite num- ber of curve-points. Consequently, the underlying spaces are infinite dimensional and computational cost becomes a significant issue. The square root velocity (SRV) frame- work provides a convenient and numerically efficient approach for analysing curves via elastic metrics and has been widely used in the recent years (cf. [14,11,4,3] and the comprehensive work [24]). In many applications the curves are naturally manifold-valued. For instance, Lie groups such as the Euclidean motion group, or more generally, symmetric spaces includ- ing the Grassmannian and the Hadamard-Cartan manifold of positive definite matrices are widely used in modelling of real world applications. Extensions of SRV framework from euclidean to general manifold-valued data can be found in [13,27,25,9,26]. arXiv:2402.04944v3  [math.DG]  28 Mar 20242 E. Nava-Yazdani Our contributions are the following. We expose for plane curves the behaviour of speed and curvature under the SRV transformation and geometric invariants. More- over, we apply the elastic approach to augmented curves, determining certain classes of surfaces, tubes, ruled surfaces and spherical strips, as well as hurricane tracks consid- ered with their intensities. We recall that with distance and geodesic at hand, signifi- cant ingredients of statistical analysis such as mean and principal geodesic components as well as approximation and modelling concepts such as splines can be computed. This paper is organized as follows. Section 2, presents the Riemannian setting and notations. Section 3 is devoted to applications. Therein, we consider time series, for which in addition to spatial data, auxiliary information give rise to augmented curves and some classes of surfaces generated by them. Thereby, we apply the elastic approach to both euclidean and spherical trajectories. Future prospects and concluding remarks are presented in 4. For the convenience of those readers primary interested in the applications, we mention that, advanced parts and details from differential geometry, presented in 2, can be skipped. Thereby, the essential point is the use of a framework (SRV) for computation of shortest paths on the spaces of curves and their shapes. 2 Riemannian Framework 2.1 Preliminaries For the background material on Riemannian geometry, we refer to [8] and [10]. Let (M, g) be a finite dimensional Riemannian manifold and M the Fr´ echet manifold of smooth immersed curves from D in M, where D denotes either the unit circle S1 or the unit interval I := [0, 1] for closed or open curves respectively. Moreover, we denote the group of orientation preserving diffeomorphisms on D by Diff +. The following reparametrization invariance is crucial for a Riemannian metric G on M: Gc◦φ(h ◦ φ, k◦ φ) = Gc(h, k), for any c ∈ M, h, k∈ TcM and φ ∈ Diff +. The above equivariance ensures that the induced distance function satisfies the following, which is often desirable in applica- tions: d(c0 ◦ φ, c1 ◦ φ) = d(c0, c1), for any two curves c0 and c1 in M. Similarly, denoting the isometry group of M by Isom(M) and the tangent map of F ∈ Isom(M) by T F, the invariance GF◦c(T F◦ h, TF◦ k) = Gc(h, k), ensures that d(F ◦ c0, F◦ c1) = d(c0, c1). With the above invariances, we can divide out the spaces Isom(M) and Diff +, and consider the natural induced distance dS on the quotient space S = M/(Diff + × Isom(M))Elastic Analysis of Augmented Curves and Constrained Surfaces 3 given by dS([c0], [c1]) = inf {d(c0, f◦ c1 ◦ φ) : φ ∈ Diff +, f∈ Isom(M)} = inf {d(f ◦ c0 ◦ φ, c1) : φ ∈ Diff +, f∈ Isom(M)}. In the context of shape analysis of curves, M and S are called the pre-shape and shape space, respectively. Note that the order of quotient operations does not matter, since the left action of Isom(M) and the right action of Diff + commute. M/Diff + is the space of unparametrized curves and its inherited distance reads inf {d(c0, c1 ◦ φ) : φ ∈ Diff +}. We remark that particular essential challenges are due to the fact that some basic concepts and results from finite dimensional differential geometry such as Hopf-Rinow theorem, do not carry over to the infinite dimensional case. Now, let ∇ be the Levi- Civita connection of M and denote the arc length parameter, speed and unit tangent of c by θ, ω and T respectively. Thus, we have ω = |˙c|, dθ = ωdt and T = ˙c ω , where dot stands for derivation with respect to the parameter t. Due to a remarkable result in [16] the geodesic distance induced by the simplest natural choice, the L2-metric GL2 c (h, k) = Z D gc(h, k) dθ, always vanishes. Consequently, some stronger Sobolev metrics have been considered in several works including [17,7,5]. They are given by Gc(h, k) = nX i=0 Z D aigc(∇i T h, ∇i T k) dθ, with a1 non-vanishing and all ai non-negative, distinguish the curves. We consider first order metrics with constant coefficients. We remark that the coefficients ai can be chosen such that the metric is scale invariant, which is a desired property for some applications in shape analysis. A family of certain weighted Sobolev-type metrics, the so-called elastic metrics, based on a decomposition of derivatives of the vector fields into normal and tangent components, has been introduced in [18,19]: Ga,b c (h, k) = Z D agc((∇T h)⊤, (∇T k)⊤) + bgc((∇T h)⊥, (∇T k)⊥) dθ, with 4b ≥ a >0. In this work, we use the square root velocity (SRV) framework, which allows for a convenient and computationally efficient elastic approach. The main tool in this framework is the square root velocity transformation, which for euclidean M reads q : c 7→ ˙cp |˙c| .4 E. Nava-Yazdani It isometrically maps curves modulo translations, with the metric G1,1/4 to M with the flat L2-metric given by G0(v, w) = Z D g(v(t), w(t))dt. This metric is frequently called (cf. [15,2,6]) flat, to emphasize its footpoint indepen- dence. Note that the elastic metric G1,1 corresponds to the first order Sobolev metric with a0 = 0 and a1 = 1. We remark, that for plane curves, the work [23] has extended the SRV transformation to general parameters a, b >0. For further reading on the SRV framework and applications in shape analysis, we refer to [14], [11] (numerical aspects), the survey [6] and particularly, the comprehensive work [24]. 2.2 Plane Curves A natural question that arises is, how essential geometric characteristics of a curve behave under the SRV transformation. In the following, we provide an answer for speed and curvature in the case of plane curves. Let M = R2, ˜c := q(c) and denote the curvature of c by κ. Note that ˜c does not need to be an immersion. Proposition 1. Denoting the speed of˜c by ˜ω, we have ˜ω = r ˙ω2 4ω + ω3κ2. (1) Moreover, ˜c is an immersion if and only ifκ and ˙ω have no common zeros. In this case, ˜κ˜ω = κω + ˙φ, (2) where ˜κ denotes the curvature of˜c and φ := arctan \u00122ω2κ ˙ω \u0013 . Proof. Let N denote the unit normal of c. With the shorthand notations α := √ω and β := α3κ, a straightforward application of the Frenet equations ˙T = ωκN and ˙N = −ωκT , yields ˙˜c = ˙αT + βN, ¨˜c = (¨α − β2 α )T + ( ˙β + ˙αβ α )N. Thus, we have ˜ω = p ˙α2 + β2, immediately implying (1). Obviously, zeros of ˜ω are common zeros of κ and ˙ω. Thus, ˜c is an immersion if and only if κ and ˙ω have no common zeros. In this case, ˜ κ and φ = arctan (β/ ˙α) = arctan \u0010 2ω2κ ˙ω \u0011 are well-defined and ˜κ˜ω3 = ˜ω2β/α + ˙α ˙β − ¨αβ, which immediately implies the curvature formula (2).Elastic Analysis of Augmented Curves and Constrained Surfaces 5 Next, we apply the proposition to study some geometric quantities, which are invariant under the SRV transformation. For closed curves, integrating the curvature formula above over D = S1 (note that in this case, ˜ω >0 almost everywhere), we see that the SRV transformation preserves the total curvature and particularly the turning number. Moreover, κω is preserved if and only if κ = a d dt \u00001 ω \u0001 with a constant a. Clearly, with κ and ω at hand, utilizing Frenet equations, we can compute c up to rigid motions. The following explicit solution is an immediate application of the above proposition. In light of the above proposition, immersed curves, which are mapped to straight lines, can easily be determined as follows. Example 1. Let a, b, Abe constants withab, A >0, ω(t) = A/ sin2(at+b) and κ = a/ω. A straightforward computation, utilizing the curvature formula (2), implies ˜κ = 0. 2.3 Curves in Homogeneous Spaces For the background material on Lie groups and homogeneous spaces, we refer to [10]. The works [13,27] provide extensions of the SRV framework for euclidean curves to the case of general manifolds. The former has high computational cost, while the latter, transported SRV, depends on a reference point and also suffers from distortion or bias caused by holonomy effects. We use the natural extension to homogeneous spaces exposed in [26,9]. For reader’s convenience, we sketch the core ingredients of the approach and refer to the mentioned works for details and some applications. Let M be a homogeneous space, i.e., M = H/K, where K is a closed Lie subgroup of a Lie Group H. Let ∥·∥ denote the induced norm by a left invariant metric on H, L the tangent map of the left translation, and Imm(D, H) the space of immersed curves from D to H. The SRV transformation is given by Q(α) = (α(0), q(α)), where q(α) = Lα−1 ˙αp ∥ ˙α∥ Here, α−1(t) denotes the inverse element of α(t) in H and H the Lie algebra of H. The map Q is a bijection from Imm(D, H) onto H × L2(D, H). Now, M can be equipped with the Riemannian metric given by the pullback of the product metric of H × L2(D, H) using the map Q and horizontal lifting. Let c1 and c2 be immersed curves in M with horizontal lifts α1 and α2 respectively. The induced distance on M reads d(c1, c2) = inf \u001aq d2 H(α1(0), α2(0)x) + ∥q(α1) − Adx−1 (q(α2)∥2 L2 : x ∈ K \u001b . 3 Applications Frequently, besides spatiotemporal data, represented by a curve γ in a manifold M, there are additional or auxiliary information associated with the curve, thus with the same time-correspondence. These can jointly with γ be comprised and represented as a6 E. Nava-Yazdani so-called augmented curve ˜γ in a higher dimensional manifold ˜M. In some applications, the curve ˜γ uniquely determines a submanifold N of M via a natural construction. An important example is provided, when ˜M is a submanifold of the tangent bundle of M, where the auxiliary information is represented as a vector field along γ and the con- struction is given by the Riemannian exponential map. Significant special cases occur, when M is R3 or the unit two-sphere S2 and N a surface. In the next two subsections, we consider certain classes of surfaces in R3, which often arise in applications and are determined by augmented curves in R4. In the last two subsections, we consider certain spherical regions as well as hurricane tracks together with their intensities. In both cases, we utilize the Riemannian distance from subsection 2.3 to S2 × R, which is a homogeneous space (recall that S2 can be identified with SO(3)/SO(2)). For our example applications, we present geodesic paths representing deformations, minimizing the elastic energy within the SRV framework. We remark, that in a Rie- mannian setting, distance and geodesics are essential Building blocks for many major issues in the morphology and shape analysis, such as computation of mean and test statistics as well as principal component or geodesic analysis. Moreover, besides sta- tistical analysis, also some methods for clustering and classification use Riemannian metrics and geodesics. For the code implementing our approach, which particularly includes Riemannian optimization for the computation of geodesic paths, we utilized our publicly available python package https://github.com/morphomatics, introduced in [1]. 3.1 Tubes A tube or canal surface c is a one-parameter family of circles, whose centers constitute a regular curve γ such that the circles are perpendicular to γ. More precisely, denoting the radii of the circles by r, c(s, .) = γ + r(N cos s + B sin s), 0 ≤ s ≤ 2π, where N and B are the normal and binormal of the curve γ = γ(t), t∈ D, resp. Due to the unique correspondence of c to (γ, r), comparison of tubes reduces to comparison of curves in R4. Figure 1 shows some examples of shortest paths of tubes. Real world applications include a variety of fields such as examination of vein, pipes, capsules and plant roots. Clearly, tubes include surfaces of revolution. 3.2 Ruled Surfaces A ruled surface is formed by moving a straight line segment (possibly with varying length) along a base curve. More precisely, let γ be a curve in R3 and v a unit vector field along γ. Then c(s, .) = γ + sv, s∈ I, parametrizes a ruled surface generated by ( γ, v). Figure 2 depicts an example, where each surface consists of straight line segments connecting the blue (for better visibility) curves γ and γ + v. The class of ruled surfaces includes many prominent surfacesElastic Analysis of Augmented Curves and Constrained Surfaces 7 Fig. 1.Two shortest paths of tubes such as cone, cylinder, helicoid (a minimal surface) and M¨ obius strip. They arise in manufacturing (construction by bending a flat sheet), cartography, architecture and biochemistry (secondary and tertiary structure of protein molecules). Fig. 2.Shortest path of ruled surfaces 3.3 Spherical Strips Let exp denote the exponential map of the unit two-sphere S2. We recall that for any non-zero tangent vector to S2 at a point x: expx(v) = cos(|v|)x + sin(|v|) v |v|8 E. Nava-Yazdani and expx(0) = x. Now, let γ be a curve in S2 with binormal B (cross product of γ and its unit tangent), and r a scalar function along γ. Then, the map c given by c(s, .) := expγ s(rB), s∈ I, parametrizes a spherical strip with bandwidth r. Figure 3 depicts an example of the shortest path between two spherical curves comprised with their bandwidth functions visualised as strips. Fig. 3.Shortest path of spherical strips 3.4 Hurricane Tracks Hurricanes belong to the most extreme natural phenomena and can cause major im- pacts regarding environment, economy, etc. Intensity of a hurricane is determined by the maximum sustained wind (maxwind), monotonically classifying the storms into categories (due to Saffir–Simpson wind scale; for instance, maxwind ≥ 137 knots cor- responds to category 5). Due to their major impacts on economy, human life and envi- ronment, as well as extreme variability and complexity, hurricanes have been studies in a large number of works. For our example, we used the HURDAT 2 database provided by the U.S. National Oceanic and Atmospheric Administration publicly available on https://www.nhc.noaa.gov/data/, supplying latitude, longitude, and maxwind on a 6 hours base of Atlantic hurricanes. Fig. 4.2010 Atlantic hurricane tracks (left) and the shortest path between two of them (right) with color-coded maximum sustained wind (in knots)Elastic Analysis of Augmented Curves and Constrained Surfaces 9 We represent the tracks as discrete trajectories in S2. For further details and com- parison with other approaches, we refer to [24,25] and the recent work [21]. The latter, also provides statistical analysis and a classification of hurricane tracks in terms of their intensities. Fig. 4 illustrates this data set with a visualization of the 2010 hurricane tracks and a shortest path, where the intensities, considered as auxiliary information, are color-marked. 4 Conclusion In this paper, we analysed the behaviour of speed and curvature under the square root velocity framework for elastic approach to plane curves. Moreover, we applied an extension of this framework to homogeneous Spaces, to metrically compare augmented curves and special surfaces, generated by those curves, using a natural construction via the Riemannian exponential map. Our approach, allows for computationally efficient determination of geodesic paths in the shape spaces of the respective classes of surfaces. Our example applications include tubes, ruled surfaces, spherical strips and hurricane tracks. Future work includes further real world applications, particularly concerning statistical analysis of longitudinal data such as comparison of group wise trends within a hierarchical model as well as classification and prediction. Acknowledgements This work was supported through the German Research Foun- dation (DFG) via individual funding (project ID 499571814). References 1. Ambellan, F., Hanik, M., von Tycowicz, C.: Morphomatics: Geometric morphomet- rics in non-Euclidean shape spaces (2021). https://doi.org/10.12752/8544, https:// morphomatics.github.io/ 2. Bauer, M., Bruveris, M., Marsland, S., Michor, P.: Constructing reparametrization in- variant metrics on spaces of plane curves. arXiv: Differential Geometry (2012), https: //arxiv.org/pdf/1207.5965.pdf 3. Bauer, M., Bruveris, M., Charon, N., Møller-Andersen, J.: A relaxed approach for curve matching with elastic metrics. ESAIM: Control, Optimisation and Calculus of Variations 25 (03 2018). https://doi.org/10.1051/cocv/2018053 4. Bauer, M., Bruveris, M., Harms, Philipp Michor, P.W.: Soliton solutions for the elastic metric on spaces of curves. Discrete & Continuous Dynamical Systems - A 38, 1161–1185 (2018). https://doi.org/10.3934/dcds.2018049 5. Bauer, M., Bruveris, M., Michor, P.W.: Overview of the geometries of shape spaces and diffeomorphism groups. Journal of Mathematical Imaging and Vision 50(1-2), 60–97 (2014) 6. Bauer, M., Charon, N., Klassen, E., Brigant, A.L.: Intrinsic riemannian metrics on spaces of curves: theory and computation. arXiv preprint (2020), https://arxiv.org/abs/ 2003.05590 7. Bauer, M., Harms, P., Michor, P.W., et al.: Sobolev metrics on the manifold of all rie- mannian metrics. Journal of Differential Geometry 94(2), 187–208 (2013)10 E. Nava-Yazdani 8. do Carmo, M.P.: Riemannian Geometry. Mathematics: Theory and Applications, Birkh¨ auser Boston, Cambridge, MA, USA, 2 edn. (1992) 9. Celledoni, E., Eidnes, S., Schmeding, A.: Shape analysis on homogeneous spaces: a gener- alised srvt framework. In: Computation and Combinatorics in Dynamics, Stochastics and Control: The Abel Symposium, Rosendal, Norway, August 2016. pp. 187–220. Springer (2018) 10. Gallot, S., Hullin, D., Lafontaine, J.: Riemannian Geometry. Universitext, Springer, Berlin, 3 edn. (2004) 11. Huang, W., Gallivan, K.A., Srivastava, A., Absil, P.A.: Riemannian optimization for registration of curves in elastic shape analysis. Journal of Mathematical Imaging and Vision 54(3), 320–343 (2016) 12. Kendall, D., Barden, D. Carne, T., Le, H.: Shape and Shape Theory. John Wiley & Sons (1999) 13. Le Brigant, A.: Computing distances and geodesics between manifold-valued curves in the srv framework. Journal of Geometric Mechanics 9(2) (2017) 14. Liu, W., Srivastava, A., Zhang, J.: Protein structure alignment using elastic shape anal- ysis. In: Proceedings of the First ACM International Conference on Bioinformatics and Computational Biology. pp. 62–70 (2010) 15. Michor, P., Mumford, D., Shah, J., Younes, L.: A metric on shape space with explicit geodesics. Atti Accad. Naz. Lincei Cl. Sci. Fis. Mat. Natur. Rend. Lincei (9) Mat. Appl. 19 (07 2007). https://doi.org/10.4171/RLM/506 16. Michor, P.W., Mumford, D.: Vanishing geodesic distance on spaces of submanifolds and diffeomorphisms. Documenta Mathematica 10, 217–245 (2005) 17. Michor, P.W., Mumford, D.: An overview of the riemannian metrics on spaces of curves using the hamiltonian approach. Applied and Computational Harmonic Analysis 23(1), 74–113 (2007) 18. Mio, W., Srivastava, A., Joshi, S.H.: On shape of plane elastic curves. International Journal of Computer Vision 73, 307–324 (2006) 19. Mio, W., Srivastava, A., Joshi, S.: On shape of plane elastic curves. International Journal of Computer Vision 73, 307–324 (07 2007). https://doi.org/10.1007/s11263-006-9968-0 20. Nava-XYazdani, E., Hege, H.C., Sullivan, T.J., von Tycowicz, C.: Geodesic analysis in kendall’s shape space with epidemiological applications. Journal of Mathematical Imaging and Vision pp. 1–11 (2020). https://doi.org/10.1007/s10851-020-00945-w 21. Nava-Yazdani, E., Ambellan, F., Hanik, M., von Tycowicz, C.: Sasaki metric for spline models of manifold-valued trajectories. Computer Aided Geometric Design 104, 102220 (2023). https://doi.org/10.1016/j.cagd.2023.102220 22. Nava-Yazdani, E., Hege, H.C., von Tycowicz, C.: A hierarchical geodesic model for longi- tudinal analysis on manif olds. Journal of Mathematical Imaging and Vision 64(4), 395 – 407 (2022). https://doi.org/10.1007/s10851-022-01079-x 23. Needham, T., Kurtek, S.: Simplifying transforms for general elastic metrics on the space of plane curves. SIAM Journal on Imaging Sciences 13(1), 445–473 (2020). https://doi.org/10.1137/19M1265132 24. Srivastava, A., Klassen, E.P.: Functional and shape data analysis, vol. 1. Springer (2016) 25. Su, Z., Klassen, E., Bauer, M.: The square root velocity framework for curves in a ho- mogeneous space. 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW) pp. 680–689 (2017) 26. Su, Z., Klassen, E., Bauer, M.: Comparing curves in homogeneous spaces. Differential Geometry and its Applications 60, 9–32 (2018) 27. Zhang, Z., Su, J., Klassen, E., Le, H., Srivastava, A.: Rate-invariant analysis of covariance trajectories. Journal of Mathematical Imaging and Vision 60, 1306–1323 (2018)",
      "references": [
        "Morphomatics: Geometric morphomet- rics in non-Euclidean shape spaces",
        "Constructing reparametrization in- variant metrics on spaces of plane curves.",
        "A relaxed approach for curve matching with elastic metrics.",
        "Soliton solutions for the elastic metric on spaces of curves.",
        "Overview of the geometries of shape spaces and diffeomorphism groups.",
        "Intrinsic riemannian metrics on spaces of curves: theory and computation.",
        "Sobolev metrics on the manifold of all rie- mannian metrics.",
        "Riemannian Geometry.",
        "Shape analysis on homogeneous spaces: a gener- alised srvt framework.",
        "Riemannian optimization for registration of curves in elastic shape analysis.",
        "Shape and Shape Theory.",
        "Computing distances and geodesics between manifold-valued curves in the srv framework.",
        "Protein structure alignment using elastic shape anal- ysis.",
        "A metric on shape space with explicit geodesics.",
        "Vanishing geodesic distance on spaces of submanifolds and diffeomorphisms.",
        "An overview of the riemannian metrics on spaces of curves using the hamiltonian approach.",
        "On shape of plane elastic curves.",
        "Geodesic analysis in kendall’s shape space with epidemiological applications.",
        "Sasaki metric for spline models of manifold-valued trajectories.",
        "A hierarchical geodesic model for longi- tudinal analysis on manif olds.",
        "Simplifying transforms for general elastic metrics on the space of plane curves.",
        "Functional and shape data analysis, vol. 1.",
        "The square root velocity framework for curves in a ho- mogeneous space.",
        "Comparing curves in homogeneous spaces.",
        "Rate-invariant analysis of covariance trajectories."
      ],
      "meta_data": {
        "arxiv_id": "2402.04944v3",
        "authors": [
          "Esfandiar Nava-Yazdani"
        ],
        "published_date": "2024-02-07T15:25:20Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Studies geometric effects of the square root velocity (SRV) transform for elastic curve analysis, deriving explicit formulas for how speed and curvature of plane curves change under SRV and identifying SRV-invariant quantities (e.g., total curvature/turning number for closed curves). Extends and applies SRV-based elastic shape analysis to manifold-valued/augmented curves that parameterize constrained surface classes (tubes/canal surfaces, ruled surfaces, spherical strips) and to real trajectories with auxiliary attributes (hurricane tracks with intensity), enabling intrinsic distances and geodesics in the corresponding (pre-)shape/shape spaces.",
        "methodology": "Riemannian elastic shape framework on spaces of immersed curves with reparametrization and isometry invariance; uses elastic Sobolev-type metrics Ga,b and especially the SRV transform q(c)=ċ/|ċ|^{1/2} which is an isometry (mod translations) between (a,b)=(1,1/4) elastic metric and an L2 metric on transformed representations. For plane curves, uses Frenet–Serret formulas to compute SRV-induced speed/curvature relations and invariants. For manifold-valued curves in homogeneous spaces M=H/K, uses the generalized SRV transform via left translation to the Lie algebra, horizontal lifts, and an optimization over the isotropy subgroup K to compute distances. Models surfaces via augmented curves: tubes by (centerline γ, radius r)∈R^3×R; ruled surfaces by (γ, direction field v)∈R^3×S^2; spherical strips by (γ on S^2, bandwidth r) using the sphere exponential map. Computes geodesic paths with Riemannian optimization (implemented with the Morphomatics Python package).",
        "experimental_setup": "Demonstrations on synthetic/illustrative examples of geodesic deformations between shapes within each class: (i) tubes in R^3 represented as curves in R^4 (γ,r), (ii) ruled surfaces represented by (γ,v), (iii) spherical strips represented by a spherical curve with a bandwidth function, and (iv) Atlantic hurricane tracks augmented with intensity (max sustained wind) represented as trajectories on S^2×R. Real data: NOAA HURDAT2 database (6-hourly latitude/longitude and maxwind); example visualization focuses on 2010 Atlantic tracks and a computed shortest path between two tracks with color-coded intensity. Validation is qualitative/visual (geodesic paths/shortest deformations); no quantitative benchmarks or statistical tests are reported.",
        "limitations": "Primarily methodological with limited quantitative evaluation; application results are illustrative (figures) without benchmark comparisons, error analysis, runtime/scalability studies, or statistical significance. SRV analysis of speed/curvature is derived for plane curves; extension to higher-dimensional Euclidean curves or general manifolds is not developed at the same level of geometric detail. Homogeneous-space SRV requires a homogeneous-space structure and relies on horizontal lifts and minimizing over K; practical computations depend on discretization and numerical optimization and may be sensitive to sampling, noise, and local minima. Surface models are restricted to specific parametrized families (tubes, ruled surfaces, spherical strips) and assume well-defined Frenet frames/unit tangents; SRV transform can yield non-immersions when curvature and speed derivative share zeros.",
        "future_research_directions": "Develop quantitative evaluations and benchmarking of distances/geodesics (accuracy, robustness to noise, computational complexity) across datasets and against alternative registration/trajectory methods. Extend geometric analysis of SRV effects (beyond plane curves) and incorporate more general elastic metrics (arbitrary a,b) and scale-invariant choices in manifold settings. Build full statistical pipelines on the resulting shape spaces: Fréchet means, principal geodesic analysis, hypothesis testing, clustering/classification, and predictive models (as suggested: hierarchical models for group-wise longitudinal trends, classification and prediction of hurricane tracks with intensity). Generalize augmented-curve constructions to broader surface/field-constrained submanifolds (e.g., vector-field-augmented trajectories in tangent bundles using Sasaki-type metrics) and handle imperfect data with smoothing/splines and uncertainty quantification.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "StablePrompt : Automatic Prompt Tuning using Reinforcement Learning for Large Language Model",
      "full_text": "Explainable AI improves task performance in human-AI collaboration Julian Senoner1∗, Simon Schallmoser2,3∗, Bernhard Kratzwald1, Stefan Feuerriegel2,3, Torbjørn Netland1† 1ETH Zurich, Zurich, Switzerland 2LMU Munich, Munich, Germany 3Munich Center for Machine Learning (MCML), Munich, Germany ∗Contributed equally †Correspondence: Torbjørn Netland (tnetland@ethz.ch) 1 arXiv:2406.08271v1  [cs.HC]  12 Jun 2024Abstract Artificial intelligence (AI) provides considerable opportunities to assist human work. How- ever, one crucial challenge of human-AI collaboration is that many AI algorithms operate in a black-box manner where the way how the AI makes predictions remains opaque. This makes it difficult for humans to validate a prediction made by AI against their own domain knowledge. For this reason, we hypothesize that augmenting humans with explainable AI as a decision aid improves task performance in human-AI collaboration. To test this hypothe- sis, we analyze the effect of augmenting domain experts with explainable AI in the form of visual heatmaps. We then compare participants that were either supported by (a) black-box AI or (b) explainable AI, where the latter supports them to follow AI predictions when the AI is accurate or overrule the AI when the AI predictions are wrong. We conducted two preregistered experiments with representative, real-world visual inspection tasks from man- ufacturing and medicine. The first experiment was conducted with factory workers from an electronics factory, who performed N = 9, 600 assessments of whether electronic prod- ucts have defects. The second experiment was conducted with radiologists, who performed N = 5, 650 assessments of chest X-ray images to identify lung lesions. The results of our experiments with domain experts performing real-world tasks show that task performance improves when participants are supported by explainable AI instead of black-box AI. For example, in the manufacturing setting, we find that augmenting participants with explain- able AI (as opposed to black-box AI) leads to a five-fold decrease in the median error rate of human decisions, which gives a significant improvement in task performance. Keywords: Explainable AI, task performance, decision-making, human-centered AI, human-AI col- laboration 2Introduction Artificial intelligence (AI) provides considerable opportunities to assist human work in various domains [1, 2]. For example, in manufacturing, AI is widely used to support humans when inspecting the quality of produced products to identify defects [3]. Similarly, in medicine, disease diagnosis now makes increasing use of AI systems. For instance, a recent survey found that AI is used by about 15% of radiologists at least weekly [4]. More broadly, an analysis showed that about 30% of all jobs in the United States are at high exposure to be assisted by AI [5]. Hence, the importance of human-AI collaboration is expected to grow in the near future. However, many questions regarding the effective design of human-AI collaborations remain open. One particular challenge in the use of AI for human work is that state-of-the-art AI algorithms, which frequently involve millions of trainable parameters [6, 7], operate as “black- box” algorithms. The term “black-box” refers to the opacity of these systems, meaning that the internal workings and decision-making processes of these algorithms are not transparent or easily understandable by humans [8]. This can have crucial implications in practice as the lack of transparency makes it difficult – or even impossible – for humans to validate the predictions made by an AI against their domain knowledge. Hence, without being able to assess whether a prediction generated by an AI is accurate, humans will not be able to correct predictions of the AI, because of which the unique expertise of workers is essentially lost, which will make the collaboration between domain experts and AI largely ineffective. Increasing efforts have been made to overcome the black-box nature of AI by developing methods that generate explanations for how AI algorithms reach their decisions [9, 10, 11, 12, 13, 14, 15]. Explainable AI refers to a set of methods that support humans in understanding how AI algorithms map certain inputs (e.g., lung X-rays, patient characteristics) to certain outputs (e.g., probability estimates for pneumonia) [16, 17]. Explainable AI can be broadly categorized into inherently interpretable models and post-hoc explanation techniques (see Supplement A for an extended literature review on explainable AI). For inherently interpretable algorithms, the decision-making of the algorithm can be inspected by humans, e.g., by inspecting the coefficients in linear regression or the splitting rules in decision trees [18]. Post-hoc explanation techniques are required when the inner workings of an AI algorithm become too complex to be understood by humans such as in deep neural networks. For example, one approach is to approximate the 3behavior of a black-box AI with a simpler model (e.g., a linear model) that can be interpreted [19]. Other methods rely on game theory to estimate the contribution of each model input to the model output while considering possible interaction effects [20]. Common methods for explaining AI algorithms in computer vision include the use of heatmaps. These heatmaps visually highlight the areas that are most relevant to the predictions made by the AI [21, 22]. Such explanation techniques are commonly used by AI engineers in the development of AI algorithms. Hence, this literature stream is orthogonal to the use of explainable AI in our work, where we use post-hoc explanation techniques to improve decisions by domain experts in real-world job tasks. Several works have studied behavioral dimensions of human-AI collaboration. For example, it has been examined whether humans are willing to delegate work to AI [23, 24, 25]. Another common dimension is algorithm aversion, where humans are averse to following decisions by algorithms and instead rely on their own (mis)judgment [26, 27, 28, 29, 30]. An antecedent to algorithm aversion is trust in AI , critically influencing whether humans adopt or reject AI recommendations [31, 32, 33]. Oppositely to algorithm aversion, overreliance is also a problem negatively impacting the effectiveness of human-AI collaboration [34, 35, 36]. That is, humans risk following AI predictions blindly without attentively performing the task. While all these dimensions are interesting from a behavioral perspective, the main outcome of interest for busi- ness and healthcare organizations is task performance. However, the impact of explainable AI on task performance in human-AI collaboration in real-world job tasks remains unclear. We hypothesize that augmenting domain experts with explainable AI, as opposed to black- box AI, improves task performance in human-AI collaboration. Specifically, we treat explainable AI as a form of decision aid that supports domain experts in better understanding algorithmic decisions. Experts can then compare the explanations to their domain knowledge, thereby val- idating whether the AI is correct or overwriting the AI if is not correct. Here, explainable AI does not provide more information from an AI perspective (i.e., identical predictive perfor- mance). However, for domain experts, it gives rich additional information by making the AI predictions more accessible. Thus, we expect that domain experts supported by explainable AI will outperform those supported by black-box AI in two ways: (1) they are more likely to follow AI predictions when they are accurate, and (2) they are more likely to overrule AI predictions when they were wrong. Previous research has studied the effect of explainable AI on task performance in human-AI 4collaboration (see Supplement A for a detailed overview), yet with key limitations. In particular, existing works are typically restricted by either (i) recruiting laypeople or (ii) overly simplified tasks that are not representative of real job tasks [37, 38, 39, 40]. However, a realistic estimate of the effect of explainable AI on task performance requires a real-world task performed by domain experts. Such works that actually study real-world tasks with domain experts are on the other hand restricted by (i) comparing explainable AI vs humans alone [41, 42], (ii) using no real explainable AI [43], or (iii) research designs that do not isolate the effect of explainable AI on task performance [44, 45, 46]. In contrast, the strength of our work is that we study the effect of explainable AI on task performance relative to black-box AI in human-AI collaborations with real-world tasks and actual domain experts. In this paper, we analyze the effect of augmenting domain experts with explainable AI on task performance in human-AI collaboration. For this, we conducted two preregistered experiments in which domain experts were asked to solve real-world visual inspection tasks in manufacturing (Study 1) and medicine (Study 2). We followed a between-subject design where we randomly assigned participants to two treatments: (a) black-box AI (i.e., where AI predictions are opaque) and (b) explainable AI (i.e., where AI predictions are explained). The latter thus offers not only the prediction from the AI but further shows explanations in the form of a visual heatmap as a decision aid. Heatmaps are frequently used and are considered state-of-the-art with respect to their localization performance across various settings [47, 48, 49, 50]. Study 1 was conducted in a manufacturing setting, where participants had to identify quality defects in electronic products. For this, we specifically recruited actual factory workers performing N = 9, 600 assessments of electronic products at Siemens. Study 2 was conducted in a medical setting, where participants had to identify lung lesions on chest X-ray images. To that end, medical professionals, i.e., radiologists, were recruited and performed N = 5, 650 assessments of chest X-ray images. In both studies, participants performed better when being supported by explainable AI as a decision aid. The tasks of both experiments are representative of many real-world human-AI collabo- rations. The manufacturing task is an identical, one-to-one copy of a real-world job task at Siemens and, hence, highly representative of visual inspection tasks in manufacturing [51, 52]. Visual inspection tasks are standard in the manufacturing industry. Regardless of how much manufacturers have sought to build quality into products and processes, labor-intensive inspec- 5tion tasks still abound [53]. In healthcare, visual inspection tasks are common across many different subdisciplines such as dermatology, radiology, pathology, ophthalmology, and dentistry, among many others. As concrete examples, physicians have to inspect, for instance, skin lesions in dermatology, tissues in pathology, and lung lesions in radiology [54, 55, 50]. Hence, establish- ing whether physicians benefit from explainable AI in visual inspection tasks is highly relevant for setting correct disease diagnosis and subsequent treatment. Results To analyze the effect of explainable AI on task performance in human-AI collaboration, we con- ducted two randomized experiments across two different settings, i.e., in manufacturing (Study 1) and medicine (Study 2). In both experiments, participants had to perform a visual inspection task. In the manufacturing experiment, factory workers were asked to inspect electronic products and to identify defective products. In the medical experiment, radiologists were asked to decide whether lung lesions are visible in chest X-ray images. Participants were randomly assigned to one of two different treatments aiding them in the task: (a) black-box AI or (b) explainable AI (Figure 1). Participants with black-box AI received an opaque AI score as a decision aid. Participants with explainable AI received the same score and an additional decision aid: the ex- planation of the score in the form of a heatmap. The heatmap does not provide more information from an AI perspective (the score is identical) but allows users to verify the prediction made by the AI. However, heatmaps provide a clear and intuitive way of highlighting quality defects/lung lesions [56]. We hypothesized that explainable AI as a decision aid improves task performance of domain experts in human-AI collaboration. Details on both experiments are provided in the Methods section. 6A B Figure 1: Overview of the experiments for assessing the effect of explainable AI on task performance. (A) Experimental design of the manufacturing experiment where factory workers were asked to “approve” images of faultless products and to “reject” images of defective products through a computer interface. ( B) Experimental design of the medical experiment where radiologists were asked to decide whether lung lesions are visible in the chest X-ray image. In both experiments, participants were randomly assigned to one of the two treatments: (a) black-box AI or (b) explainable AI. 7Study 1: Manufacturing experiment The manufacturing experiment was conducted at a factory of Siemens, a global industrial con- glomerate particularly known for its consumer and industrial electronics products. The objective of the task was to identify quality defects in electronic products (e.g., missing components, wrong components, and faulty components) with high accuracy. The task is representative of a real- world inspection task at Siemens and is analogous to other inspection tasks in manufacturing [51, 52]. Factory workers from Siemens were asked to visually inspect 200 images using a com- puter interface and label them as faultless or defective. The inspection task had to be completed within 35 minutes, which corresponds to realistic field conditions. Workers were randomly assigned to two treatments where they were either supported by black-box AI or explainable AI (Figure 1A). In both treatment arms, workers received a reference image of a faultless product (note that this is common practice at Siemens). In addition, they were provided with AI predictions of the product quality given by a numerical “quality score” between 0 (most certainly defect) and 100 (most certainly faultless). Workers with explainable AI additionally received a tailored decision aid in the form of visual heatmaps that indicated the assumed location of potential quality defects. The quality scores in both treatment arms were identical, so that differences in task performance could only be attributed to the explanations (i.e., the heatmaps). The objective of the field experiment was to obtain accurate estimates of the treatment effect under real-world conditions. Hence, we ran the experiment with actual domain experts rather than laypeople. The results are based on the entire available workforce of one shift of factory workers from Siemens. The factory workers performed in total N = 9, 600 assessments of electronic products. We then analyzed the effect of explainable AI on task performance using the balanced accuracy and defect detection rate (i.e., proportion of correctly identified defective products among all actual defective products) based on the quality assessments in the visual inspection task. We found that workers supported by explainable AI achieved a better task performance than workers supported by black-box AI. Workers with black-box AI achieved a balanced accuracy with a mean of only 88.6%, whereas workers with explainable AI treatment achieved a balanced accuracy with a mean of 96.3% (Figure 2 A). We then estimated the treatment effect of explain- 8able AI by regressing the balanced accuracy on the treatment (black-box AI = 0, explainable AI = 1). The regression results show that the treatment effect of explainable AI is statistically significant and large ( β = 7.653, SE = 2.178, P = 0.001); that is, an improvement of 7.7 per- centage points. Compared to the black-box AI, the explainable AI leads to a five-fold decrease in the median error rate. Workers with explainable AI outperformed workers with black-box AI also with respect to the defect detection rate with a mean of 93.0% versus a mean of 82.0% (Figure 2 B). The regression results again confirm that the treatment effect of explainable AI is statistically significant and large (β = 11.014, SE = 3.680, P = 0.004). All regression results remain statistically significant when including relevant control variables (demographics, tenure, self-reported IT skills, and decision speed) in the regression model (see Supplement H.1). A detailed analysis of the workers’ assessments revealed that workers with explainable AI followed accurate predictions more often than workers with black-box AI (mean = 93 .5% for black-box AI, mean = 98.6% for explainable AI). In particular, workers supported by black-box AI were 3.6 times more likely to erroneously overrule an AI prediction, despite the prediction being accurate ( t = 2.437, P = 0.011). Interestingly, 73.1% of the workers with explainable AI performed even better than the standalone AI algorithm. This suggests that the explanations (i.e., the heatmaps) not only improve adherence to accurate AI predictions, but also help humans make correct assessments when the AI predictions are wrong. We found that workers with explainable AI were, on average, able to identify and overrule 96.9% of the wrong AI predictions. For comparison, workers supported by black-box AI only overruled 86.4% of the wrong AI predictions. These results are highly relevant since – regardless of an AI’s performance – wrong AI predictions can always occur due to external factors such as dust or different light conditions. The difference between both treatments is again statistically significant ( t = 2.631, P = 0.007). These findings underscore the effectiveness of augmenting humans with explainable AI. 9/uni0000002b/uni00000058/uni00000050/uni00000044/uni00000051/uni00000003/uni0000000e/uni00000003/uni00000045/uni0000004f/uni00000044/uni00000046/uni0000004e/uni00000010/uni00000045/uni00000052/uni0000005b/uni00000003/uni00000024/uni0000002c/uni0000002b/uni00000058/uni00000050/uni00000044/uni00000051/uni00000003/uni0000000e/uni00000003/uni00000048/uni0000005b/uni00000053/uni0000004f/uni00000044/uni0000004c/uni00000051/uni00000044/uni00000045/uni0000004f/uni00000048/uni00000003/uni00000024/uni0000002c /uni00000017/uni00000013/uni00000008 /uni00000018/uni00000013/uni00000008 /uni00000019/uni00000013/uni00000008 /uni0000001a/uni00000013/uni00000008 /uni0000001b/uni00000013/uni00000008 /uni0000001c/uni00000013/uni00000008 /uni00000014/uni00000013/uni00000013/uni00000008 /uni0000001c/uni00000018/uni00000011/uni00000019/uni00000008/uni00000025/uni00000044/uni0000004f/uni00000044/uni00000051/uni00000046/uni00000048/uni00000047/uni00000003/uni00000044/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c t = 3.357/uni0000000d/uni0000000d(P = 0.001) /uni00000024 /uni0000002b/uni00000058/uni00000050/uni00000044/uni00000051/uni00000003/uni0000000e/uni00000003/uni00000045/uni0000004f/uni00000044/uni00000046/uni0000004e/uni00000010/uni00000045/uni00000052/uni0000005b/uni00000003/uni00000024/uni0000002c/uni0000002b/uni00000058/uni00000050/uni00000044/uni00000051/uni00000003/uni0000000e/uni00000003/uni00000048/uni0000005b/uni00000053/uni0000004f/uni00000044/uni0000004c/uni00000051/uni00000044/uni00000045/uni0000004f/uni00000048/uni00000003/uni00000024/uni0000002c /uni00000017/uni00000013/uni00000008 /uni00000018/uni00000013/uni00000008 /uni00000019/uni00000013/uni00000008 /uni0000001a/uni00000013/uni00000008 /uni0000001b/uni00000013/uni00000008 /uni0000001c/uni00000013/uni00000008 /uni00000014/uni00000013/uni00000013/uni00000008 /uni0000001c/uni00000015/uni00000011/uni0000001c/uni00000008/uni00000027/uni00000048/uni00000049/uni00000048/uni00000046/uni00000057/uni00000003/uni00000047/uni00000048/uni00000057/uni00000048/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000055/uni00000044/uni00000057/uni00000048 t = 2.876/uni0000000d/uni0000000d(P = 0.003) /uni00000025 /uni00000033/uni00000048/uni00000055/uni00000049/uni00000052/uni00000055/uni00000050/uni00000044/uni00000051/uni00000046/uni00000048/uni00000003/uni00000052/uni00000049/uni00000003/uni00000056/uni00000057/uni00000044/uni00000051/uni00000047/uni00000044/uni0000004f/uni00000052/uni00000051/uni00000048/uni00000003/uni00000024/uni0000002c/uni00000003/uni00000044/uni0000004f/uni0000004a/uni00000052/uni00000055/uni0000004c/uni00000057/uni0000004b/uni00000050 Figure 2: Results of manufacturing experiment. The boxplots compare the task perfor- mance between the two treatments: black-box AI and explainable AI. The task performance is measured by the balanced accuracy ( A) and the defect detection rate ( B) based on the quality assessment of workers and the ground-truth labels of the product images. A balanced accu- racy of 50% provides a na¨ ıve baseline corresponding to a random guess (black dotted line). The standalone AI algorithm attains a balanced accuracy of 95.6% and a defect detection rate of 92.9% (orange dashed lines). Statistical significance is based on a one-sided Welch’s t-test (***P <0.001, **P <0.01, *P <0.05). In the boxplots, the center line denotes the median; box limits are upper and lower quartiles; whiskers are defined as the 1.5x interquartile range. Finally, we assessed whether workers with explainable AI spent more time on making their quality assessments. For this, we analyzed whether the workers’ median decision speeds across the 200 product images differed. No statistically significant difference (t = 0.308, P = 0.380) was observed between both treatments (mean = 5.01 s for black-box AI, mean = 4.88 s for explainable AI). Therefore, explainable AI improved task performance without affecting the productivity of the workers. Study 2: Medical experiment In the medical experiment, radiologists were asked to visually inspect 50 chest X-ray images and decide whether at least one lung lesion was visible (Figure 1 B). Visual inspection tasks like ours are common in medicine across various subdisciplines [55, 54]. Analogous to the manufacturing task, radiologists had 35 minutes to complete the task and were randomly assigned to be either 10supported by black-box AI or by explainable AI (Figure 1 B). Both types of AI provided a score between 0 (most certainly a lung lesion visible) and 100 (most unlikely a lung lesion visible), which was identical in both treatment arms. In addition to that, radiologists with explainable AI were provided with a heatmap that highlights regions in the chest X-ray image that the AI finds most relevant for predicting lung lesions. The results are based on a sample of N = 5, 650 assessments of chest X-ray images performed by 113 radiologists from the United States. Again, task performance was analyzed using the balanced accuracy and the disease detection rate (i.e., the true negative rate, where chest X-ray images containing lung lesions were considered a negative sample) based on the assessments made in the visual inspection task. Radiologists augmented with explainable AI outperformed peers with black-box AI. Radi- ologists with black-box AI achieved a balanced accuracy with a mean of only 79.1%, whereas radiologists with explainable AI achieved a balanced accuracy with a mean of 83.8% (Figure 3A). We again estimated the treatment effect of explainable AI by regressing the balanced accuracy on the treatment (black-box AI = 0, explainable AI = 1). The regression results show that the treatment effect of explainable AI is statistically significant and large ( β = 4.693, SE = 1.800, P = 0 .01); that is, an improvement of 4.7 percentage points. All results remain statistically significant when including relevant control variables (tenure, self-reported IT skills, and decision speed) in the regression model (see Supplement H.2). In contrast to the manufacturing experi- ment, no difference in task performance with respect to the disease detection rate was observed; radiologists in both treatment arms achieved a disease detection rate with a mean of 90.4% (Figure 3B). This was also observed when regressing the disease detection rate on the treatment (β = −0.014, SE = 2.244, P = 0.995). This can be expected since missing a lung lesion has more serious consequences than erroneously believing a lung lesion is visible; thus, leading to conservative decision-making of radiologists. Therefore, we additionally inspected precision as a task performance metric. We find that radiologists augmented with explainable AI were sig- nificantly more precise (improvement of 6.4 percentage points, P = 0.014) in identifying lung lesions compared to radiologists with black-box AI (see Supplement G). As in Study 1, we found that radiologists with explainable AI followed accurate AI predictions more often than radiologists with black-box AI treatment (mean = 72 .4% for black-box AI, mean = 82 .1% for explainable AI). In particular, radiologists supported by black-box AI were 1154.2% times more likely to erroneously overrule an AI prediction, although it was correct ( t = 3.084, P = 0.001). We observed that radiologists with explainable AI only overruled 50.8% of the wrong AI predictions compared to 57.7% for radiologists with black-box AI treatment. /uni0000002b/uni00000058/uni00000050/uni00000044/uni00000051/uni00000003/uni0000000e/uni00000003/uni00000045/uni0000004f/uni00000044/uni00000046/uni0000004e/uni00000010/uni00000045/uni00000052/uni0000005b/uni00000003/uni00000024/uni0000002c/uni0000002b/uni00000058/uni00000050/uni00000044/uni00000051/uni00000003/uni0000000e/uni00000003/uni00000048/uni0000005b/uni00000053/uni0000004f/uni00000044/uni0000004c/uni00000051/uni00000044/uni00000045/uni0000004f/uni00000048/uni00000003/uni00000024/uni0000002c /uni00000017/uni00000013/uni00000008 /uni00000018/uni00000013/uni00000008 /uni00000019/uni00000013/uni00000008 /uni0000001a/uni00000013/uni00000008 /uni0000001c/uni00000013/uni00000008 /uni00000014/uni00000013/uni00000013/uni00000008 /uni0000001b/uni00000015/uni00000011/uni00000015/uni00000008/uni00000025/uni00000044/uni0000004f/uni00000044/uni00000051/uni00000046/uni00000048/uni00000047/uni00000003/uni00000044/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c t = 2.709/uni0000000d/uni0000000d(P = 0.004) /uni00000024 /uni0000002b/uni00000058/uni00000050/uni00000044/uni00000051/uni00000003/uni0000000e/uni00000003/uni00000045/uni0000004f/uni00000044/uni00000046/uni0000004e/uni00000010/uni00000045/uni00000052/uni0000005b/uni00000003/uni00000024/uni0000002c/uni0000002b/uni00000058/uni00000050/uni00000044/uni00000051/uni00000003/uni0000000e/uni00000003/uni00000048/uni0000005b/uni00000053/uni0000004f/uni00000044/uni0000004c/uni00000051/uni00000044/uni00000045/uni0000004f/uni00000048/uni00000003/uni00000024/uni0000002c /uni00000017/uni00000013/uni00000008 /uni00000018/uni00000013/uni00000008 /uni00000019/uni00000013/uni00000008 /uni0000001b/uni00000013/uni00000008 /uni0000001c/uni00000013/uni00000008 /uni00000014/uni00000013/uni00000013/uni00000008 /uni0000001a/uni00000014/uni00000011/uni00000017/uni00000008/uni00000027/uni0000004c/uni00000056/uni00000048/uni00000044/uni00000056/uni00000048/uni00000003/uni00000047/uni00000048/uni00000057/uni00000048/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000055/uni00000044/uni00000057/uni00000048 t = 0.006/uni00000003(P = 0.498) /uni00000025 /uni00000033/uni00000048/uni00000055/uni00000049/uni00000052/uni00000055/uni00000050/uni00000044/uni00000051/uni00000046/uni00000048/uni00000003/uni00000052/uni00000049/uni00000003/uni00000056/uni00000057/uni00000044/uni00000051/uni00000047/uni00000044/uni0000004f/uni00000052/uni00000051/uni00000048/uni00000003/uni00000024/uni0000002c/uni00000003/uni00000044/uni0000004f/uni0000004a/uni00000052/uni00000055/uni0000004c/uni00000057/uni0000004b/uni00000050 Figure 3: Results of medical experiment. The boxplots compare the task performance between the two treatments: black-box AI and explainable AI. The task performance is measured by the balanced accuracy (A) and the disease detection rate (B) based on the quality assessment of radiologists and the ground-truth labels of the chest X-ray images. A balanced accuracy of 50% provides a na¨ ıve baseline corresponding to a random guess (black dotted line). The standalone AI algorithm attains a balanced accuracy of 82.2% and a disease detection rate of 71.4% (orange dashed lines). Statistical significance is based on a one-sided Welch’s t-test ( ***P < 0.001, **P < 0.01, *P < 0.05). In the boxplots, the center line denotes the median; box limits are upper and lower quartiles; whiskers are defined as the 1.5x interquartile range. Again, we assessed the decision speed of radiologists in both treatment arms. We found no significant difference ( t = 0.392, P = 0.348) between both treatments (mean = 10 .71 s for black-box AI, mean = 10 .29 s for explainable AI). Thus, task performance was improved by explainable AI without reducing the productivity of the radiologists. Discussion The “age of AI” redefines the way humans and machines collaborate, thus raising questions about how human-AI collaborations can be effectively designed. As we show, the effectiveness of 12human-AI collaboration largely depends on the extent to which humans incorporate correct AI predictions and overrule wrong ones. However, many state-of-the-art AI algorithms operate as black-box, thus making it difficult for humans to compare the reasoning of the AI to their own domain knowledge. In this paper, we contribute a unique perspective by studying the impact of AI explainability on task performance of domain experts in human-AI collaboration, presenting empirical evidence from different domains with robust and generalizable results. We conducted two preregistered experiments to estimate the effect of explainability in human- AI collaboration in real-world visual inspection tasks. Our results demonstrate that domain experts make subpar decisions when they are supported by a black-box AI algorithm with opaque predictions. In contrast, we find that explanations from an explainable AI are a powerful decision aid. Explanations were provided in the form of heatmaps, which provide a clear and intuitive way of highlighting areas that are determinants of AI predictions. The explanations do not provide more information from an AI perspective (i.e., the prediction performance is identical), but rather make the information more accessible to domain experts. Specifically, compared to black-box AI, augmenting domain experts with explainable AI improved the task performance by 7.7 percentage points in a manufacturing experiment and by 4.7 percentage points in a medical experiment. In the manufacturing experiment, 73.1% of the domain experts even outperformed the standalone AI algorithm when they were augmented with explainable AI. The prime reason was that domain experts supported by explainable AI were more likely to follow AI predictions when they were accurate and more likely to overrule them when they were wrong. Improving the task performance of domain experts has practical implications in many fields such as manufacturing and medicine. For example, factory workers at Siemens augmented with explainable AI were able to identify 13% more defects than peers augmented with black-box AI. Thus, explainable AI could help to reduce downstream costs for manufacturing companies by filtering out defective products at the earliest possible stage. Similarly in medicine, task performance of physicians is crucial, especially for important tasks such as identifying possibly cancerogenous lung lesions. By showing that the results are consistent across different settings, we demonstrate that our insights are generalizable. Our work contributes experimental evidence to the literature on human-AI collaboration [57, 58, 59, 60, 26, 27, 28, 61, 62, 63, 64, 65]. Algorithm aversion provides a barrier to the wider adoption of human-AI collaboration. Prior literature has presented several remedies, such 13as describing the functional logic of an algorithm [60], giving users permission to modify an algorithm [27], or letting users integrate their own forecasts into an algorithm [62]. This paper presents evidence of an effective alternative; that is, explaining individual predictions from an otherwise opaque AI algorithm. Such explanations allow domain experts to validate how an AI arrives at a certain prediction. Interestingly, while many studies advocate for complete automation, we also show the importance of human-AI collaboration: domain knowledge can help to identify errors in the AI and lead to a better performance than an AI-only system. A strength of our study is that we gather empirical evidence of improved task performance by explainable AI compared to black-box AI. In particular, by performing experiments in two different settings, we demonstrate that these results are generalizable. Unlike previous works on studying task performance in human-AI collaboration [37, 38, 39, 40], we (i) conducted experiments of two real-world job tasks in manufacturing and medicine and (ii) recruited domain experts for those tasks, i.e., factory workers and radiologists. When experiments use simplified decision tasks (e.g., object recognition) that are not representative of actual human work in the field, real-world validity is reduced. In contrast, our study has high external validity. Our work is orthogonal to the literature on explainable AI in computer science, where the main goal is to develop and evaluate new methods for explaining black-box AI algorithms. Contrary, we are interested in a behavioral outcome, namely task performance in human-AI collaboration. A previous study on the effect of explainable AI on task performance found an improvement of 1.5 percentage points in accuracy relative to black-box AI [46]. However, their experiment was designed such that participants were not only shown the real explainable AI but also systemically biased explainable AI. This could have decreased the trust of the participants in the AI and, thus, explain the smaller treatment effect in comparison to our experiments. Prior work has also made use of expert annotations as a proxy for explainable AI [43]. However, this prevents any conclusion on whether real explainable AI improves task performance. One limitation of our research is that we, in both experiments, studied one specific human- AI work setting (a visual inspection task) with one specific form of explainability (a heatmap indicating the location of potential quality defects or lung lesions). However, this experimental task is representative of many real-world human-AI work settings and heatmaps are standard in explaining AI predictions of images. We also show that other heatmap algorithms lead to similar results (see Supplement F). Still, we invite future research replicating our findings in other 14work settings using other methods for explainability. We also acknowledge reservations against using explainable AI in general. Explainable AI can be fooled by adversarial attacks [66] or may itself generate explanations that are unreliable and thus lead to misleading conclusions [8]. Nevertheless, it is likely that performance improvements from explainable AI can be achieved in other settings, where explanations serve as a decision aid. Further, it is important to note that, in both experiments, 14% of the images contained quality defects/lung lesions. Thus, quality defects/lung lesions were more prevalent than what domain experts would typically encounter in their respective job. This discrepancy might have influenced their prior expectations while performing the task. However, as participants in both treatment arms were shown exactly the same images, this factor likely had minimal impact on our findings. Policy initiatives in many countries aim to promote transparency in AI algorithms (e.g., the United States [67] and the European Union [68]). These efforts are usually motivated from the perspective of ethics, regulation, and safety [9, 69, 70, 71]. Our research suggests that the benefits of algorithmic transparency are more profound: augmenting domain experts with explainable AI can enable better decisions with benefits for individuals, organizations, and society. 15Methods This work analyzes the effect of augmenting domain experts with explainable AI (as opposed to black-box AI) in human-AI collaboration. We preregistered our hypotheses (i.e., Study 1: https://osf.io/7djxb and Study 2: https://osf.io/69yqt; see also Supplement K), which were tested in two randomized experiments. We comply with all ethical regulations and the research design was approved by the Ethics Commission of ETH Zurich (EK 2021-N-34). All participants provided informed consent. Tasks In the following, details of both visual inspection tasks are provided. Study 1: Manufacturing experiment For the manufacturing task, we designed a representative, real-world visual inspection task in collaboration with Siemens Smart Infrastructure in Zug, Switzerland. The experimental task is representative of various domains in which workers have to make decisions under a limited time budget. During the experiment, workers were shown images of electronic products and were asked to label them as faultless or defective. Reassuringly, we emphasize that our experiment involved a real work scenario: we conducted it with real workers familiar with quality management practice, a real user interface for state-of-the-art quality management, realistic incentives, and real product images. All steps in the experiments were carried out via a computer interface that was designed analogously to the real-world quality inspection setup at Siemens (see Supplement D for details). In the experiment, we made sure that all workers have the exact same conditions (exact same product images, same computer setup, same time limits, etc.). Thereby, we can rule out confounding variables that would arise naturally during the usual work routines and thus ensure that the experiment is scientifically sound. We obtained 200 images of four different types of electronic products (printed circuit boards) from Siemens. Example images are provided in Supplement B. All four different product types are of equal importance to Siemens. Each product type comprised 43 images with faultless products and 7 images with defective products (e.g., missing components, wrong components, and faulty components). The different defects are all considered equally bad by the partner 16company, i.e., the products are considered to be either functional or non-functional. Hence, we considered defective products as scrap as best practice in quality management [51, 52]. We implemented an AI algorithm that computed an individual quality score for each image. The quality score gives a numerical value between 0 (most certainly defect) and 100 (most certainly faultless). Workers were instructed that a quality score below 90 suggests an increased likelihood of a quality defect and that the AI algorithm can make mistakes. As humans cannot understand how the AI algorithm arrives at the prediction, the quality score is regarded as opaque (“black-box AI”). When evaluating the quality score with a cutoff of 90 for mapping the numerical value onto a binary faultless/defect label, the standalone AI algorithm achieves a balanced accuracy of 95.6% and a defect detection rate of 92.9%. The prediction performance of the standalone AI algorithm was not communicated to the workers. The AI algorithm was trained on an additional set of product images that was not included in the experiment. We used anomaly heatmaps [47] to explain the opaque quality from the AI algorithm. The heatmaps were computed with standard computer vision methods and highlighted image regions with suspected quality defects (i.e., deviations from a faultless product). We chose heatmaps as the explanation technique for our AI algorithm as they provide a clear and intuitive way of highlighting areas with quality defects. This is especially important since the recruited do- main experts are typically not familiar with explanation techniques for AI algorithms. Further, heatmaps are frequently used for images and are considered state-of-the-art with respect to their localization performance across various settings [47, 48, 49, 50]. Details on the implementation of the AI algorithm and heatmaps are provided in Supplement C. In the experiment, workers were randomly assigned to one of the two treatments: (a) black- box AI or (b) explainable AI. Workers in the black-box AI treatment arm were only supported by the opaque quality score. Workers in the explainable AI treatment arm had access to the same quality score but additionally received the heatmap that explained the otherwise opaque quality score. Of note, the explainable AI had the same accuracy as the black-box AI and did not carry more information from an AI perspective (i.e., the heatmaps were of the same predictive power). The procedure of the experiment was as follows. Before starting the experiment, workers had to give written consent to participate and then pass a tutorial on how to use the interface. After that workers were randomly assigned to one of the two treatments, i.e., either black-box 17AI or explainable AI. During the experiment, the 200 product images were consecutively shown in random order. For each image, the workers had to assess the quality; that is, to “approve” or “reject” the shown product. We tracked the decision speed and the quality assessment (i.e., labeled as faultless or defective) made by the worker. To match real-world conditions, the workers were given a maximum of 35 minutes to finish the inspection task of 200 product images (around 10 seconds per image). In total, N = 9, 600 assessments of product images were performed by the workers. Finally, workers completed a post-experimental questionnaire (Supplement L). Study 2: Medical experiment For the medical task, radiologists had to identify lung lesions in real chest X-ray images. Lung lesions are common findings in chest X-ray images [72] and can be easily overlooked due to their frequently small size [73]. The radiologists were asked whether at least one lung lesion was visible in the X-ray image. The experiment was conducted via Qualtrics. To ensure a realistic experimental setup that resembles the same task in daily, medical practice, we implemented a zoom function, which allowed the radiologists to investigate an enlarged view of the image by moving their computer mouse over the image. Analogous to Study 1, we emphasize that our medical experiment involved a realistic work scenario: we conducted it with actual medical professionals, who were asked to investigate real chest X-ray images. We used 50 chest X-ray images from the CheXpert dataset [74]. The dataset comprised 7 images with at least one lung lesion and 43 images without lung lesions. Example images are provided in Supplement B. We implemented an AI algorithm that outputs the probability of whether a lung lesion is visible in the chest X-ray image. We transformed these probability outputs for lung lesions such that the AI score gives a numerical value between 0 (most certainly contains a lung lesion) and 100 (most certainly does not contain a lung lesion) to mirror the quality score from the manufacturing setting. Hence, the AI output can be interpreted as a risk score, which are widely used in medical practice. Radiologists were instructed that an AI score below 90 indicates that the AI algorithm suspects at least one lung lesion is visible and that the AI algorithm can make mistakes. When evaluating the AI score with a cutoff of 90 for mapping the numerical value onto a binary label (lung lesion visible yes/no), the standalone AI algorithm achieves a balanced accuracy of 82.2% and a disease detection rate of 71.4%. Analogously to the manufacturing 18task, the prediction performance of the standalone AI algorithm was not communicated to the participants. As in the manufacturing task, the black-box AI algorithm was converted into an explainable AI by explaining the AI score via a heatmap, which is a state-of-the-art explanation technique for chest X-ray images in medicine [50]. Further details about the implementation of the AI algorithm and the heatmap are provided in Supplement C. The procedure was analogous to the manufacturing experiment. Before starting the ex- periment, radiologists had to confirm their area of specialization and give written consent to participate. Subsequently, the task was explained and the radiologists had to pass a tutorial on how to use the interface. After that radiologists were randomly assigned to one of the two treatments, i.e., either black-box AI or explainable AI. During the experiment, 50 chest X-ray images were randomly shown either in forward or reverse order. For each chest X-ray image, the radiologists had to answer whether at least one lung lesion is visible. The corresponding answers as well as the decision speed were tracked. Radiologists were given a maximum of 35 minutes to finish the inspection task of 50 chest X-ray images. Radiologists were given more time per image compared to factory workers in the manufacturing experiment to reflect the differences in manufacturing and clinical practice. In total, N = 5 , 650 assessments of chest X-ray im- ages were performed by the radiologists. Finally, all radiologists completed a post-experimental questionnaire (Supplement L). Study populations The inclusion were as follows. Participants had to be at least 18 years old. For the manufacturing task, participants additionally had to have no self-reported visual impairment. The exclusion criteria were preregistered and were as follows. In both studies, we excluded participants that failed the tutorial or did not finish the inspection task on time. Participants with obvious misbehavior were also excluded from our analyses. In the manufacturing task, this was the case for workers that approved all products (i.e., labeled no images as defective). In the medical task, this was the case for radiologists that assigned the same label for all 50 chest X-ray images. In both studies, participants whose performance with respect to balanced accuracy was more than three standard deviations worse than the mean of their respective treatment arm were excluded. 19We performed randomization checks to confirm that all treatment arms were demographically unbiased (Supplement E). Study 1: Manufacturing experiment The manufacturing experiment was carried out from June 29 to July 8, 2021 on-site at aSiemens factory in Zug, Switzerland. The objective of the field experiment was to get a real-world estimate of the treatment effect based on a representative sample of actual factory workers. Therefore, we only considered factory workers who were experienced in quality control practices. The factory workers were well familiar with the shown products and the visual inspection task. Overall, 56 factory workers (consisting of manufacturing employees, quality engineers, and team leaders) participated in our study. Out of them, all workers passed the tutorial; 6 did not finish on time; and 2 were excluded due to obvious misbehavior. The final sample consisted of 48 factory workers with an average working experience of 13.8 years. A larger sample size in our manufacturing experiment was not possible because the entire available workforce in one shift did not exceed 56 workers. Still, the experiment is well-powered as the treatment effect in the field experiment is considerably large. No additional financial incentive was given beyond the base salary to be representative of many real-world tasks from domain experts (e.g., as in manufacturing at Siemens). Study 2: Medical experiment The medical experiment was carried out via an online interface from February 27 to March 31, 2024. Actual radiologists based in the United States were recruited via MSI-ACI (https://site.msi-aci.com/). MSI-ACI paid a financial compensation to radiologists regardless of performance and adheres to the federal minimum wage in the United States. Overall, 122 radi- ologists started the study. Out of them, all passed the tutorial; 4 did not complete the study; 2 did not finish on time; and 3 were excluded due to obvious misbehavior. Hence, the final sample consisted of 113 radiologists with an average tenure as radiologist of 13.5 years. Statistical analysis In manufacturing, it is common that all defects (e.g., missing components, wrong components, and faulty components) are considered equally bad [51, 52], so that the product to inspect 20could be either functional or non-functional. Analogously, in the medical setting, either lung lesions were present or not. Hence, in both settings, the outcomes were binary. Therefore, task performance in the visual inspection tasks between the participants’ assessments and the ground- truth labels was computed via (1) balanced accuracy (i.e., average sensitivity across faultless and defective products) and (2) defect/disease detection rate. For (1), the balanced accuracy is calculated via 0 .5 × [TP/P + TN /N] with true positives TP, positives P, true negatives TN , and negatives N. Here, we used balanced accuracy since it accounts for imbalanced distributions of labels by equally weighing the performance on each label, thus following best practice [75]. In contrast, the standard accuracy score would not account for the imbalanced distribution of positive and negative labels encountered in both settings (i.e., 172 faultless products and 28 defective products in the manufacturing setting; 7 chest X-ray images with and 43 without lung lesions in the medical setting). For (2), the defect/disease detection rate is defined as TN /N, where defective products and chest X-ray images with lung lesions were defined as negatives. In our manufacturing setting, missing a defective product has more severe implications than labeling a faultless product as defective. In medicine, missing a lung lesion on a chest X-ray image has more severe implications than additionally performing a CT scan for a healthy patient. Hence, it is crucial to find the negative samples. All statistical tests in the results are based on one-sided Welch’s t-tests. We further used ordinary least square (OLS) regression models to estimate the treatment effect of explainable AI on task performance. The OLS models are estimated via Yi = β0 + β1 Treatmenti + εi, (1) where Yi is the observed task performance (i.e., balanced accuracy or defect/disease detection rate), T reatmenti is a binary variable which equals 0 if participant i received the black-box AI treatment and 1 if participant i received the explainable AI treatment. A significance level of α = 0.05 was preregistered. Robustness checks We conducted the following robustness checks. First, we repeated our analyses using precision as an additional task performance metric (Supplement G). Second, we repeated the OLS re- 21gression models with additional participant-specific controls to estimate the treatment effect of explainable AI (Supplement H). Third, we estimated the treatment effect with quasi-binomial regression (Supplement H). Fourth, we estimated the regression models including participants that were previously excluded due to obvious misbehavior or because they did not finish the inspection task in time (Supplement I). All robustness checks yielded conclusive findings. To demonstrate that the heatmaps in our medical setting are robust with respect to the choice of algorithm, we used two additional, different algorithms to generate heatmaps. We find that different algorithms lead to similar heatmaps (Supplement F). Comparison to non-experts Additionally, we repeated the manufacturing task with non-experts recruited from Amazon MTurk as a robustness check (Supplement J). Typically, non-experts can not leverage expla- nations in the same way as domain experts due to missing domain knowledge. Hence, we were interested whether the large treatment effect of explainable AI on task performance we observed with domain experts transfers also to non-experts. We found that non-experts supported by explainable AI also achieved a higher task perfor- mance than non-experts supported by black-box AI. Task performance of non-experts augmented with explainable AI was improved by 6.3 percentage points with respect to balanced accuracy. However, the treatment effect of explainable AI is slightly smaller as compared to the experi- ment with domain experts (where the balanced accuracy increased by 7.7 percentage points). Furthermore, non-experts with explainable AI achieved a higher defect detection rate with an improvement of 11.3 percentage points. This is of a similar effect size as in the real-world experiment (11.0 percentage points). The treatment effects in both metrics were again statisti- cally significant (balanced accuracy: β = 6.252, SE = 1.733, P < 0.001; defect detection rate: β = 11.271, SE = 3.276, P = 0.001). For more details, see Supplement J. References [1] Brynjolfsson, E. & Mitchell, T. What can machine learning do? Workforce implications. Science 358, 1530–1534 (2017). 22[2] Perrault, R. & Clark, J. Artificial intelligence index report 2024. Human-Centered Artifi- cal Intelligence. United States of America. Retrieved from https://policycommons.net/ artifacts/12089781/hai_ai-index-report-2024/12983534/ on 26 Apr 2024. CID: 20.500.12592/h70s46h (2024). [3] Bertolini, M., Mezzogori, D., Neroni, M. & Zammori, F. Machine learning for industrial applications: A comprehensive literature review. Expert Systems with Applications 175, 114820 (2021). [4] Scheetz, J., Rothschild, P., McGuinness, M., Hadoux, X., Soyer, H. P., Janda, M., Condon, J. J. J., Oakden-Rayner, L., Palmer, L. J., Keel, S. & van Wijngaarden, P. A survey of clinicians on the use of artificial intelligence in ophthalmology, dermatology, radiology and radiation oncology. Scientific Reports 11, 5193 (2021). [5] Cazzaniga, M., Jaumotte, F., Li, L., Melina, G., Panton, A. J., Pizzinelli, C., Rockall, E. J. & Tavares, M. M. Gen-AI: Artificial intelligence and the fu- ture of work. International Monetary Fund. Staff Discussion Notes 2024/001 https://www.imf.org/en/Publications/Staff-Discussion-Notes/Issues/2024/ 01/14/Gen-AI-Artificial-Intelligence-and-the-Future-of-Work-542379 (2024). [6] Simonyan, K. & Zisserman, A. Very deep convolutional networks for large-scale image recognition. In International Conference on Learning Representations (2015). [7] He, K., Zhang, X., Ren, S. & Sun, J. Deep residual learning for image recognition. In IEEE Conference on Computer Vision and Pattern Recognition , 770–778 (2016). [8] Rudin, C. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nature Machine Intelligence 1, 206–215 (2019). [9] Guidotti, R., Monreale, A., Ruggieri, S., Turini, F., Giannotti, F. & Pedreschi, D. A survey of methods for explaining black box models. ACM Computing Surveys 51, 1–42 (2018). [10] Murdoch, W. J., Singh, C., Kumbier, K., Abbasi-Asl, R. & Yu, B. Definitions, methods, and applications in interpretable machine learning. Proceedings of the National Academy of Sciences 116, 22071–22080 (2019). 23[11] Gunning, D., Stefik, M., Choi, J., Miller, T., Stumpf, S. & Yang, G.-Z. XAI—Explainable artificial intelligence. Science Robotics 4, eaay7120 (2019). [12] Cheng, H.-F., Wang, R., Zhang, Z., O 'Connell, F., Gray, T., Harper, F. M. & Zhu, H. Explaining decision-making algorithms through UI. In CHI Conference on Human Factors in Computing Systems , 1–12 (2019). [13] Liao, Q. V., Gruen, D. & Miller, S. Questioning the AI: Informing design practices for explainable AI user experiences. In CHI Conference on Human Factors in Computing Systems (2020). [14] Bodria, F., Giannotti, F., Guidotti, R., Naretto, F., Pedreschi, D. & Rinzivillo, S. Bench- marking and survey of explanation methods for black box models. Data Mining and Knowledge Discovery 37, 1719–1778 (2023). [15] Dwivedi, R., Dave, D., Naik, H., Singhal, S., Omer, R., Patel, P., Qian, B., Wen, Z., Shah, T., Morgan, G. & Ranjan, R. Explainable AI (XAI): Core ideas, techniques, and solutions. ACM Computing Surveys 55, 1–33 (2023). [16] Samek, W., Montavon, G., Vedaldi, A., Hansen, L. K. & M¨ uller, K.-R. Explainable AI: Interpreting, Explaining and Visualizing Deep Learning (Springer Nature, 2019). [17] Samek, W., Montavon, G., Lapuschkin, S., Anders, C. J. & Muller, K.-R. Explaining deep neural networks and beyond: A review of methods and applications. Proceedings of the IEEE 109, 247–278 (2021). [18] Molnar, C. Interpretable machine learning: A guide for making Black Box Models inter- pretable (Lulu. com, 2019). [19] Ribeiro, M. T., Singh, S. & Guestrin, C. Why should I trust you? Explaining the predic- tions of any classifier. In ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (2016). [20] Lundberg, S. & Lee, S.-I. A unified approach to interpreting model predictions. InAdvances in Neural Information Processing Systems (2017). 24[21] Simonyan, K., Vedaldi, A. & Zisserman, A. Deep inside convolutional networks: Visualising image classification models and saliency maps. In Workshop at International Conference on Learning Representations (2014). [22] Selvaraju, R. R., Cogswell, M., Das, A., Vedantam, R., Parikh, D. & Batra, D. Grad- CAM: Visual explanations from deep networks via gradient-based localization. In IEEE International Conference on Computer Vision , 618–626 (2017). [23] F¨ ugener, A., Grahl, J., Gupta, A. & Ketter, W. Cognitive challenges in human–artificial in- telligence collaboration: Investigating the path toward productive delegation. Information Systems Research 33, 678–696 (2022). [24] Andreas F¨ ugener, Alok Gupta, J¨ orn Grahl, Wolfgang Ketter & Anna Taudien. Exploring user heterogeneity in human delegation behavior towards AI. In International Conference on Information Systems (2021). [25] Bauer, K., von Zahn, M. & Hinz, O. Please take over: XAI, delegation of authority, and domain knowledge. Preprint at SSRN https://doi.org/10.2139/ssrn.4512594 (2023). [26] Dietvorst, B. J., Simmons, J. P. & Massey, C. Algorithm aversion: People erroneously avoid algorithms after seeing them err. Journal of Experimental Psychology: General 144, 114–126 (2015). [27] Dietvorst, B. J., Simmons, J. P. & Massey, C. Overcoming algorithm aversion: People will use imperfect algorithms if they can (even slightly) modify them. Management Science 64, 1155–1170 (2018). [28] Dietvorst, B. J. & Bharti, S. People reject algorithms in uncertain decision domains because they have diminishing sensitivity to forecasting error. Psychological Science 31, 1302–1314 (2020). [29] Burton, J. W., Stein, M.-K. & Jensen, T. B. A systematic review of algorithm aversion in augmented decision making. Journal of Behavioral Decision Making 33, 220–239 (2020). [30] Ben David, D., Resheff, Y. S. & Tron, T. Explainable AI and adoption of financial algo- rithmic advisors. In AAAI/ACM Conference on AI, Ethics, and Society , 390–400 (2021). 25[31] Choung, H., David, P. & Ross, A. Trust in AI and its role in the acceptance of AI tech- nologies. International Journal of Human–Computer Interaction 39, 1727–1739 (2023). [32] Nourani, M., Kabir, S., Mohseni, S. & Ragan, E. D. The effects of meaningful and mean- ingless explanations on trust and perceived system accuracy in intelligent systems. AAAI Conference on Human Computation and Crowdsourcing 7 (2019). [33] Panigutti, C., Beretta, A., Giannotti, F. & Pedreschi, D. Understanding the impact of explanations on advice-taking: a user study for AI-based clinical decision support systems. In CHI Conference on Human Factors in Computing Systems (2022). [34] Bansal, G., Wu, T., Zhou, J., Fok, R., Nushi, B., Kamar, E., Ribeiro, M. T. & Weld, D. Does the whole exceed its parts? The effect of AI explanations on complementary team performance. In CHI Conference on Human Factors in Computing Systems (2021). [35] Vasconcelos, H., J¨ orke, M., Grunde-McLaughlin, M., Gerstenberg, T., Bernstein, M. S. & Krishna, R. Explanations can reduce overreliance on AI systems during decision-making. ACM on Human-Computer Interaction 7, 129 (2023). [36] Chen, V., Liao, Q. V., Wortman Vaughan, J. & Bansal, G. Understanding the role of human intuition on reliance in human-AI decision-making with explanations. ACM on Human-Computer Interaction 7, 370 (2023). [37] Bu¸ cinca, Z., Lin, P., Gajos, K. Z. & Glassman, E. L. Proxy tasks and subjective measures can be misleading in evaluating explainable AI systems. In International Conference on Intelligent User Interfaces (2020). [38] Chu, E., Roy, D. & Andreas, J. Are visual explanations useful? A case study in model-in- the-loop prediction. Preprint at arXiv https://doi.org/10.48550/arXiv.2007.12248 (2020). [39] Alufaisan, Y., Marusich, L. R., Bakdash, J. Z., Zhou, Y. & Kantarcioglu, M. Does explain- able artificial intelligence improve human decision-making? AAAI Conference on Artificial Intelligence 35 (2021). 26[40] Schemmer, M., Kuehl, N., Benz, C., Bartos, A. & Satzger, G. Appropriate reliance on AI advice: Conceptualization and the effect of explanations. In International Conference on Intelligent User Interfaces (2023). [41] Lundberg, S. M., Erion, G., Chen, H., DeGrave, A., Prutkin, J. M., Nair, B., Katz, R., Himmelfarb, J., Bansal, N. & Lee, S.-I. From local explanations to global understanding with explainable AI for trees. Nature Machine Intelligence 2, 56–67 (2020). [42] Das, N. et al. Collaboration between explainable artificial intelligence and pulmonologists improves the accuracy of pulmonary function test interpretation. European Respiratory Journal 61, 2201720 (2023). [43] Gaube, S., Suresh, H., Raue, M., Lermer, E., Koch, T. K., Hudecek, M. F. C., Ackery, A. D., Grover, S. C., Coughlin, J. F., Frey, D., Kitamura, F. C., Ghassemi, M. & Colak, E. Non-task expert physicians benefit from correct explainable AI advice when reviewing X-rays. Scientific Reports 13, 1383 (2023). [44] Jesus, S., Bel´ em, C., Balayan, V., Bento, J., Saleiro, P., Bizarro, P. & Gama, J. How can I choose an explainer? In ACM Conference on Fairness, Accountability, and Transparency (2021). [45] Sivaraman, V., Bukowski, L. A., Levin, J., Kahn, J. M. & Perer, A. Ignore, trust, or negotiate: Understanding clinician acceptance of AI-based treatment recommendations in health care. In CHI Conference on Human Factors in Computing Systems (2023). [46] Jabbour, S., Fouhey, D., Shepard, S., Valley, T. S., Kazerooni, E. A., Banovic, N., Wiens, J. & Sjoding, M. W. Measuring the impact of AI in the diagnosis of hospitalized patients: A randomized clinical vignette survey study. JAMA 330, 2275–2284 (2023). [47] Bergmann, P., L¨ owe, S., Fauser, M., Sattlegger, D. & Steger, C. Improving unsupervised defect segmentation by applying structural similarity to autoencoders. Preprint at arXiv https://doi.org/10.48550/arXiv.1807.02011 (2019). [48] Bergmann, P., Fauser, M., Sattlegger, D. & Steger, C. MVTec AD — A comprehensive real-world dataset for unsupervised anomaly detection. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (2019). 27[49] Binder, A., Bockmayr, M., H¨ agele, M., Wienert, S., Heim, D., Hellweg, K., Ishii, M., Stenzinger, A., Hocke, A., Denkert, C., M¨ uller, K.-R. & Klauschen, F. Morphological and molecular breast cancer profiling through explainable machine learning. Nature Machine Intelligence 3, 355–366 (2021). [50] Saporta, A., Gui, X., Agrawal, A., Pareek, A., Truong, S. Q. H., Nguyen, C. D. T., Ngo, V.-D., Seekins, J., Blankenberg, F. G., Ng, A. Y., Lungren, M. P. & Rajpurkar, P. Benchmarking saliency methods for chest x-ray interpretation.Nature Machine Intelligence 4, 867–878 (2022). [51] Juran, J., Gryna, F. & Bingham, R. Quality control handbook (New York: McGraw-Hill, 1979). [52] Hoyle, D. Quality Management Essentials (Butterworth-Heinemann, 2007). [53] Baudin, M. & Netland, T. H. Introduction to manufacturing: An industrial engineering and management perspective (Taylor & Francis, 2022). [54] Tschandl, P. et al. Human-computer collaboration for skin cancer recognition. Nature Medicine 26, 1229–1234 (2020). [55] Pantanowitz, L., Valenstein, P. N., Evans, A. J., Kaplan, K. J., Pfeifer, J. D., Wilbur, D. C., Collins, L. C. & Colgan, T. J. Review of the current state of whole slide imaging in pathology. Journal of Pathology Informatics 2, 36 (2011). [56] Ibrahim, R. & Shafiq, M. O. Explainable convolutional neural networks: A taxonomy, review, and future directions. ACM Computing Surveys 55, 1–37 (2023). [57] Amershi, S., Weld, D., Vorvoreanu, M., Fourney, A., Nushi, B., Collisson, P., Suh, J., Iqbal, S., Bennett, P. N., Inkpen, K., Teevan, J., Kikin-Gil, R. & Horvitz, E. Guidelines for human-AI interaction. In CHI Conference on Human Factors in Computing Systems (2019). [58] De-Arteaga, M., Fogliato, R. & Chouldechova, A. A case for humans-in-the-loop: Decisions in the presence of erroneous algorithmic scores. In CHI Conference on Human Factors in Computing Systems (2020). 28[59] F¨ ugener, A., Grahl, J., Gupta, A. & Ketter, W. Will humans-in-the-loop become borgs? Merits and pitfalls of working with AI. MIS Quarterly 45, 1527–1556 (2021). [60] Cadario, R., Longoni, C. & Morewedge, C. K. Understanding, explaining, and utilizing medical artificial intelligence. Nature Human Behaviour 5, 1636–1642 (2021). [61] Sun, J., Zhang, D. J., Hu, H. & van Mieghem, J. A. Predicting human discretion to adjust algorithmic prescription: A large-scale field experiment in warehouse operations. Management Science 68, 846–865 (2022). [62] Kawaguchi, K. When will workers follow an algorithm? A field experiment with a retail business. Management Science 67, 1670–1695 (2021). [63] Castelo, N., Bos, M. W. & Lehmann, D. R. Task-dependent algorithm aversion. Journal of Marketing Research 56, 809–825 (2019). [64] Yeomans, M., Shah, A., Mullainathan, S. & Kleinberg, J. Making sense of recommenda- tions. Journal of Behavioral Decision Making 32, 403–414 (2019). [65] Senoner, J., Netland, T. & Feuerriegel, S. Using explainable artificial intelligence to im- prove process quality: Evidence from semiconductor manufacturing. Management Science 68, 5704–5723 (2022). [66] Slack, D., Hilgard, S., Jia, E., Singh, S. & Lakkaraju, H. Fooling LIME and SHAP: Adversarial attacks on post hoc explanation methods. In AAAI/ACM Conference on AI, Ethics, and Society (2020). [67] Algorithmic Accountability Act. H.R.2231 - Algorithmic Accountability Act of 2019 (116th Congress) (2019). URL https://www.congress.gov/bill/116th-congress/ house-bill/2231. [68] European Commission. Ethics Guidelines for Trustworthy AI (2019). URL https:// digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai . [69] Wachter, S., Mittelstadt, B. & Floridi, L. Transparent, explainable, and accountable AI for robotics. Science Robotics 2, eaan6080 (2017). [70] Muller, H., Mayrhofer, M. T., van Veen, E.-B. & Holzinger, A. The ten commandments of ethical medical AI. Computer 54, 119–123 (2021). 29[71] Jobin, A., Ienca, M. & Vayena, E. The global landscape of AI ethics guidelines. Nature Machine Intelligence 1, 389–399 (2019). [72] UTSouthwestern Medical Center. Pulmonary nodules and lung lesions (Ac- cessed 04/09/24). URL https://utswmed.org/conditions-treatments/ pulmonary-nodules-and-lung-lesions/ . [73] Oestmann, J. W., Greene, R., Kushner, D. C., Bourgouin, P. M., Linetsky, L. & Llewellyn, H. J. Lung lesions: correlation between viewing time and detection. Radiology 166, 451– 453 (1988). [74] Irvin, J. et al. CheXpert: A large chest radiograph dataset with uncertainty labels and expert comparison. AAAI Conference on Artificial Intelligence 33 (2019). [75] Hollon, T. et al. Artificial-intelligence-based molecular classification of diffuse gliomas using rapid, label-free optical imaging. Nature Medicine 29, 828–832 (2023). [76] Linardatos, P., Papastefanopoulos, V. & Kotsiantis, S. Explainable AI: A review of machine learning interpretability methods. Entropy 23 (2020). [77] Nelder, J. A. & Wedderburn, R. W. M. Generalized linear models. Journal of the Royal Statistical Society. Series A (General) 135, 370–384 (1972). [78] Hastie, T. & Tibshirani, R. Generalized additive models (Chapman & Hall/CRC, 1990). [79] Lou, Y., Caruana, R. & Gehrke, J. Intelligible models for classification and regression. In ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (2012). [80] Kraus, M., Tschernutter, D., Weinzierl, S. & Zschech, P. Interpretable generalized additive neural networks. European Journal of Operational Research 317, 303–316 (2024). [81] Shapley, L. S. A value for n-person games. In Contributions to the Theory of Games , Annals of Mathematics Studies, 307–318 (Princeton University Press, 1953). [82] Hansen, K., Baehrens, D., Schroeter, T., Rupp, M. & M¨ uller, K.-R. Visual interpretation of kernel-based prediction models. Molecular Informatics 30, 817–826 (2011). 30[83] Buhrmester, V., M¨ unch, D. & Arens, M. Analysis of explainers of black box deep neural networks for computer vision: A survey. Machine Learning and Knowledge Extraction 3, 966–989 (2021). [84] Zeiler, M. D. & Fergus, R. Visualizing and understanding convolutional networks. In European Conference on Computer Vision , 8689 (2014). [85] Bach, S., Binder, A., Montavon, G., Klauschen, F., M¨ uller, K.-R. & Samek, W. On pixel- wise explanations for non-linear classifier decisions by layer-wise relevance propagation. PLOS ONE 10, e0130140 (2015). [86] Sundararajan, M., Taly, A. & Yan, Q. Axiomatic attribution for deep networks. In International Conference on Machine Learning , 70 (2017). [87] Shrikumar, A., Greenside, P. & Kundaje, A. Learning important features through propa- gating activation differences. In International Conference on Machine Learning, 70 (2017). [88] Zhou, B., Khosla, A., Lapedriza, A., Oliva, A. & Torralba, A. Learning deep features for discriminative localization. In IEEE Conference on Computer Vision and Pattern Recognition (2016). [89] Chattopadhay, A., Sarkar, A., Howlader, P. & Balasubramanian, V. N. Grad-CAM++: Generalized gradient-based visual explanations for deep convolutional networks. In IEEE Winter Conference on Applications of Computer Vision , 839–847 (2018). [90] Bany Muhammad, M. & Yeasin, M. Eigen-CAM: Visual explanations for deep convolu- tional neural networks. SN Computer Science 2 (2021). [91] Verma, S., Dickerson, J. P. & Hines, K. E. Counterfactual explanations for machine learning: A review. Preprint at arXiv https://doi.org/10.48550/arXiv.2010.10596 (2020). [92] Zipfel, J., Verworner, F., Fischer, M., Wieland, U., Kraus, M. & Zschech, P. Anomaly detection for industrial quality assurance: A comparative evaluation of unsupervised deep learning models. Computers & Industrial Engineering 177, 109045 (2023). [93] Liu, J., Xie, G., Wang, J., Li, S., Wang, C., Zheng, F. & Jin, Y. Deep industrial image anomaly detection: A survey. Machine Intelligence Research 21, 104–135 (2024). 31[94] Doshi-Velez, F. & Kim, B. Towards a rigorous science of interpretable machine learning. Preprint at arXiv https://doi.org/10.48550/arXiv.1702.08608 (2017). [95] Plumb, G., Molitor, D. & Talwalkar, A. S. Model agnostic supervised local explanations. In Advances in Neural Information Processing Systems (2018). [96] Hooker, S., Erhan, D., Kindermans, P.-J. & Kim, B. A benchmark for interpretability methods in deep neural networks. In Advances in Neural Information Processing Systems (2019). [97] Alvarez-Melis, D. & Jaakkola, T. S. On the robustness of interpretability methods. Preprint at arXiv https://doi.org/10.48550/arXiv.1806.08049 (2018). [98] Narayanan, M., Chen, E., He, J., Kim, B., Gershman, S. & Doshi-Velez, F. How do humans understand explanations from machine learning systems? An evaluation of the human-interpretability of explanation. Preprint at arXiv https://doi.org/10.48550/ arXiv.1802.00682 (2018). [99] Amarasinghe, K., Rodolfa, K. T., Jesus, S., Chen, V., Balayan, V., Saleiro, P., Bizarro, P., Talwalkar, A. & Ghani, R. On the importance of application-grounded experimental design for evaluating explainable ML methods. AAAI Conference on Artificial Intelligence 38 (2024). [100] Liu, M., Shi, J., Li, Z., Li, C., Zhu, J. & Liu, S. Towards better analysis of deep convolu- tional neural networks. IEEE Transactions on Visualization and Computer Graphics 23, 91–100 (2017). [101] Ming, Y., Cao, S., Zhang, R., Li, Z., Chen, Y., Song, Y. & Qu, H. Understanding hidden memories of recurrent neural networks. In IEEE Conference on Visual Analytics Science and Technology (2017). [102] Pezzotti, N., Hollt, T., van Gemert, J., Lelieveldt, B. P. F., Eisemann, E. & Vilanova, A. DeepEyes: Progressive visual analytics for designing deep neural networks. IEEE Transactions on Visualization and Computer Graphics 24, 98–108 (2018). 32[103] Strobelt, H., Gehrmann, S., Pfister, H. & Rush, A. M. LSTMVis: A tool for visual analysis of hidden state dynamics in recurrent neural networks.IEEE Transactions on Visualization and Computer Graphics 24, 667–676 (2018). [104] Candrian, C. & Scherer, A. Rise of the machines: Delegating decisions to autonomous AI. Computers in Human Behavior 134, 107308 (2022). [105] Jessica Ochmann, Leonard Michels, Sandra Zilker, Verena Tiefenbeck & Sven Laumer. The influence of algorithm aversion and anthropomorphic agent design on the acceptance of AI-based job recommendations. In International Conference on Information Systems (2020). [106] Hou, Y. T.-Y. & Jung, M. F. Who is the expert? Reconciling algorithm aversion and algorithm appreciation in AI-supported decision making. ACM on Human-Computer In- teraction 5, 477 (2021). [107] Bogert, E., Schecter, A. & Watson, R. T. Humans rely more on algorithms than social influence as a task becomes more difficult. Scientific Reports 11, 8028 (2021). [108] Fleiß, J., B¨ ack, E. & Thalmann, S. Mitigating algorithm aversion in recruiting: A study on explainable AI for conversational agents. ACM SIGMIS Database: the DATABASE for Advances in Information Systems 55, 56–87 (2024). [109] Cai, C. J., Reif, E., Hegde, N., Hipp, J., Kim, B., Smilkov, D., Wattenberg, M., Viegas, F., Corrado, G. S., Stumpe, M. C. & Terry, M. Human-centered tools for coping with imperfect algorithms during medical decision-making. In CHI Conference on Human Factors in Computing Systems (2019). [110] Branley-Bell, D., Whitworth, R. & Coventry, L. User trust and understanding of ex- plainable AI: Exploring algorithm visualisations and user biases. In Human-Computer Interaction. Human Values and Quality of Life , 12183, 382–399 (2020). [111] Zhang, Y., Liao, Q. V. & Bellamy, R. K. E. Effect of confidence and explanation on accuracy and trust calibration in AI-assisted decision making. In Conference on Fairness, Accountability, and Transparency, 295–305 (2020). 33[112] Lancaster Farrell, C.-J. Explainability does not improve biochemistry staff trust in artificial intelligence-based decision support. Annals of Clinical Biochemistry 59, 447–449 (2022). [113] Panigutti, C., Beretta, A., Fadda, D., Giannotti, F., Pedreschi, D., Perotti, A. & Rinzivillo, S. Co-design of human-centered, explainable AI for clinical decision support. ACM Trans- actions on Interactive Intelligent Systems 13 (2023). [114] Leichtmann, B., Humer, C., Hinterreiter, A., Streit, M. & Mara, M. Effects of explainable artificial intelligence on trust and human behavior in a high-risk decision task. Computers in Human Behavior 139, 107539 (2023). [115] Bu¸ cinca, Z., Malaya, M. B. & Gajos, K. Z. To trust or to think: Cognitive forcing functions can reduce overreliance on AI in AI-assisted decision-making. ACM on Human-Computer Interaction 5, 188 (2021). [116] Lee, J. D. & See, K. A. Trust in automation: designing for appropriate reliance. Human factors 46, 50–80 (2004). [117] Green, B. & Chen, Y. The principles and limits of algorithm-in-the-loop decision making. ACM on Human-Computer Interaction 3, 50 (2019). [118] Lage, I., Chen, E., He, J., Narayanan, M., Kim, B., Gershman, S. J. & Doshi-Velez, F. Human evaluation of models built for interpretability. AAAI Conference on Human Computation and Crowdsourcing 7, 59–67 (2019). [119] Lai, V. & Tan, C. On human predictions with explanations and predictions of machine learning models. In Conference on Fairness, Accountability, and Transparency (2019). [120] Cai, C. J., Jongejan, J. & Holbrook, J. The effects of example-based explanations in a machine learning interface. In International Conference on Intelligent User Interfaces (2019). [121] Carton, S., Mei, Q. & Resnick, P. Feature-based explanations don’t help people detect misclassifications of online toxicity. International AAAI Conference on Web and Social Media 14 (2020). [122] Lai, V., Liu, H. & Tan, C. “Why is ’Chicago’ deceptive?” Towards building model-driven tutorials for humans. In CHI Conference on Human Factors in Computing Systems (2020). 34[123] Yang, F., Huang, Z., Scholtz, J. & Arendt, D. L. How do visual explanations foster end users’ appropriate trust in machine learning? In International Conference on Intelligent User Interfaces (2020). [124] Alqaraawi, A., Schuessler, M., Weiß, P., Costanza, E. & Berthouze, N. Evaluating saliency map explanations for convolutional neural networks. In International Conference on In- telligent User Interfaces (2020). [125] Wang, X. & Yin, M. Are explanations helpful? A comparative study of the effects of explanations in AI-assisted decision-making. In International Conference on Intelligent User Interfaces (2021). [126] Poursabzi-Sangdeh, F., Goldstein, D. G., Hofman, J. M., Wortman Vaughan, J. W. & Wallach, H. Manipulating and measuring model interpretability. In CHI Conference on Human Factors in Computing Systems (2021). [127] van der Waa, J., Nieuwburg, E., Cremers, A. & Neerincx, M. Evaluating XAI: A com- parison of rule-based and example-based explanations. Artificial Intelligence 291, 103404 (2021). [128] Kim, S. S. Y., Meister, N., Ramaswamy, V. V., Fong, R. & Russakovsky, O. Hive: Evaluat- ing the human interpretability of visual explanations. InEuropean Conference on Computer Vision, 13672 (2022). [129] Leichtmann, B., Hinterreiter, A., Humer, C., Streit, M. & Mara, M. Explainable artificial intelligence improves human decision-making: Results from a mushroom picking experi- ment at a public art festival. International Journal of Human–Computer Interaction 1–18 (2023). [130] M¨ uller, R., Reindel, D. F. & Stadtfeld, Y. D. The benefits and costs of explainable artifi- cial intelligence in visual quality control: Evidence from fault detection performance and eye movements. Human Factors and Ergonomics in Manufacturing & Service Industries (2024). [131] Metta, C., Beretta, A., Guidotti, R., Yin, Y., Gallinari, P., Rinzivillo, S. & Giannotti, F. Improving trust and confidence in medical skin lesion diagnosis through explainable deep learning. International Journal of Data Science and Analytics (2023). 35[132] Nagendran, M., Festor, P., Komorowski, M., Gordon, A. C. & Faisal, A. A. Quantifying the impact of AI recommendations with explanations on prescription decision making. npj Digital Medicine 6, 206 (2023). [133] Pimentel, M. A., Clifton, D. A., Clifton, L. & Tarassenko, L. A review of novelty detection. Signal Processing 99, 215–249 (2014). [134] Wang, Z., Bovik, A. C., Sheikh, H. R. & Simoncelli, E. P. Image quality assessment: From error visibility to structural similarity. IEEE Transactions on Image Processing 13, 600–612 (2004). [135] Huang, G., Liu, Z., van der Maaten, L. & Weinberger, K. Q. Densely connected con- volutional networks. In IEEE Conference on Computer Vision and Pattern Recognition , 4700–4708 (2017). [136] Peer, E., Vosgerau, J. & Acquisti, A. Reputation as a sufficient condition for data quality on Amazon Mechanical Turk. Behavior Research Methods 46, 1023–1031 (2013). [137] NASA. Nasa Task Load Index (TLX) (1986). URL https://humansystems.arc.nasa. gov/groups/TLX/downloads/TLX.pdf. [138] Davis, F. D. Perceived usefulness, perceived ease of use, and user acceptance of information technology. MIS Quarterly 13, 319–340 (1989). [139] Jian, J.-Y., Bisantz, A. M. & Drury, C. G. Foundations for an empirically determined scale of trust in automated systems. International Journal of Cognitive Ergonomics 4, 53–71 (2000). 36Acknowledgements The authors thank all participants, as well as Davide Vecchione, Burak Seyid, and Alexander Dierolf for enabling the manufacturing experiment at Siemens. SF acknowledges funding via Swiss National Science Foundation Grant 186932. TN acknowledges funding from Siemens; however, without competing interest. Author contributions All authors contributed to the research design, data analysis, interpretation of results, and writing of the paper. JS and BK performed the manufacturing experiment. SS performed the medical experiment. Competing interests The funding bodies had no control over design, conduct, data, analysis, review, reporting, or interpretation of the research conducted. Data and code availability All analyses were conducted using Python (3.11) with numpy (1.24.3), pandas (1.5.3), scipy (1.11.1), and statsmodels (0.14.0). The data visualizations were created with seaborn (0.12.2) and matplotlib (3.7.1). The data and code to reproduce the results from all studies will be made publicly available at https://osf.io/ upon publication. 37Supplements Contents A Extended literature review 40 A.1 Explainable AI in computer science . . . . . . . . . . . . . . . . . . . . . . . . . . 40 A.2 Explainable AI in behavioral science . . . . . . . . . . . . . . . . . . . . . . . . . 43 B Research setting 45 B.1 Manufacturing setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 B.2 Medical setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48 C Implementation of AI algorithm 50 C.1 Manufacturing setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50 C.2 Medical setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52 D Experimental interface 55 D.1 Manufacturing setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55 D.2 Medical setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56 E Randomization checks 58 E.1 Study 1: Manufacturing experiment . . . . . . . . . . . . . . . . . . . . . . . . . 58 E.2 Study 2: Medical experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58 F Robustness of the heatmap 60 G Results with precision as task performance metric 62 H Regression models 64 H.1 Study 1: Manufacturing experiment . . . . . . . . . . . . . . . . . . . . . . . . . 64 H.2 Study 2: Medical experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65 38I Analysis with excluded participants 67 I.1 Study 1: Manufacturing experiment . . . . . . . . . . . . . . . . . . . . . . . . . 67 I.2 Study 2: Medical experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68 J Experiment with non-experts 69 J.1 Results with precision as task performance metric . . . . . . . . . . . . . . . . . . 71 J.2 Randomization checks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72 J.3 Regression models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73 J.4 Analysis with excluded participants . . . . . . . . . . . . . . . . . . . . . . . . . . 74 K Preregistered hypotheses 76 L Post-experimental questionnaire 78 39Supplement A Extended literature review In the following, we provide an extended literature review of explainable artificial intelligence (AI). In particular, we differentiate research on explainable AI in computer science (which is primarily focused on methodological outcomes) from our work (which is focused on behavioral science outcomes). An overview is provided in Table S1. A.1 Explainable AI in computer science In the field of computer science, the primary objective concerning explainable AI is to develop and evaluate new methods to achieve better transparency of AI algorithms. For a general overview of explainable AI, see for example [9, 16, 76, 17]. AI algorithms can be broadly divided into two categories: algorithms that are considered to be inherently interpretable and algorithms that are not due to their complexity [8]. The latter are often referred to as black-box algorithms. An inherently interpretable model is linear regression, where the decision-making can be directly followed by inspecting the coefficients. Extensions of linear regression are generalized linear models (GLMs) and generalized additive models (GAMs) [77, 78]. GLMs were introduced as a unification of various methods that allow for different distributions of the dependent variable (e.g., a binary dependent variable as in logistic regression). GAMs were introduced to also allow for non-linear relationships between an independent and the dependent variable, which can be modeled for example with decision trees or shallow neural networks (see e.g., [79, 80]). While GLMs and GAMs are still considered to be inherently interpretable, they are not as straightforward to interpret as linear regression. Decision trees are also considered to be inherently interpretable by simply following the deci- sion rules from the root to the leaf nodes. Other inherently interpretable models are na¨ ıve Bayes classifier, k-nearest neighbor algorithm, rule-based learning, etc. (see [18] for an introduction). In contrast, post-hoc explanation techniques can be applied to better understand black-box algorithms such as neural networks. These explanation techniques are applied after the AI al- gorithm has been trained. Post-hoc explanation methods can be divided into global and local methods [76], where the former aim at explaining the algorithm’s overall decision-making process while the latter provide explanations for a single, specific input. An example of global methods are feature importance rankings, which rank the features based on their importance in predict- 40ing a model’s outcome, usually measured across the entire model rather than for individual predictions. A further example are partial dependence plots, which show the effect of a single feature on the predicted outcome of a model, averaged over a dataset. Prominent examples of local methods are local interpretable model-agnostic explanations ( LIME) and SHapley Addi- tive exPlanations ( SHAP) [19, 20]. LIME approximates a black-box model locally around the prediction with an interpretable model (like a linear model) to explain individual predictions. SHAP leverages a concept from cooperative game theory (Shapley values [81]) to explain the output of a model by computing the contribution of each feature to the prediction while also considering possible interaction effects. Post-hoc explanation methods can be further catego- rized into model-specific and model-agnostic methods. Model-specific methods are designed for a specific class of AI algorithms or even for a single AI algorithm. Model-specific methods exist, for example, for convolutional neural networks [22] or for kernel-based AI algorithms such as support vector machines [82]. In contrast, model-agnostic methods can be applied to any AI algorithm; notable examples are LIME and SHAP. Another dimension to differentiate post-hoc explanation methods is for which data type (tabular, text, audio, images, etc.) the method was developed. In this study, we focus on images, and, in the following, we thus present some of the most relevant post-hoc explanation methods for AI algorithms in computer vision. For general overviews of explainable AI in computer vision, we refer to [83, 56]. One of the earliest attempts to explain convolutional neural networks (CNN), which are nowadays widely used in computer vision, was made by Zeiler and Fergus [84]. Therein, the authors present the deconvolutional network ( DeconvNet) as a visualization method that maps feature activations back to the input image. Additionally, a simple technique called occlusion was discussed as a method for explaining the predictions of a CNN. For that, different portions of the input image are systematically occluded with a grey square, and the impact on the output of the network is observed. Significant changes in the output probabilities indicate the regions of the image most important for classification. In [21], a method was introduced for generating saliency maps by computing the gradient of the output category with respect to the input image. This technique highlights the regions of the image that contribute most to the model’s classification decision, offering a straightforward visual explanation of where the network is “looking” to make its predictions. Layer-wise relevance propagation (LRP) backtracks the output decision of the network 41through the layers to assign relevance scores to individual pixels. This method helps in under- standing which parts of the input image were most relevant for the model’s decision, emphasizing a layer-by-layer decomposition of the prediction [85]. Integrated gradients attribute the prediction of a neural network to its input features, cal- culating the gradients of the output prediction with respect to the input image. It integrates these gradients along the path from a baseline (zero input) to the actual input, offering a way to visualize the importance of each pixel [86]. Deep learning important features ( DeepLIFT) compares the activation of each neuron to its ‘reference activation’ and assigns contribution scores according to the difference. This method can identify which features of the input contribute to differences in the output from some baseline, offering a more detailed view than simple gradient-based methods [87]. By using the global average pooling layers in CNNs, class activation mapping ( CAM ) gener- ates heatmaps that highlight the discriminative parts of the image used by the network to identify specific classes, facilitating visual explanations of model decisions [88]. Several extensions to this approach exist [22, 89, 90], with GradCAM being one of the most often used approach [22]. In contrast to CAM, GradCAM employs a gradient-based approach to generate heatmaps, and, as a result, no changes to the network architecture are required. Counterfactual explanations provide insights by showing how a small change in the input image could change the classification result. This method helps in understanding model decisions by answering “what-if” scenarios, offering a direct way to comprehend how the model might react to different inputs [91]. A special case of post-hoc explanation in computer vision are anomaly heatmaps. These were developed for unsupervised computer vision AI algorithms, i.e., algorithms that do not require a labeled dataset but rather aim at finding anomalies automatically [47, 92, 93]. A plethora of post-hoc explanation methods exists and often it is not obvious which method to choose. Thus, different evaluation measures for post-hoc explanation methods have been proposed [94]. These include fidelity, which measures how accurately the explanations reflect the decisions of the underlying model [19, 95, 96, 50], and robustness, assessing stability under small changes in the input [97]. Another measure is human-interpretability, which examines how understandable the explanations are to humans [98]. But also application-grounded measures have been proposed, where the evaluation metric is how humans perform in a certain task [44, 99]. 42In another stream of literature in computer science, tools that help AI engineers with design- ing and training AI algorithms are developed. Especially, deep neural networks are difficult to train and require a certain amount of experience. Therefore, visualization tools have emerged that facilitate this task (e.g., see [100, 101, 102, 103]). A.2 Explainable AI in behavioral science In behavioral science, different outcomes of human-AI collaboration have been studied. Human delegation of tasks and decisions to AI algorithms has been intensively studied recently [59, 23, 24, 104]. Specifically, it has been examined whether the use of explainable AI can increase the likelihood of humans delegating decisions to AI algorithms [25]. Algorithm aversion refers to the phenomenon where humans are reluctant to use algorithms [26, 27, 28, 29, 105, 106, 107, 60, 61, 62, 63, 64]. To overcome human aversion towards AI algorithms, it has been hypothesized that providing an explanation of an AI’s decision may be beneficial. This has been tested with mixed findings [30, 108]. Missing trust in an AI’s decision can lead to algorithm aversion [31]. Therefore, previous studies investigated whether explainable AI can increase the trust in AI algorithms [109, 32, 110, 111, 112, 33, 113, 114, 45]. A contrary phenomenon to algorithm aversion is overreliance [115, 35], where humans place too much trust in AI algorithms, potentially overlooking or ignoring their limitations. Previous research has found mixed results on whether explainability of AI leads to decreased overreliance [34, 35, 36]. However, for a good task performance, it is crucial that humans only adhere to correct AI predictions and overrule wrong ones, which is also referred to as appropriate reliance [116]. The effect of explainable AI on task performance has been studied previously. In the majority of studies, however, non-experts (e.g., via Amazon Mechanical Turk or university students) were recruited and those oftentimes performed simplified, non-realistic, or even non-relevant tasks (see, e.g., [117, 118, 119, 120, 37, 121, 122, 123, 38, 111, 124, 125, 126, 127, 39, 34, 115, 128, 36, 129, 130]). It has been hypothesized that non-experts can not fully harness explanations due to a lack of domain knowledge [36]. Thus, an empirical evaluation of the effect of explainable AI on task performance in real job tasks, requires actual domain experts of those tasks [94]. Previous studies that recruited domain experts to perform real-job tasks have other draw- backs. For example, prior work has compared the effect of explainable AI against humans 43alone [41, 42]. Others have used expert annotations as a proxy for explainable AI [43] or re- search designs that prevent isolating the treatment effect of explainable AI on task performance [44, 46, 45]. Finally, also other outcomes have been studied such as trust, confidence, and per- ceived usefulness [131, 132], while, as our novelty, we add by focusing on task performance with domain experts. Table S1: Overview of key literature on explainable AI. Domain Concept Research summary Dependent variable References (examples) Computer science New explana- tion methods Derivations of new meth- ods where the focus is on mathematical / algorith- mic contributions n/a [77, 78, 79, 80, 19, 20, 82, 84, 21, 85, 86, 87, 88, 22, 89, 90, 91, 47, 92] Benchmarking methods/ datasets Proposing new methods or datasets to benchmark the performance of explainable AI n/a [95, 96, 50, 97, 98, 44, 99] New visualiza- tion tools Visualization tools that fa- cilitate the development of complex machine learning models n/a [100, 101, 102, 103] Behavioral science Delegation be- tween humans and AI Humans avoid delegation to algorithms Delegation frequency [59, 23, 24, 104, 25] Algorithm aversion Humans reject advice from algorithm Adherence [26, 27, 28, 29, 105, 106, 107, 60, 61, 62, 63, 64, 30, 108] Trust in AI Humans do not trust AI al- gorithms Trust [109, 32, 110, 111, 112, 33, 113, 114, 45] Overreliance on AI Humans follow advice from algorithms blindly Overreliance [34, 35, 36] Task per- formance in response to explainable AI Comparison of black-box AI vs explainable AI for task performance using unrealistic tasks, non- experts, or non-causal research designs Task per- formance [117, 118, 119, 120, 37, 121, 122, 123, 38, 111, 124, 125, 126, 127, 39, 34, 115, 128, 36, 129, 130, 41, 42, 43, 44, 46, 45] Real-world job tasks with domain experts for esti- mating treatment effects of explainable AI vs black- box AI Task per- formance ours 44Supplement B Research setting B.1 Manufacturing setting Poor quality generates 10% to 15% of the operating expenses in manufacturing. 1 Identifying defective products before they move downstream in the value chain is essential to maintain a high operational performance. For this purpose, manufacturers conduct visual quality inspec- tions to assess whether products have defects (e.g., assembly errors or surface damages) [47]. In manufacturing operations, many quality inspections are still conducted manually, which is often a tedious, tiring, and error-prone task. AI offers promising opportunities to overcome these drawbacks by supporting factory workers in automatically detecting quality defects before prod- ucts are sold to customers. Specifically, AI can assist workers in detecting the location and type of error so rework can be conducted more effectively and efficiently. Therefore, AI algorithms enable factory workers to be more productive by focusing on their key value-creating work tasks. Our research was carried out at Siemens Smart Infrastructure in Zug, Switzerland. To test our hypotheses, the company provided us with real-world product images (each with 1920×1080 pixels) from their factory. The images comprise four different types of electronic products, all of which are printed circuit boards. Figure S1 shows example images of the four types of electronic products that were inspected during the experiment. Figure S2 shows three examples of quality defects, which include products with wrong components, products with assembly errors, and products with faulty components. Overall, we received two datasets. The first dataset comprised 200 images, including 43 correct products and 7 defective products for each of the four product types. All experiments (and thus the empirical results in the main analysis) are based on the first dataset. The second dataset comprised 200 additional faultless images (50 for each of the four product types). These images were used to train the AI algorithm that was used to compute the quality scores in the experiment (see Supplement C). 1American Society for Quality. Cost of Quality (COQ) . URL: https://asq.org/quality-resources/cost-of- quality, last accessed on June 13, 2024. 45A  B C  D Figure S1: Four types of electronic products (printed circuit boards) . ( A-D) Exem- plary images of faultless products that were inspected during the experiment. 46A B C Figure S2: Examples of quality defects. (A) Example of a defective product with wrong components. ( B) Example of a defective product with a component assembled in the wrong orientation. (C) Example of a defective product with a faulty component. 47B.2 Medical setting Chest radiography (capturing X-ray images) is a widely performed diagnostic imaging test across the world and plays a crucial role in the screening, diagnosis, and management of numerous diseases that pose a threat to life [74]. One of such diseases are lung lesions, which include lung nodules and masses in our experiment. Lung nodules are common and are encountered on roughly one out of 500 chest X-ray images [72]. Overlooking a lung lesion on a chest X-ray can have serious, potentially life-threatening con- sequences for patients. The failure to detect a lesion at an early stage can lead to a delay in diagnosis and treatment, allowing diseases to progress to more advanced stages. This can sig- nificantly worsen the prognosis for conditions such as lung cancer, tuberculosis, and pneumonia, where early intervention can often lead to better outcomes. Beyond the immediate health risks, there are also implications for patient care, including increased medical costs due to more com- plex and prolonged treatment that may become necessary as a disease progresses. However, subtle lung lesions can be easily overlooked even without any constraints on how long radiol- ogists are allowed to inspect the chest X-ray image [73]. Given these reasons, identifying lung lesions on chest X-ray images is an important, non-trivial task in daily, medical care. To that end, giving physicians a decision aid for this task is crucial. Example chest X-ray images including the corresponding heatmaps are provided in Figure S3. 48A  B C  D Figure S3: Example chest X-ray images with corresponding heatmaps . ( A) Chest X-ray image without lung lesions. ( B) Heatmap overlaid over the chest X-ray image from A. (C) Chest X-ray image with a lung lesion annotated in red by an experienced radiologist. (D) Heatmap overlaid over the chest X-ray image from C. 49Supplement C Implementation of AI algorithm C.1 Manufacturing setting As part of this research, we implemented an AI algorithm that provided the predictions (i.e., quality scores) that were shown to the participants during the manufacturing experiment. Our AI algorithm builds upon unsupervised anomaly detection [133] and, as such, follows common standards in industry for visually analyzing the quality of product images [47, 48]. Algorithms based on unsupervised anomaly detection are particularly suitable for industrial settings because they only require a set of faultless product images to be trained. Therefore, there is no need to specify defect types beforehand, which also allows identifying quality defects that have never been observed before. This is reflected in anomaly detection where product images with sufficiently large deviations from “normal” products are labeled as defective. AI algorithm. In our case, the AI algorithm performs unsupervised anomaly detection as follows [133]. First, we are given an existing training set T with images of faultless products. Upon deployment, an out-of-sample product image x is subject to assessment; that is, whether it is similar to any of the images t ∈ T and thus likely faultless or whether it is highly dissimilar and thus likely defective. Here, anomaly detection compares the similarity (with regard to some similarity function d) between the new image x and the existing images t ∈ T. For each, a similarity d(x, t), for all t ∈ T is computed. If the similarity d falls below a certain threshold θ∗, an image is labeled as defective. AI predictions. The similarity of product images is computed by following best practice in computer vision. As such, we refrain from simply computing the L2-norm (or some other norm) between x and t. The reason is that such distance would give equal weight to all pixels and cannot properly account for the semantic similarity in images. Rather, we follow established practice and compute the similarity via the so-called structural similarity index [134]. The structural similarity index is a standard computer vision method for quantifying the similarity of images between 0 (i.e., no similarity at all) and 1 (i.e., perfect similarity). Product images with a low structural similarity indicate an increased probability of a quality defect because they are less similar to the training data (i.e., images of faultless products). For details on the computation of the structural similarity index, we refer to [134]. Eventually, we scaled the structural similarity 50index of all images between 0 and 100 and rounded the values to the nearest integer to enhance readability. The resulting similarity measure corresponds to the quality scores that were shown to the participants in the experiment. Prediction performance. We evaluated the out-of-sample prediction accuracy of the AI al- gorithm as follows. In a first step, we mapped the quality scores onto a binary faultless/defective label. For this, we introduced a quality score of θ∗ = 90 as a cutoff (i.e., predicting that a prod- uct is defective if the quality score is below 90 and faultless otherwise). We then compared the predictions of the algorithm against the ground-truth quality labels provided by Siemens. Ta- ble S2 gives the confusion matrix for the 200 out-of-sample images used in the experiment. We measure the prediction performance via the balanced accuracy (i.e., average sensitivity across faultless and defective products). We choose the balanced accuracy as our main performance metric because it accounts for the unbalanced distribution between faultless and defective prod- ucts (i.e., 172 products are faultless and 28 products are defective). The standalone AI algorithm achieves a balanced accuracy of 95.6% (i.e., 0.5 ×[169/172 + 26/28]). Additionally, we evaluated the defect detection rate (true negative rate) of the AI algorithm, i.e., how many of the defective products were identified as such. The defect detection rate of the standalone AI algorithm was 92.9% (26/28). Table S2: Confusion matrix comparing AI predictions with ground-truth labels in the manufacturing setting Predicted label Actual label Faultless Defective Faultless 169 3 Defective 2 26 Explainable AI. We extended the above AI algorithm to produce explanations for each prediction as follows. We followed other research in computer vision that generates so-called “anomaly heatmaps” [47]. Anomaly heatmaps visualize in what area of an image a quality defect is predicted to be. Formally, in an anomaly heatmap, each pixel xi is associated with a score measuring the likelihood of a defect at that location. Pixels that receive a bright color (yellow, orange, red, etc.) correspond to “anomalous” regions because they have a large distance to the training data (i.e., the pixel is dissimilar to the one in a faultless product). In contrast, 51pixels that are colored in blue have a small distance to the training data and should thus be considered as “normal.” For better usability, we overlay the anomaly heatmap over the actual product image (with partially transparent colors). Examples of two anomaly heatmaps are shown in Figure S4. In the experiment, the heatmaps were shown to the participants in the explainable AI treatment arm as an additional decision aid. A B Figure S4: Anomaly heatmaps for AI predictions . ( A) Example anomaly heatmap for a faultless product. ( B) Example anomaly heatmap for a defective product. C.2 Medical setting AI algorithm. We used an already trained DenseNet121 from [50]. DenseNet121 is a convolu- tional neural network that is part of the DenseNet family, known for its dense connectivity pattern where each layer is connected to every other layer in a feed-forward fashion [135]. The “121” in DenseNet121 stands for the total number of layers in the network, including convolutional layers, pooling layers, and fully connected layers, summing up to 121. The DenseNet121 was set up as a multi-label classifier, which takes a chest X-ray image as input and outputs probabilities for the following 10 labels: airspace opacity, atelectasis, cardiomegaly, consolidation, edema, enlarged 52cardiomediastinum, lung lesion, pleural effusion, pneumothorax, and support devices. It was trained on 224,316 chest X-ray images from 65,240 patients. AI predictions. The probabilities returned by the DenseNet121 were mapped onto binary yes/no labels by finding the probability threshold that maximized the balanced accuracy on a validation set of chest X-ray images, which were not used during training. In order to have a score identical to the quality score from the manufacturing setting, where a smaller score indicates a greater likelihood of showing a defect and with a cutoff of 90 that divides the quality scores into defective and faultless, the following transformations were performed: (i) the probabilities were inverted, (ii) the threshold that divides the two classes was set to 90, (iii) the inverted probabilities larger than that threshold were rescaled using min-max scaling on a scale from 90 to 100, and (iv) the inverted probabilities smaller than that threshold were rescaled using min-max scaling on a scale from 0 to 90. Prediction performance. As in the manufacturing setting, we evaluate performance of the AI algorithm on the 50 chest X-ray images by calculating the balanced accuracy and the disease detection rate. Those 50 images were neither used for training nor for finding the class dividing threshold. The standalone AI algorithm achieved a balanced accuracy of 82.2% (i.e., 0.5 ×[40/43 + 5/7]) and a disease detection rate of 71.4% (i.e., 5 /7). Additionally, the confusion matrix for the 50 images is shown in Table S3. Table S3: Confusion matrix comparing AI predictions with ground-truth labels in the medical setting Predicted label Actual label No lung lesion At least one lung lesion No lung lesion 40 3 At least one lung lesion 2 5 Explainable AI. As explanation technique for the above AI algorithm, we used GradCAM [22]. GradCAM outputs heatmaps similar to the anomaly heatmaps from the manufacturing setting and showed state-of-the-art localization performance on chest X-ray images across a variety of diagnoses [50]. Analogous to the manufacturing setting, pixels with bright colors (yellow, orange, red, etc.) correspond to regions that were most relevant for predicting lung lesions, whereas blue pixels were least relevant. To increase usability, heatmaps were overlaid 53over the raw chest X-ray images with partially transparent colors. Examples of two heatmaps next to the original chest X-ray images are shown in Figure S3. Heatmaps were only provided to radiologists in the explainable AI treatment arm. 54Supplement D Experimental interface D.1 Manufacturing setting The experiment was carried out via a computer interface that was analogously designed to the real-world quality inspection setup at Siemens. The experiment comprises the following steps: (1) the study description and study consent, (2) a tutorial on how to use the application, (3) the visual inspection task involving 200 images, (4) a post-experimental questionnaire. Depending on the randomly assigned treatment, different versions of the quality inspection interface were shown to participants (Figure S5). Similar to the real-world setting at Siemens, all participants had access to a reference image, which showed a faultless product. The partic- ipants were asked to evaluate each of the 200 images individually and to make an “approve” (faultless product) or “reject” (defective product) decision by clicking the respective buttons. This represents the quality assessments that we use for all analyses. The participants were al- lowed to change their quality assessment before submitting their decision and proceeding to the next image. Once a decision was submitted, participants could no longer return to the previous image. Overall, the participants were given 35 minutes to solve the task, which corresponds to realistic field conditions. The remaining time was always shown on the top of the interface. We tracked several metrics during the experiment. In the tutorial, we tracked whether par- ticipants were following the steps correctly and screened out those that did not complete the tutorial successfully. During the visual inspection task, we tracked the final quality assessment (i.e., faultless or defective) and the decision speed of the users. In the post-experimental ques- tionnaire, we saved the answers to individual questions. The aggregated user data were stored in a database and later converted into a comma-separated values (CSV) file. 55A  B C D Figure S5: Different interfaces depending on treatment arm. (A) The interface for the black-box AI. (B) The interface for the explainble AI. ( C) The interface for the human without AI treatment. ( D) The interface for the post-experimental questionnaire. D.2 Medical setting The experiment was conducted via Qualtrics. The experiment was divided in the following steps: (1) physician confirmation and study consent, (2) a tutorial on how to perform the experiment, (3) the visual inspection task consisting of 50 chest X-ray images, and (4) a post-experimental questionnaire. A different version of the chest X-ray inspection interface was shown to radiologists depending on the randomly assigned treatment (Figure S6). For both treatment arms, the chest X-ray image to inspect was shown on the left. To the right of it, an enlarged view was shown, which could be altered by moving the mouse over the chest X-ray image. Radiologists in the treatment arm with explainable AI additionally received a heatmap, which was displayed right of the enlarged view. The radiologists were asked to inspect 50 chest X-ray images and to answer the question “Is at least one lung lesion visible in the chest X-ray image below?” with either “YES” or “NO” for each image. The radiologists were allowed to change their assessment before submitting their decision (clicking the blue arrow button to proceed to the next page). Each chest X-ray image was shown on a separate page and radiologists were not allowed to go back to a previous image 56once a decision was submitted. The radiologists had 35 minutes to complete the task and the remaining time was always shown on the top-left of the page. Several metrics were recorded during the experiment: In the tutorial, we tracked whether ra- diologists understood how to perform the task. In the visual inspection task, the final assessment for each image as well as the corresponding decision speed were recorded. In the post-hoc ques- tionnaire, we saved the answers to the individual questions. The data was stored on Qualtrics and exported as a CSV file. A B Figure S6: Different interfaces depending on treatment arm. (A) The interface for the black-box AI. (B) The interface for the explainble AI. 57Supplement E Randomization checks E.1 Study 1: Manufacturing experiment We performed randomization checks to confirm that the distribution of workers in the two treat- ment arms of the manufacturing experiment was unbiased. The following demographic variables were collected: age bracket [ <20, 20–30, 30–40, 40–50, 50–60, 60–70, >70], gender [male, fe- male, not listed], and highest level of education [ISCED1, ISCED2, ISCED3, ISCED4, ISCED5, ISCED6, ISCED7].2 We further collected the participant-specific tenure atSiemens measured in years since start of employment. Table S4 reports the observed frequencies and the mean tenure (with standard deviation in parentheses) for both treatment arms. The randomization checks for age, gender, and education are based on X2-tests of independence. The randomization check for tenure is based on a two-sided Welch’s t-test. The results suggest no statistically significant differences between the participants in the two treatment arms. Table S4: Randomization checks for manufacturing experiment Human with black-box AI Human with explainable AI P-value Age 0 |1 |7 |8 |4 |2 |0 0 |1 |3 |9 |12 |1 |0 0.223 Gender 16 |6 |0 17 |9 |0 0.815 Education 0 |9 |4 |2 |3 |3 |1 0 |14 |2 |3 |3 |3 |1 0.897 Tenure 11.91 (8.83) 15.38 (10.42) 0.217 Observations 22 26 – Notes: The table reports the frequency of participants that fall in the specific subgroups of age, gender, and education (separated by vertical bars) and the average tenure per treatment arm (standard deviation in parentheses). The P-values for the randomization checks are computed based on X2-tests of independence (age, gender, education) and a two-sided Welch’s t-test (tenure). E.2 Study 2: Medical experiment We performed a randomization check to confirm that the distribution of radiologists with respect to tenure in the two treatment arms of the medical experiment was unbiased. The randomization check is based on a two-sided Welch’s t-test. The result suggests no statistically significant differences between the radiologists in the two treatment arms. 2UNESCO. International Standard Classification of Education (ISCED) . URL: http://uis.unesco.org/en/topic/international-standard-classification-education-isced, last accessed on June 13, 2024. 58Table S5: Randomization checks for medical experiment Human with black-box AI Human with explainable AI P-value Tenure 11.89 (8.96) 15.4 (11.71) 0.08 Observations 61 52 – Notes: The table reports the average tenure per treatment arm (standard deviation in parentheses). The P-value is computed based on a two-sided Welch’s t-test. 59Supplement F Robustness of the heatmap We applied two additional algorithms to generate the heatmaps in the medical setting in order to show that different algorithms lead to similar heatmaps. In particular, we compared our heatmaps generated by GradCAM to heatmaps generated by DeepLIFT and LRP (for an in- troduction to these two methods see Supplement A) [87, 85]. Figure S7 shows the heatmaps generated by the three distinct algorithms for the eight chest X-ray images, where the AI algo- rithm predicted that lung lesions are visible. Additionally, we calculated Pearson correlation coefficients between the heatmaps generated by different algorithms to quantify whether they highlight similar regions. The Pearson correla- tion coefficient is calculated via r = cov(xi,xj)√ Var(xi) Var(xj) , where xi and xj denote the flattened array of heatmaps generated by algorithm i and j. The average Pearson correlation coefficient between GradCAM and DeepLIFT is r = 0.92, and the average Pearson correlation coefficient between GradCAM and LRP is r = 0.63. The exact Pearson correlation coefficient for each heatmap pair is reported in Figure S7. The Pearson correlation coefficient was statistically significant for each pair of heatmaps ( P < 0.001). In general, we observe that all three algorithms produce similar heatmaps. The heatmaps we used in our medical setting (generated by GradCAM) are more similar to the heatmaps generated by DeepLIFT than to those generated by LRP. 60Figure S7: Heatmaps generated by three different algorithms. The left column shows the heatmaps generated by GradCAM (the algorithm we used for our medical experiment). The middle columns shows the heatmaps generated by DeepLIFT. The right column shows the heatmaps generated by LRP. r denotes the Pearson correlation coefficient between the heatmaps generated by GradCAM (the algorithm we used) and DeepLIFT/LRP, respectively. 61Supplement G Results with precision as task performance metric In addition to the balanced accuracy and defect detection rate, we also report precision as a metric for task performance of the participants in combination with the defect/disease detec- tion rate. Formally, precision is computed via TN /PN with true negatives TN and predicted negatives PN . We again compare the effect of augmenting humans with explainable AI versus black-box AI. Figure S8 reports the results for the manufacturing experiment (Study 1) and Figure S9 for the medical experiment (Study 2) /uni0000002b/uni00000058/uni00000050/uni00000044/uni00000051/uni00000003/uni0000000e/uni00000003/uni00000045/uni0000004f/uni00000044/uni00000046/uni0000004e/uni00000010/uni00000045/uni00000052/uni0000005b/uni00000003/uni00000024/uni0000002c/uni0000002b/uni00000058/uni00000050/uni00000044/uni00000051/uni00000003/uni0000000e/uni00000003/uni00000048/uni0000005b/uni00000053/uni0000004f/uni00000044/uni0000004c/uni00000051/uni00000044/uni00000045/uni0000004f/uni00000048/uni00000003/uni00000024/uni0000002c/uni00000013/uni00000008 /uni00000015/uni00000013/uni00000008 /uni00000017/uni00000013/uni00000008 /uni00000019/uni00000013/uni00000008 /uni0000001b/uni00000013/uni00000008 /uni00000014/uni00000013/uni00000013/uni00000008 /uni0000001c/uni00000015/uni00000011/uni0000001c/uni00000008/uni00000027/uni00000048/uni00000049/uni00000048/uni00000046/uni00000057/uni00000003/uni00000047/uni00000048/uni00000057/uni00000048/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000055/uni00000044/uni00000057/uni00000048 t = 2.876/uni0000000d/uni0000000d(P = 0.003) /uni00000024 /uni0000002b/uni00000058/uni00000050/uni00000044/uni00000051/uni00000003/uni0000000e/uni00000003/uni00000045/uni0000004f/uni00000044/uni00000046/uni0000004e/uni00000010/uni00000045/uni00000052/uni0000005b/uni00000003/uni00000024/uni0000002c/uni0000002b/uni00000058/uni00000050/uni00000044/uni00000051/uni00000003/uni0000000e/uni00000003/uni00000048/uni0000005b/uni00000053/uni0000004f/uni00000044/uni0000004c/uni00000051/uni00000044/uni00000045/uni0000004f/uni00000048/uni00000003/uni00000024/uni0000002c/uni00000013/uni00000008 /uni00000015/uni00000013/uni00000008 /uni00000017/uni00000013/uni00000008 /uni00000019/uni00000013/uni00000008 /uni0000001b/uni00000013/uni00000008 /uni00000014/uni00000013/uni00000013/uni00000008 /uni0000001b/uni0000001c/uni00000011/uni0000001a/uni00000008/uni00000033/uni00000055/uni00000048/uni00000046/uni0000004c/uni00000056/uni0000004c/uni00000052/uni00000051 t = 1.674/uni00000003(P = 0.053) /uni00000025 /uni00000033/uni00000048/uni00000055/uni00000049/uni00000052/uni00000055/uni00000050/uni00000044/uni00000051/uni00000046/uni00000048/uni00000003/uni00000052/uni00000049/uni00000003/uni00000056/uni00000057/uni00000044/uni00000051/uni00000047/uni00000044/uni0000004f/uni00000052/uni00000051/uni00000048/uni00000003/uni00000024/uni0000002c/uni00000003/uni00000044/uni0000004f/uni0000004a/uni00000052/uni00000055/uni0000004c/uni00000057/uni0000004b/uni00000050 Figure S8: Results of manufacturing experiment. The boxplots compare the task perfor- mance between the two treatments: black-box AI and explainable AI. The task performance is measured by the defect detection rate (A) and the precision (B) based on the quality assessment of workers and the ground-truth labels of the product images. The standalone AI algorithm at- tains a defect detection rate of 92.9% and a precision of 89.7% (orange dashed lines). Statistical significance is based on a one-sided Welch’s t-test (***P <0.001, **P <0.01, *P <0.05). In the boxplots, the center line denotes the median; box limits are upper and lower quartiles; whiskers are defined as the 1.5x interquartile range. 62/uni0000002b/uni00000058/uni00000050/uni00000044/uni00000051/uni00000003/uni0000000e/uni00000003/uni00000045/uni0000004f/uni00000044/uni00000046/uni0000004e/uni00000010/uni00000045/uni00000052/uni0000005b/uni00000003/uni00000024/uni0000002c/uni0000002b/uni00000058/uni00000050/uni00000044/uni00000051/uni00000003/uni0000000e/uni00000003/uni00000048/uni0000005b/uni00000053/uni0000004f/uni00000044/uni0000004c/uni00000051/uni00000044/uni00000045/uni0000004f/uni00000048/uni00000003/uni00000024/uni0000002c/uni00000013/uni00000008 /uni00000015/uni00000013/uni00000008 /uni00000017/uni00000013/uni00000008 /uni00000019/uni00000013/uni00000008 /uni0000001b/uni00000013/uni00000008 /uni00000014/uni00000013/uni00000013/uni00000008 /uni0000001a/uni00000014/uni00000011/uni00000017/uni00000008/uni00000027/uni0000004c/uni00000056/uni00000048/uni00000044/uni00000056/uni00000048/uni00000003/uni00000047/uni00000048/uni00000057/uni00000048/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000055/uni00000044/uni00000057/uni00000048 t = 0.006/uni00000003(P = 0.498) /uni00000024 /uni0000002b/uni00000058/uni00000050/uni00000044/uni00000051/uni00000003/uni0000000e/uni00000003/uni00000045/uni0000004f/uni00000044/uni00000046/uni0000004e/uni00000010/uni00000045/uni00000052/uni0000005b/uni00000003/uni00000024/uni0000002c/uni0000002b/uni00000058/uni00000050/uni00000044/uni00000051/uni00000003/uni0000000e/uni00000003/uni00000048/uni0000005b/uni00000053/uni0000004f/uni00000044/uni0000004c/uni00000051/uni00000044/uni00000045/uni0000004f/uni00000048/uni00000003/uni00000024/uni0000002c/uni00000013/uni00000008 /uni00000015/uni00000013/uni00000008 /uni00000017/uni00000013/uni00000008 /uni0000001b/uni00000013/uni00000008 /uni00000014/uni00000013/uni00000013/uni00000008 /uni00000019/uni00000015/uni00000011/uni00000018/uni00000008/uni00000033/uni00000055/uni00000048/uni00000046/uni0000004c/uni00000056/uni0000004c/uni00000052/uni00000051 t = 2.235/uni0000000d(P = 0.014) /uni00000025 /uni00000033/uni00000048/uni00000055/uni00000049/uni00000052/uni00000055/uni00000050/uni00000044/uni00000051/uni00000046/uni00000048/uni00000003/uni00000052/uni00000049/uni00000003/uni00000056/uni00000057/uni00000044/uni00000051/uni00000047/uni00000044/uni0000004f/uni00000052/uni00000051/uni00000048/uni00000003/uni00000024/uni0000002c/uni00000003/uni00000044/uni0000004f/uni0000004a/uni00000052/uni00000055/uni0000004c/uni00000057/uni0000004b/uni00000050 Figure S9: Results of medical experiment. The boxplots compare the task performance between the two treatments: black-box AI and explainable AI. The task performance is mea- sured by the disease detection rate ( A) and the precision (B) based on the quality assessment of radiologists and the ground-truth labels of the chest X-ray images. The standalone AI algorithm attains a disease detection rate of 71.4% and a precision of 62.5% (orange dashed lines). Statis- tical significance is based on a one-sided Welch’s t-test (***P <0.001, **P <0.01, *P <0.05). In the boxplots, the center line denotes the median; box limits are upper and lower quartiles; whiskers are defined as the 1.5x interquartile range. 63Supplement H Regression models This section reports various regression models estimating the treatment effect of augmenting humans with explainable AI. The models are estimated via Yi = β0 + β1 Treatmenti + β2 Xi + εi, (S1) where Yi is the observed task performance (i.e., balanced accuracy or defect/disease detection rate), T reatmenti is a binary variable which equals 0 if participant i received the black-box AI treatment and 1 if participant i received the explainable AI treatment, and Xi is the vector of participant-specific control variables. The above regression is estimated via ordinary least squares (OLS). We acknowledge that the balanced accuracy is only defined between 0 and 100. Because OLS regression models could return values below 0 and above 100, we additionally estimate quasi-binomial regression models with a logit link function. For this, we set the scale parameter of the regression models to the Pearson X2-statistic divided by the residual degrees of freedom. H.1 Study 1: Manufacturing experiment Table S6 reports three OLS regression models estimating the treatment effect with different control variables. Model (1) estimates the treatment effect for explainable AI with demographic controls (age, gender, and highest level of education) and the tenure at Siemens measured in years from start of employment. Model (2) estimates the treatment effect for explainable AI with demographic controls, tenure, and self-reported IT skills (ranging from 1: “novice” to 5: “expert”). Model (3) estimates the treatment effect for explainable AI with demographic controls, tenure, self-reported IT skills, and the decision speed (median across the 200 images). All three models return a significant treatment effect for both metrics (balanced accuracy and defect detection rate) as dependent variables. 64Table S6: OLS regression results for treatment effect (manufacturing experiment) Balanced accuracy Defect detection rate Model (1) Model (2) Model (3) Model (1) Model (2) Model (3) Treatment 8.131*** 7.513*** 7.508** 11.783** 10.914** 10.888** (explainable AI) (2.087) (2.098) (2.117) (3.732) (3.790) (3.717) Demographics Yes Yes Yes Yes Yes Yes Tenure Yes Yes Yes Yes Yes Yes IT skills No Yes Yes No Yes Yes Decision speed No No Yes No No Yes Observations 48 48 48 48 48 48 Notes: The table reports three OLS regression models with different sets of control variables and two different metrics as dependent variables. The standard errors of the treatment effect are reported in parentheses. Statistical significance: ***P <0.001, **P <0.01, *P <0.05. Table S7 reports three quasi-binomial regression models estimating the treatment effect with the same control variables as before. Again, all three models return a significant treatment effect for both metrics as dependent variables. Table S7: Quasi-binomial regression results for treatment effect (manufacturing experiment) Balanced accuracy Defect detection rate Model (1) Model (2) Model (3) Model (1) Model (2) Model (3) Treatment 1.295*** 1.175** 1.184*** 1.157** 1.060** 1.089** (explainable AI) (0.340) (0.358) (0.357) (0.369) (0.386) (0.375) Demographics Yes Yes Yes Yes Yes Yes Tenure Yes Yes Yes Yes Yes Yes IT skills No Yes Yes No Yes Yes Decision speed No No Yes No No Yes Observations 48 48 48 48 48 48 Notes: The table reports three quasi-binomial regression models with different sets of control variables and two different metrics as dependent variables. The standard errors of the treatment effect are reported in parentheses. Statistical significance: ***P <0.001, **P <0.01, *P <0.05. H.2 Study 2: Medical experiment Table S8 reports three OLS regression models estimating the treatment effect with different control variables. Model (1) estimates the treatment effect for explainable AI with tenure mea- sured in years as a control variable. Model (2) estimates the treatment effect for explainable AI with tenure and self-reported IT skills (ranging from 1: “novice” to 5: “expert”). Model (3) estimates the treatment effect for explainable AI with tenure, self-reported IT skills, and the 65decision speed (median across the 50 images). All three models return a significant treatment effect for balanced accuracy as a dependent variable. Table S8: OLS regression results for treatment effect (medical experiment) Balanced accuracy Defect detection rate Model (1) Model (2) Model (3) Model (1) Model (2) Model (3) Treatment 4.637* 4.452* 4.473* 0.129 0.304 0.645 (explainable AI) (1.834) (1.853) (1.863) (2.286) (2.312) (2.215) Tenure Yes Yes Yes Yes Yes Yes IT skills No Yes Yes No Yes Yes Decision speed No No Yes No No Yes Observations 113 113 113 113 113 113 Notes: The table reports three OLS regression models with different sets of control variables and two different metrics as dependent variables. The standard errors of the treatment effect are reported in parentheses. Statistical significance: ***P <0.001, **P <0.01, *P <0.05. Table S9 reports three quasi-binomial regression models estimating the treatment effect with the same control variables as before. Again, all three models return a significant treatment effect for balanced accuracy as dependent variable. Table S9: Quasi-binomial regression results for treatment effect (medical experi- ment) Balanced accuracy Defect detection rate Model (1) Model (2) Model (3) Model (1) Model (2) Model (3) Treatment 0.308* 0.296* 0.297* 0.015 0.035 0.073 (explainable AI) (0.120) (0.121) (0.122) (0.264) (0.268) (0.259) Tenure Yes Yes Yes Yes Yes Yes IT skills No Yes Yes No Yes Yes Decision speed No No Yes No No Yes Observations 113 113 113 113 113 113 Notes: The table reports three quasi-binomial regression models with different sets of control variables and two different metrics as dependent variables. The standard errors of the treatment effect are reported in parentheses. Statistical significance: ***P <0.001, **P <0.01, *P <0.05. 66Supplement I Analysis with excluded participants In our data analyses, we followed our preregistration and excluded participants who did not finish the task in time or participants with obvious misbehavior. Specifically, six and two participants were excluded from Study 1 and Study 2, respectively, because they did not finish the task in time. Further, in Study 1, we excluded participants who did not label a single product as defective (which corresponds to one participant) and in Study 2, radiologists were excluded if they assigned only one label to all chest X-ray images (which corresponds to 1 radiologist). Further, participants whose performance was more than three standard deviations worse than the mean of their respective treatment arm were excluded (which corresponds to one worker in Study 1 and two radiologists in Study 2). This section repeats the OLS regression from the main paper (without control variables) with participants who were excluded due to obvious misbehavior. Overall, we arrive at consistent findings. Table S10: Excluded participants across treatment arms Study 1: Manufacturing Study 2: Medical Black-box AI Explainable AI Black-box AI Explainable AI Time-out 5 1 0 2 No defective 1 0 – – Single label – – 1 0 Worse than 3σ 0 1 1 1 I.1 Study 1: Manufacturing experiment Table S11 reports the OLS regression model estimating the treatment effect for all different combinations of exclusion criteria. As in our main analysis, the effect of explainable AI is statistically significant for both metrics, balanced accuracy and defect detection rate. The only exception is for the defect detection rate when workers that timed-out and did not label a single product as defective were excluded while including those that were worse than three standard deviations than the mean. 67Table S11: OLS regression results with excluded participants (manufacturing ex- periment) Excluded: Observations Balanced accuracy Defect detection rate Time-out No defective Worse than 3 σ ✗ ✗ ✗ 56 8.143** (2.754) 11.590* (4.965) ✓ ✗ ✗ 50 7.944* (3.016) 12.192* (5.472) ✗ ✓ ✗ 55 6.769** (2.432) 8.651* (4.077) ✗ ✗ ✓ 54 8.120*** (2.042) 10.961** (3.391) ✓ ✓ ✗ 49 6.253* (2.642) 8.628 (4.477) ✓ ✗ ✓ 48 7.653** (2.178) 11.014** (3.680) ✗ ✓ ✓ 54 8.120*** (2.042) 10.961** (3.391) Notes: The table reports the OLS regression model with two different metrics as dependent variables for all combinations of exclusion criteria. The standard errors of the treatment effect are reported in parentheses. Statistical significance: ***P <0.001, **P <0.01, *P <0.05. I.2 Study 2: Medical experiment Table S12 reports the OLS regression model estimating the treatment effect for all different combinations of exclusion criteria. As in our main analysis, the effect of explainable AI is statistically significant for balanced accuracy, irrespective of the exclusion criteria. Only when radiologists that assigned one label to all chest X-ray images are excluded the treatment effect for balanced accuracy was not statistically significant. For the disease detection rate, the treatment effect was not statistically significant in the main analysis. This did not change when different exclusion criteria are considered. Table S12: OLS regression results with excluded participants (medical experiment) Excluded: Observations Balanced accuracy Disease detection rate Time-out Single label Worse than 3 σ ✗ ✗ ✗ 118 4.418* (2.067) -0.282 (2.337) ✓ ✗ ✗ 116 5.266** (1.996) 0.543 (2.274) ✗ ✓ ✗ 117 3.967 (2.026) -0.121 (2.349) ✗ ✗ ✓ 115 4.988** (1.846) -0.256 (2.212) ✓ ✓ ✗ 115 4.815* (1.950) 0.704 (2.286) ✓ ✗ ✓ 114 5.162** (1.857) -0.168 (2.232) ✗ ✓ ✓ 114 4.520* (1.790) -0.102 (2.224) Notes: The table reports the OLS regression model with two different metrics as dependent variables for all combinations of exclusion criteria. The standard errors of the treatment effect are reported in parentheses. Statistical significance: ***P <0.001, **P <0.01, *P <0.05. 68Supplement J Experiment with non-experts To extend our findings to non-experts, we conducted a third experiment with participants re- cruited via Amazon MTurk to perform the visual inspection task in the manufacturing setting. We chose the manufacturing task for this because, in principle, non-experts could compare elec- tronic products against a reference image (a faultless product looks always identical). For chest X-ray images, this is hardly possible since these can look very different across different healthy patients. We followed common practice by only admitting MTurk workers with an approval rat- ing above 95% [136]. We prevented double participation by tracking the IP of participants. The participants received a base compensation ( $5) and had the opportunity to earn a performance- dependent bonus proportional to the correctly labeled quality defects ( $3). Participants were randomly assigned to one of the following three treatments: (a) human with black-box AI, (b) human with explainable AI, and (c) human without AI. Following the preregistration, we aimed to include approximately 600 participants excluding dropouts. We thus recruited 861 participants (U.S. residents) who started the study between July 19 and July 21, 2021. Out of them, 117 participants did not complete the study; 152 failed the tutorial; 92 did not finish on time; and 70 participants were excluded due to obvious misbehavior. The final sample consisted of 430 participants, out of which 288 were assigned to treatment arms (a) or (b), performing N = 57, 600 assessments of electronic products. We found that participants supported by explainable AI reached a higher task performance than the participants supported by black-box AI across both metrics (Figure S10). Partici- pants with black-box AI treatment only achieved a balanced accuracy with a mean of 81.4%, whereas participants with explainable AI treatment achieved a balanced accuracy with a mean of 87.6%. We then estimated the overall treatment effect on the task performance by regressing the balanced accuracy on the treatment (black-box AI = 0, explainable AI = 1). The regres- sion results suggest that the treatment effect of explainable AI is statistically significant and large (β = 6.252, SE = 1.733, P < 0.001); that is, an improvement of 6.3 percentage points. Accordingly, participants equipped with explainable AI achieved a higher defect detection rate with mean of 77.7% compared to participants with black-box AI with a mean of 66.4%. Again, the regression results showed a large and statistically significant treatment effect of explainable AI (β = 11.271, SE = 3.276, P = 0.001). The regression results remain statistically significant 69for both metrics when including relevant control variables (demographics, self-reported IT skills, and decision speed) in the regression model (Supplement J.3). We additionally compared how humans without AI support performed relative to humans with black-box AI or explainable AI. For this, we further recruited 142 participants and assigned them to a third treatment: human without AI. Here, participants only got images of the to- be-inspected products and the corresponding reference images of faultless products, but not the AI-based quality scores or the heatmaps. We found that participants without AI support only achieved a balanced accuracy with a mean of 72.4% (Figure S10) and were significantly outperformed by participants with both black-box AI ( t = 5.507, P < 0.001) and explainable AI ( t = 9 .017, P < 0.001). Similar results were found for the defect detection rate, where participants without AI achived a mean of 53.6% and were outperformed by participants with both black-box AI ( t = 5.202, P <0.001) and explainable AI ( t = 8.733, P <0.001). We further explored whether the performance difference between the treatments (black-box AI versus explainable AI) was associated with adherence to AI predictions. For this, we compared how likely participants were to follow quality scores that were accurate (i.e., the AI prediction for the inspected product was correct). The results suggest that participants with explainable AI were more likely to adhere to accurate quality scores than participants with black-box AI (mean = 92.9% for black-box AI, mean = 95 .2% for explainable AI). Overall, participants sup- ported by black-box AI were 47.9% more likely to erroneously overrule an AI prediction, despite the prediction being accurate ( t = 2.377, P = 0.009). We also analyzed whether participants were able to identify and overrule AI predictions that were wrong. Here, we found that partic- ipants supported by black-box AI only overruled 65.8% of the wrong AI predictions, whereas participants supported by explainable AI overruled 79.1% of the wrong AI predictions. The difference between both treatments is statistically significant ( t = 4.563, P <0.001). Evidently, explainable AI gives a powerful decision aid: it made participants not only less averse to following accurate AI predictions but also helped them overrule wrong AI predictions. 70/uni0000002b/uni00000058/uni00000050/uni00000044/uni00000051/uni00000003/uni0000005a/uni0000004c/uni00000057/uni0000004b/uni00000052/uni00000058/uni00000057/uni00000003/uni00000024/uni0000002c/uni0000002b/uni00000058/uni00000050/uni00000044/uni00000051/uni00000003/uni0000000e/uni00000003/uni00000045/uni0000004f/uni00000044/uni00000046/uni0000004e/uni00000010/uni00000045/uni00000052/uni0000005b/uni00000003/uni00000024/uni0000002c/uni0000002b/uni00000058/uni00000050/uni00000044/uni00000051/uni00000003/uni0000000e/uni00000003/uni00000048/uni0000005b/uni00000053/uni0000004f/uni00000044/uni0000004c/uni00000051/uni00000044/uni00000045/uni0000004f/uni00000048/uni00000003/uni00000024/uni0000002c/uni00000013/uni00000008 /uni00000015/uni00000013/uni00000008 /uni00000017/uni00000013/uni00000008 /uni00000019/uni00000013/uni00000008 /uni0000001b/uni00000013/uni00000008 /uni00000014/uni00000013/uni00000013/uni00000008 /uni0000001c/uni00000018/uni00000011/uni00000019/uni00000008/uni00000025/uni00000044/uni0000004f/uni00000044/uni00000051/uni00000046/uni00000048/uni00000047/uni00000003/uni00000044/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c t = 3.605/uni0000000d/uni0000000d/uni0000000d(P = 0.0002) /uni00000024 /uni0000002b/uni00000058/uni00000050/uni00000044/uni00000051/uni00000003/uni0000005a/uni0000004c/uni00000057/uni0000004b/uni00000052/uni00000058/uni00000057/uni00000003/uni00000024/uni0000002c/uni0000002b/uni00000058/uni00000050/uni00000044/uni00000051/uni00000003/uni0000000e/uni00000003/uni00000045/uni0000004f/uni00000044/uni00000046/uni0000004e/uni00000010/uni00000045/uni00000052/uni0000005b/uni00000003/uni00000024/uni0000002c/uni0000002b/uni00000058/uni00000050/uni00000044/uni00000051/uni00000003/uni0000000e/uni00000003/uni00000048/uni0000005b/uni00000053/uni0000004f/uni00000044/uni0000004c/uni00000051/uni00000044/uni00000045/uni0000004f/uni00000048/uni00000003/uni00000024/uni0000002c/uni00000013/uni00000008 /uni00000015/uni00000013/uni00000008 /uni00000017/uni00000013/uni00000008 /uni00000019/uni00000013/uni00000008 /uni0000001b/uni00000013/uni00000008 /uni00000014/uni00000013/uni00000013/uni00000008 /uni0000001c/uni00000015/uni00000011/uni0000001c/uni00000008/uni00000027/uni00000048/uni00000049/uni00000048/uni00000046/uni00000057/uni00000003/uni00000047/uni00000048/uni00000057/uni00000048/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000055/uni00000044/uni00000057/uni00000048 t = 3.447/uni0000000d/uni0000000d/uni0000000d(P = 0.0003) /uni00000025 /uni00000033/uni00000048/uni00000055/uni00000049/uni00000052/uni00000055/uni00000050/uni00000044/uni00000051/uni00000046/uni00000048/uni00000003/uni00000052/uni00000049/uni00000003/uni00000056/uni00000057/uni00000044/uni00000051/uni00000047/uni00000044/uni0000004f/uni00000052/uni00000051/uni00000048/uni00000003/uni00000024/uni0000002c/uni00000003/uni00000044/uni0000004f/uni0000004a/uni00000052/uni00000055/uni0000004c/uni00000057/uni0000004b/uni00000050 Figure S10: Results of non-experts experiment. The boxplots compare the task per- formance between humans without AI, with black-box AI, and with explainable AI. The task performance is measured by the balanced accuracy ( A) and the defect detection rate ( B) based on the quality assessment of participants and the ground-truth labels of the product images. A balanced accuracy of 50% provides a na¨ ıve baseline corresponding to a random guess (black dotted line). The standalone AI algorithm attains a balanced accuracy of 95.6% and a defect detection rate of 92.9% (orange dashed lines). Statistical significance is based on a one-sided Welch’s t-test (***P <0.001, **P <0.01, *P <0.05). In the boxplots, the center line denotes the median; box limits are upper and lower quartiles; whiskers are defined as the 1.5x interquartile range. We also assessed whether participants with explainable AI invested more time for the visual inspection task. For this, we compared participants’ median decision speeds across the 200 product images. No significant differences ( t = 0 .584, P = 0 .280) between both treatments (mean = 4 .61 s for black-box AI, mean = 4 .50 s for explainable AI) were observed. Hence, explainable AI improved task performance, but not at the cost of decision speed. J.1 Results with precision as task performance metric In Figure S11, the results with precision as task performance metric are shown. We find that non-experts augmented by explainable AI are more precise in identifying defective electronic products in comparison to peers supported by black-box AI. 71/uni0000002b/uni00000058/uni00000050/uni00000044/uni00000051/uni00000003/uni0000005a/uni0000004c/uni00000057/uni0000004b/uni00000052/uni00000058/uni00000057/uni00000003/uni00000024/uni0000002c/uni0000002b/uni00000058/uni00000050/uni00000044/uni00000051/uni00000003/uni0000000e/uni00000003/uni00000045/uni0000004f/uni00000044/uni00000046/uni0000004e/uni00000010/uni00000045/uni00000052/uni0000005b/uni00000003/uni00000024/uni0000002c/uni0000002b/uni00000058/uni00000050/uni00000044/uni00000051/uni00000003/uni0000000e/uni00000003/uni00000048/uni0000005b/uni00000053/uni0000004f/uni00000044/uni0000004c/uni00000051/uni00000044/uni00000045/uni0000004f/uni00000048/uni00000003/uni00000024/uni0000002c/uni00000013/uni00000008 /uni00000015/uni00000013/uni00000008 /uni00000017/uni00000013/uni00000008 /uni00000019/uni00000013/uni00000008 /uni0000001b/uni00000013/uni00000008 /uni00000014/uni00000013/uni00000013/uni00000008 /uni0000001c/uni00000015/uni00000011/uni0000001c/uni00000008/uni00000027/uni00000048/uni00000049/uni00000048/uni00000046/uni00000057/uni00000003/uni00000047/uni00000048/uni00000057/uni00000048/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000055/uni00000044/uni00000057/uni00000048 t = 3.447/uni0000000d/uni0000000d/uni0000000d(P = 0.0003) /uni00000024 /uni0000002b/uni00000058/uni00000050/uni00000044/uni00000051/uni00000003/uni0000005a/uni0000004c/uni00000057/uni0000004b/uni00000052/uni00000058/uni00000057/uni00000003/uni00000024/uni0000002c/uni0000002b/uni00000058/uni00000050/uni00000044/uni00000051/uni00000003/uni0000000e/uni00000003/uni00000045/uni0000004f/uni00000044/uni00000046/uni0000004e/uni00000010/uni00000045/uni00000052/uni0000005b/uni00000003/uni00000024/uni0000002c/uni0000002b/uni00000058/uni00000050/uni00000044/uni00000051/uni00000003/uni0000000e/uni00000003/uni00000048/uni0000005b/uni00000053/uni0000004f/uni00000044/uni0000004c/uni00000051/uni00000044/uni00000045/uni0000004f/uni00000048/uni00000003/uni00000024/uni0000002c/uni00000013/uni00000008 /uni00000015/uni00000013/uni00000008 /uni00000017/uni00000013/uni00000008 /uni00000019/uni00000013/uni00000008 /uni0000001b/uni00000013/uni00000008 /uni00000014/uni00000013/uni00000013/uni00000008 /uni0000001b/uni0000001c/uni00000011/uni0000001a/uni00000008/uni00000033/uni00000055/uni00000048/uni00000046/uni0000004c/uni00000056/uni0000004c/uni00000052/uni00000051 t = 2.946/uni0000000d/uni0000000d(P = 0.0017) /uni00000025 /uni00000033/uni00000048/uni00000055/uni00000049/uni00000052/uni00000055/uni00000050/uni00000044/uni00000051/uni00000046/uni00000048/uni00000003/uni00000052/uni00000049/uni00000003/uni00000056/uni00000057/uni00000044/uni00000051/uni00000047/uni00000044/uni0000004f/uni00000052/uni00000051/uni00000048/uni00000003/uni00000024/uni0000002c/uni00000003/uni00000044/uni0000004f/uni0000004a/uni00000052/uni00000055/uni0000004c/uni00000057/uni0000004b/uni00000050 Figure S11: Results of non-expert experiment. The boxplots compare the task perfor- mance between the two treatments: black-box AI and explainable AI. The task performance is measured by the defect detection rate ( A) and the precision ( B) based on the quality assess- ment of radiologists and the ground-truth labels of the chest X-ray images. The standalone AI algorithm attains a defect detection rate of 92.9% and a precision of 89.7% (orange dashed lines). Statistical significance is based on a one-sided Welch’s t-test (***P <0.001, **P <0.01, *P <0.05). In the boxplots, the center line denotes the median; box limits are upper and lower quartiles; whiskers are defined as the 1.5x interquartile range. J.2 Randomization checks We performed randomization checks to confirm that the distribution of participants in the three treatment arms of the non-experts experiment was unbiased. The following demographic vari- ables were collected: age bracket [ <20, 20–30, 30–40, 40–50, 50–60, 60–70, >70], gender [male, female, not listed], and highest level of education [no schooling, primary school, some high-school; no degree, high school degree, Bachelor’s degree, Master’s degree, doctorate]. Table S13 reports the observed frequencies in the three treatment arms. The randomization checks are based on X2-tests of independence. The results suggest no statistically significant differences between the participants in the three treatment arms. 72Table S13: Randomization checks for non-experts experiment Human with black-box AI Human with explainable AI Human without AI P-value Age 0 |39 |58 |36 |16 |1 |1 0 |26 |56 |34 |13 |8 |0 1 |46 |49 |28 |13 |5 |0 0.168 Gender 104 |47 |0 86 |51 |0 89 |53 |0 0.443 Education 0 |0 |1 |30 |88 |32 |0 0 |0 |1 |24 |91 |21 |0 0 |0 |2 |16 |90 |34 |0 0.278 Observations 151 137 142 – Notes: The table reports the frequency of participants that fall in the specific subgroups of age, gender, and education (separated by vertical bars). The P-values for the randomization checks are computed based on X2-tests of independence. J.3 Regression models Table S14 reports three OLS regression models estimating the treatment effect with different control variables. Model (1) estimates the treatment effect for explainable AI with demographic controls (age, gender, and highest level of education). Model (2) estimates the treatment effect for explainable AI with demographic controls and self-reported IT skills (ranging from 1: “novice” to 5: “expert”). Model (3) estimates the treatment effect for explainable AI with demographic controls, self-reported IT skills, and the decision speed (median across the 200 images). All three models return a significant treatment effect for both metrics (balanced accuracy and defect detection rate) as dependent variables. Table S14: OLS regression results for treatment effect (non-experts experiment) Balanced accuracy Defect detection rate Model (1) Model (2) Model (3) Model (1) Model (2) Model (3) Treatment 5.792*** 5.832*** 5.570** 10.435** 10.509** 10.299** (explainable AI) (1.729) (1.720) (1.707) (3.274) (3.258) (3.263) Demographics Yes Yes Yes Yes Yes Yes IT skills No Yes Yes No Yes Yes Decision speed No No Yes No No Yes Observations 288 288 288 288 288 288 Notes: The table reports three OLS regression models with different sets of control variables and two different metrics as dependent variables. The standard errors of the treatment effect are reported in parentheses. Statistical significance: ***P <0.001, **P <0.01, *P <0.05. Table S15 reports three quasi-binomial regression models estimating the treatment effect with 73the same control variables as before. Again, all three models return a significant treatment effect for both metrics as dependent variables. Table S15: Quasi-binomial regression results for treatment effect (non-experts ex- periment) Balanced accuracy Defect detection rate Model (1) Model (2) Model (3) Model (1) Model (2) Model (3) Treatment 0.449** 0.455*** 0.442** 0.528** 0.536** 0.528** (explainable AI) (0.137) (0.136) (0.136) (0.168) (0.167) (0.168) Demographics Yes Yes Yes Yes Yes Yes IT skills No Yes Yes No Yes Yes Decision speed No No Yes No No Yes Observations 288 288 288 288 288 288 Notes: The table reports three quasi-binomial regression models with different sets of control variables and two different metrics as dependent variables. The standard errors of the treatment effect are reported in parentheses. Statistical significance: ***P <0.001, **P <0.01, *P <0.05. J.4 Analysis with excluded participants Table S16 reports the number of patients that were excluded according to the three different criteria that we have preregistered. Table S16: Excluded participants across treatment arms Black-box AI Explain- able AI Without AI Time-out 26 31 35 No defective 21 21 27 Worse than 3σ 0 1 0 Table S17 reports the OLS regression model estimating the treatment effect for all different combinations of exclusion criteria. As in our main analysis, the effect of explainable AI is statistically significant for both metrics, balanced accuracy and defect detection rate, irrespective of the exclusion criteria. 74Table S17: OLS regression results with excluded participants (non-experts experi- ment) Excluded: Observations Balanced accuracy Defect detection rate Time-out No defective Worse than 3 σ ✗ ✗ ✗ 388 6.344*** (1.825) 11.539** (3.548) ✓ ✗ ✗ 331 4.843* (1.990) 8.765* (3.902) ✗ ✓ ✗ 342 6.966*** (1.648) 12.652*** (3.023) ✗ ✗ ✓ 387 6.561*** (1.817) 11.788*** (3.549) ✓ ✓ ✗ 289 5.918*** (1.756) 10.863** (3.288) ✓ ✗ ✓ 330 5.102* (1.980) 9.054* (3.904) ✗ ✓ ✓ 341 7.238*** (1.631) 12.988*** (3.014) Notes: The table reports the OLS regression model with two different metrics as dependent variables for all combinations of exclusion criteria. The standard errors of the treatment effect are reported in parentheses. Statistical significance: ***P <0.001, **P <0.01, *P <0.05. 75Supplement K Preregistered hypotheses The following hypotheses were preregistered at https://osf.io/7djxb (Study 1) and https: //osf.io/69yqt (Study 2): Hypothesis 1 (H1):Explainable AI improves the overall decision performance (measured by the balanced accuracy and defect detection rate) compared to humans without AI (i.e., manual inspection) (α = 0.05). Hypothesis 2 (H2):Explainable AI improves the overall decision performance (measured by the balanced accuracy and defect detection rate) compared to black-box AI (α = 0.05). Hypothesis 3 (H3):Explainable AI reduces variation in decision performance (measured by the variance in the balanced accuracy and defect detection rate) compared to black-box AI (α = 0.05). Hypothesis 4 (H4):Explainable AI increases the trust in model decisions (measured by the rate of correct model decisions that are not overruled by the user) compared to black-box AI (α = 0.05). Table S18 summarizes the results from all three studies: (1) the manufacturing experiment at Siemens, (2) the medical experiment, and (3) the manufacturing experiments with non- experts from Amazon MTurk. We report the P-values for both the balanced accuracy and defect detection rate for hypotheses H1, H2, and H3. The statistical testing for hypotheses H1, H2, and H4 are based on one-sided Welch’s t-tests. The statistical testing for Hypothesis H3 is based on Levene’s test for equality of variances. As specified in our preregistration, we refrained from testing Hypothesis H1 in our manufacturing field experiment and our medical experiment. The reason is that we wanted sufficient power in our main treatment arms of interest (i.e., black- box AI versus explainable AI). All hypotheses except for Hypothesis H3 in the non-experts experiment and Hypotheses H2 and H3 for the disease detection rate in the medical experiment were confirmed at a significance level ofα = 0.05. The latter can be expected since missing a lung lesion has more serious consequences than erroneously believing a lung lesion is visible; thus, leading to conservative decision-making of radiologists. Therefore, we additionally inspected precision as a task performance metric. We find that radiologists augmented with explainable AI were significantly more precise in identifying lung lesions compared to radiologists with black- box AI (see Supplement G). 76Table S18: Comparison of results against preregistered hypotheses. Study 1: Manufacturing Study 2: Medical Study 3: Non-experts H1 (BACC) not part of preregistration not part of preregistration ✓ (P <0.001) H1 (DDR) not part of preregistration not part of preregistration ✓ (P <0.001) H2 (BACC) ✓ (P = 0.001) ✓ (P = 0.004) ✓ (P <0.001) H2 (DDR) ✓ (P = 0.004) ✗ (P = 0.498) ✓ (P <0.001) H3 (BACC) ✓ (P = 0.002) ✓ (P = 0.033) ✗ (P = 0.356) H3 (DDR) ✓ (P = 0.023) ✗ (P = 0.790) ✗ (P = 0.217) H4 ✓ (P = 0.011) ✓ (P = 0.001) ✓ (P = 0.009) Notes: The significance level was preregistered at α = 0 .05 and the marks denote whether the corresponding P-value was significant at this level. BACC refers to balanced accuracy and DDR to defect/disease detection rate. 77Supplement L Post-experimental questionnaire For post-hoc exploratory analyses, we asked participants to complete a questionnaire. The ques- tions involved established constructs, such as self-reported task load [137], perceived usefulness [138], perceived ease of use [138], and self-reported trust [139]. We further asked participants about their previous experience and the perceived performance of the AI algorithm. In the man- ufacturing experiment, the questions were translated to German. In the medical experiment, the questions were adapted for the medical setting (for the exact wording, see our preregistration https://osf.io/69yqt). Participants in the non-experts experiment were asked to answer the questions from the viewpoint of a factory worker (“Imagine you work in a factory with a similar job task as you just did.”). The results for all three studies are provided in Tables S19 to S23. Table S19: Self-reported task load Study 1: Manufacturing Study 2: Medical Study 3: Non-experts Question Human with black-box AI Human with explainable AI Human with black-box AI Human with explainable AI Human with black-box AI Human with explainable AI How mentally demand- ing was the task? (1 = very low, 7 = very high) 3.41 (1.37) 3.54 (1.17) 3.80 (1.34) 3.73 (1.60) 4.72 (1.58) 4.87 (1.62) How physically de- manding was the task? (1 = very low, 7 = very high) 3.09 (1.54) 2.85 (1.38) 2.18 (1.32) 2.52 (1.42) 4.09 (2.10) 3.95 (2.06) How hurried or rushed was the pace of the task? (1 = very low, 7 = very high) 3.59 (1.10) 3.92 (1.23) 2.76 (1.35) 3.16 (1.66) 4.56 (1.55) 4.44 (1.68) How successful were you in accomplishing what you were asked to do? (1 = very poor, 7 = very good) 5.27 (0.94) 5.81 (0.85) 5.64 (1.09) 5.57 (0.93) 5.68 (1.08) 5.87 (1.02) How hard did you have to work to accomplish your level of perfor- mance? (1 = very low, 7 = very high) 4.00 (1.35) 4.00 (0.94) 3.33 (1.28) 3.36 (1.30) 5.18 (1.38) 5.29 (1.46) How insecure, dis- couraged, irritated, stressed, and annoyed were you? (1 = very low, 7 = very high) 2.77 (1.31) 2.77 (1.58) 2.76 (1.28) 2.91 (1.43) 3.52 (1.98) 3.31 (1.95) Notes: The table reports the average scores for the self-reported task load. Standard deviations are reported in parentheses. 78Table S20: Perceived usefulness Study 1: Manufacturing Study 2: Medical Study 3: Non-experts Question Human with black-box AI Human with explainable AI Human with black-box AI Human with explainable AI Human with black-box AI Human with explainable AI Using the Artificial In- telligence System in my job would enable me to accomplish tasks more quickly. (1 = very unlikely, 7 = ex- tremely likely) 4.95 (1.70) 5.12 (1.37) 5.00 (1.40) 5.25 (1.06) 5.68 (1.02) 5.86 (1.08) Using the Artificial Intelligence System would improve my job performance. (1 = very unlikely, 7 = extremely likely) 4.91 (1.54) 5.23 (0.99) 5.04 (1.24) 5.09 (1.14) 5.68 (1.04) 5.98 (1.06) Using the Artificial In- telligence System in my job would increase my productivity. (1 = very unlikely, 7 = ex- tremely likely) 4.95 (1.43) 4.96 (1.22) 5.04 (1.38) 5.39 (1.15) 5.71 (1.11) 5.98 (1.01) Using the Artificial In- telligence System in my job would enhance my effectiveness on the job. (1 = very poor, 7 = very good) 5.05 (1.53) 4.88 (1.14) 4.98 (1.34) 5.20 (1.25) 5.66 (1.12) 5.91 (0.97) Using the Artificial Intelligence System would make it easier to do my job. (1 = very unlikely, 7 = extremely likely) 5.18 (1.26) 5.08 (1.32) 4.93 (1.45) 5.23 (1.08) 5.65 (1.23) 6.01 (1.00) I would find the Arti- ficial Intelligence Sys- tem useful in my job. (1 = very unlikely, 7 = extremely likely) 5.27 (1.42) 5.23 (1.24) 5.00 (1.41) 5.09 (1.22) 5.79 (1.09) 6.07 (1.04) Notes: The table reports the average scores for the perceived usefulness of the AI algorithm. Standard deviations are reported in parentheses. 79Table S21: Perceived ease of use Study 1: Manufacturing Study 2: Medical Study 3: Non-experts Question Human with black-box AI Human with explainable AI Human with black-box AI Human with explainable AI Human with black-box AI Human with explainable AI Learning to operate the Artificial Intelli- gence System would be easy for me. (1 = very unlikely, 7 = extremely likely) 5.00 (1.45) 5.35 (1.20) 5.78 (0.88) 6.00 (0.86) 5.69 (1.11) 5.90 (0.95) I would find it easy to get the Artificial Intel- ligence System to do what I want it to do. (1 = very unlikely, 7 = extremely likely) 4.05 (1.36) 4.46 (0.90) 4.87 (1.22) 5.09 (1.03) 5.61 (1.02) 5.78 (0.97) My interaction with the Artificial Intelli- gence System would be clear and understand- able. (1 = very un- likely, 7 = extremely likely) 5.09 (0.97) 5.27 (0.96) 5.16 (1.07) 5.20 (1.21) 5.65 (1.08) 5.99 (0.93) I would find the Arti- ficial Intelligence Sys- tem to be flexible to interact with. (1 = very poor, 7 = very good) 4.82 (1.22) 5.00 (1.06) 4.82 (1.28) 4.84 (1.27) 5.36 (1.24) 5.55 (1.10) It would be easy for me to become skillful at using the Artificial Intelligence System. (1 = very unlikely, 7 = extremely likely) 5.14 (1.21) 5.38 (0.85) 5.38 (1.21) 5.61 (0.84) 5.73 (0.97) 5.93 (0.99) I would find the Arti- ficial Intelligence Sys- tem easy to use. (1 = very unlikely, 7 = ex- tremely likely) 5.14 (0.94) 5.42 (1.03) 5.16 (1.17) 5.66 (0.83) 5.71 (1.03) 6.05 (0.96) Notes: The table reports the average scores for the perceived ease of use of the AI algorithm. Standard deviations are reported in parentheses. 80Table S22: Previous experience and perceived performance Study 1: Manufacturing Study 2: Medical Study 3: Non-experts Question Human with black-box AI Human with explainable AI Human with black-box AI Human with explainable AI Human with black-box AI Human with explainable AI How strong would you consider your general IT skills? (1 = novice, 5 = expert) 2.86 (0.94) 3.08 (0.84) 3.47 (1.01) 3.77 (1.01) 3.48 (0.99) 3.48 (1.08) How often do you interact with Artificial Intelligence in your job? (1 = very little, 5 = very much) 2.32 (1.13) 2.23 (1.11) not part of preregistration 3.18 (1.31) 3.12 (1.32) How familiar do you feel with Artificial In- telligence in general? (1 = very little, 5 = very much) 2.41 (1.10) 2.88 (1.07) 3.00 (1.07) 3.07 (0.95) 3.56 (0.98) 3.48 (0.93) How well did the Arti- ficial Intelligence Sys- tem perform in com- parison to your expec- tations? (1 = very poor, 7 = very good) 5.00 (1.57) 6.08 (1.06) 4.49 (1.41) 4.48 (1.47) 5.72 (1.01) 6.15 (0.85) How likely is the Arti- ficial Intelligence Sys- tem to make a bad es- timate? (1 = very un- likely, 7 = very likely) 3.55 (1.34) 3.04 (1.08) 3.93 (1.14) 4.02 (1.13) 3.25 (1.61) 2.93 (1.68) Completing the quality inspections task has changed my opinion about Artificial Intelli- gence. (1 = strongly disagree, 7 = strongly agree) 4.64 (1.36) 4.08 (1.38) not part of preregistration 4.68 (1.65) 4.83 (1.61) How much did you rely on the Artificial Intel- ligence System? (1 = very little, 7 = very much) 4.32 (1.46) 4.77 (1.37) not part of preregistration 4.96 (1.43) 5.54 (1.38) The Artificial Intelli- gence System provides clear explanations for its outputs. (1 = strongly disagree, 7 = strongly agree) 4.59 (1.33) 5.04 (1.00) not part of preregistration 4.99 (1.54) 5.71 (1.20) Notes: The table reports the average scores for previous experience and the perceived performance of the AI algorithm. Standard deviations are reported in parentheses. 81Table S23: Self-reported trust in AI algorithm Study 1: Manufacturing Study 2: Medical Study 3: Non-experts Question Human with black-box AI Human with explainable AI Human with black-box AI Human with explainable AI Human with black-box AI Human with explainable AI The Artificial Intelli- gence System is decep- tive. (1 = strongly disagree, 7 = strongly agree) 3.27 (1.20) 2.88 (1.18) 3.31 (1.41) 3.39 (1.28) 3.79 (2.04) 3.72 (2.16) I am suspicious of the Artificial Intelligence System’s intent, ac- tion, or outputs. (1 = strongly disagree, 7 = strongly agree) 2.91 (1.27) 2.62 (0.98) 3.09 (1.69) 2.98 (1.56) 3.88 (2.04) 3.69 (2.15) The Artificial Intel- ligence System’s ac- tions will have a harm- ful outcome. (1 = strongly disagree, 7 = strongly agree) 3.09 (1.41) 2.65 (1.09) 3.38 (1.51) 3.34 (1.43) 3.73 (2.04) 3.48 (2.08) I am confident in the Artificial Intelligence System. (1 = very poor, 7 = very good) 4.91 (1.06) 4.96 (1.15) 4.04 (1.31) 4.34 (1.22) 5.52 (1.20) 5.89 (0.97) The Artificial Intelli- gence System is reli- able. (1 = strongly disagree, 7 = strongly agree) 4.95 (0.84) 5.27 (1.04) 4.07 (1.39) 4.27 (1.09) 5.63 (1.10) 5.93 (0.85) I can trust the Arti- ficial Intelligence Sys- tem. (1 = strongly disagree, 7 = strongly agree) 4.86 (0.77) 5.04 (0.92) 3.87 (1.39) 4.07 (1.11) 5.66 (1.06) 5.80 (0.94) Notes: The table reports the average scores for the self-reported trust in the AI algorithm. Standard deviations are reported in parentheses. 82",
      "references": [
        "What can machine learning do? Workforce implications.",
        "Artificial intelligence index report 2024.",
        "Machine learning for industrial applications: A comprehensive literature review.",
        "A survey of clinicians on the use of artificial intelligence in ophthalmology, dermatology, radiology and radiation oncology.",
        "Gen-AI: Artificial intelligence and the fu- ture of work.",
        "Very deep convolutional networks for large-scale image recognition.",
        "Deep residual learning for image recognition.",
        "Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead.",
        "A survey of methods for explaining black box models.",
        "Definitions, methods, and applications in interpretable machine learning.",
        "XAI—Explainable artificial intelligence.",
        "Explaining decision-making algorithms through UI.",
        "Questioning the AI: Informing design practices for explainable AI user experiences.",
        "Bench- marking and survey of explanation methods for black box models.",
        "Explainable AI (XAI): Core ideas, techniques, and solutions.",
        "Explainable AI: Interpreting, Explaining and Visualizing Deep Learning",
        "Explaining deep neural networks and beyond: A review of methods and applications.",
        "Interpretable machine learning: A guide for making Black Box Models inter- pretable",
        "Why should I trust you? Explaining the predic- tions of any classifier.",
        "A unified approach to interpreting model predictions.",
        "Deep inside convolutional networks: Visualising image classification models and saliency maps.",
        "Grad- CAM: Visual explanations from deep networks via gradient-based localization.",
        "Cognitive challenges in human–artificial in- telligence collaboration: Investigating the path toward productive delegation.",
        "Exploring user heterogeneity in human delegation behavior towards AI.",
        "Please take over: XAI, delegation of authority, and domain knowledge.",
        "Algorithm aversion: People erroneously avoid algorithms after seeing them err.",
        "Overcoming algorithm aversion: People will use imperfect algorithms if they can (even slightly) modify them.",
        "People reject algorithms in uncertain decision domains because they have diminishing sensitivity to forecasting error.",
        "A systematic review of algorithm aversion in augmented decision making.",
        "Explainable AI and adoption of financial algo- rithmic advisors.",
        "Trust in AI and its role in the acceptance of AI tech- nologies.",
        "The effects of meaningful and mean- ingless explanations on trust and perceived system accuracy in intelligent systems.",
        "Understanding the impact of explanations on advice-taking: a user study for AI-based clinical decision support systems.",
        "Does the whole exceed its parts? The effect of AI explanations on complementary team performance.",
        "Explanations can reduce overreliance on AI systems during decision-making.",
        "Understanding the role of human intuition on reliance in human-AI decision-making with explanations.",
        "Proxy tasks and subjective measures can be misleading in evaluating explainable AI systems.",
        "Are visual explanations useful? A case study in model-in- the-loop prediction.",
        "Does explain- able artificial intelligence improve human decision-making?",
        "Appropriate reliance on AI advice: Conceptualization and the effect of explanations.",
        "From local explanations to global understanding with explainable AI for trees.",
        "Collaboration between explainable artificial intelligence and pulmonologists improves the accuracy of pulmonary function test interpretation.",
        "Non-task expert physicians benefit from correct explainable AI advice when reviewing X-rays.",
        "How can I choose an explainer?",
        "Ignore, trust, or negotiate: Understanding clinician acceptance of AI-based treatment recommendations in health care.",
        "Measuring the impact of AI in the diagnosis of hospitalized patients: A randomized clinical vignette survey study.",
        "Improving unsupervised defect segmentation by applying structural similarity to autoencoders.",
        "MVTec AD — A comprehensive real-world dataset for unsupervised anomaly detection.",
        "Morphological and molecular breast cancer profiling through explainable machine learning.",
        "Benchmarking saliency methods for chest x-ray interpretation.",
        "Quality control handbook",
        "Quality Management Essentials",
        "Introduction to manufacturing: An industrial engineering and management perspective",
        "Human-computer collaboration for skin cancer recognition.",
        "Review of the current state of whole slide imaging in pathology.",
        "Explainable convolutional neural networks: A taxonomy, review, and future directions.",
        "Guidelines for human-AI interaction.",
        "A case for humans-in-the-loop: Decisions in the presence of erroneous algorithmic scores.",
        "Will humans-in-the-loop become borgs? Merits and pitfalls of working with AI.",
        "Understanding, explaining, and utilizing medical artificial intelligence.",
        "Predicting human discretion to adjust algorithmic prescription: A large-scale field experiment in warehouse operations.",
        "When will workers follow an algorithm? A field experiment with a retail business.",
        "Task-dependent algorithm aversion.",
        "Making sense of recommenda- tions.",
        "Using explainable artificial intelligence to im- prove process quality: Evidence from semiconductor manufacturing.",
        "Fooling LIME and SHAP: Adversarial attacks on post hoc explanation methods.",
        "Algorithmic Accountability Act of 2019",
        "Ethics Guidelines for Trustworthy AI",
        "Transparent, explainable, and accountable AI for robotics.",
        "The ten commandments of ethical medical AI.",
        "The global landscape of AI ethics guidelines.",
        "Pulmonary nodules and lung lesions",
        "Lung lesions: correlation between viewing time and detection.",
        "CheXpert: A large chest radiograph dataset with uncertainty labels and expert comparison.",
        "Artificial-intelligence-based molecular classification of diffuse gliomas using rapid, label-free optical imaging.",
        "Explainable AI: A review of machine learning interpretability methods.",
        "Generalized linear models.",
        "Generalized additive models",
        "Intelligible models for classification and regression.",
        "Interpretable generalized additive neural networks.",
        "A value for n-person games.",
        "Visual interpretation of kernel-based prediction models.",
        "Analysis of explainers of black box deep neural networks for computer vision: A survey.",
        "Visualizing and understanding convolutional networks.",
        "On pixel- wise explanations for non-linear classifier decisions by layer-wise relevance propagation.",
        "Axiomatic attribution for deep networks.",
        "Learning important features through propa- gating activation differences.",
        "Learning deep features for discriminative localization.",
        "Grad-CAM++: Generalized gradient-based visual explanations for deep convolutional networks.",
        "Eigen-CAM: Visual explanations for deep convolu- tional neural networks.",
        "Counterfactual explanations for machine learning: A review.",
        "Anomaly detection for industrial quality assurance: A comparative evaluation of unsupervised deep learning models.",
        "Deep industrial image anomaly detection: A survey.",
        "Towards a rigorous science of interpretable machine learning.",
        "Model agnostic supervised local explanations.",
        "A benchmark for interpretability methods in deep neural networks.",
        "On the robustness of interpretability methods.",
        "How do humans understand explanations from machine learning systems? An evaluation of the human-interpretability of explanation.",
        "On the importance of application-grounded experimental design for evaluating explainable ML methods.",
        "Towards better analysis of deep convolu- tional neural networks.",
        "Understanding hidden memories of recurrent neural networks.",
        "DeepEyes: Progressive visual analytics for designing deep neural networks.",
        "LSTMVis: A tool for visual analysis of hidden state dynamics in recurrent neural networks.",
        "Rise of the machines: Delegating decisions to autonomous AI.",
        "The influence of algorithm aversion and anthropomorphic agent design on the acceptance of AI-based job recommendations.",
        "Who is the expert? Reconciling algorithm aversion and algorithm appreciation in AI-supported decision making.",
        "Humans rely more on algorithms than social influence as a task becomes more difficult.",
        "Mitigating algorithm aversion in recruiting: A study on explainable AI for conversational agents.",
        "Human-centered tools for coping with imperfect algorithms during medical decision-making.",
        "User trust and understanding of ex- plainable AI: Exploring algorithm visualisations and user biases.",
        "Effect of confidence and explanation on accuracy and trust calibration in AI-assisted decision making.",
        "Explainability does not improve biochemistry staff trust in artificial intelligence-based decision support.",
        "Co-design of human-centered, explainable AI for clinical decision support.",
        "Effects of explainable artificial intelligence on trust and human behavior in a high-risk decision task.",
        "To trust or to think: Cognitive forcing functions can reduce overreliance on AI in AI-assisted decision-making.",
        "Trust in automation: designing for appropriate reliance.",
        "The principles and limits of algorithm-in-the-loop decision making.",
        "Human evaluation of models built for interpretability.",
        "On human predictions with explanations and predictions of machine learning models.",
        "The effects of example-based explanations in a machine learning interface.",
        "Feature-based explanations don’t help people detect misclassifications of online toxicity.",
        "“Why is ’Chicago’ deceptive?” Towards building model-driven tutorials for humans.",
        "How do visual explanations foster end users’ appropriate trust in machine learning?",
        "Evaluating saliency map explanations for convolutional neural networks.",
        "Are explanations helpful? A comparative study of the effects of explanations in AI-assisted decision-making.",
        "Manipulating and measuring model interpretability.",
        "Evaluating XAI: A com- parison of rule-based and example-based explanations.",
        "Hive: Evaluat- ing the human interpretability of visual explanations.",
        "Explainable artificial intelligence improves human decision-making: Results from a mushroom picking experi- ment at a public art festival.",
        "The benefits and costs of explainable artifi- cial intelligence in visual quality control: Evidence from fault detection performance and eye movements.",
        "Improving trust and confidence in medical skin lesion diagnosis through explainable deep learning.",
        "Quantifying the impact of AI recommendations with explanations on prescription decision making.",
        "A review of novelty detection.",
        "Image quality assessment: From error visibility to structural similarity.",
        "Densely connected con- volutional networks.",
        "Reputation as a sufficient condition for data quality on Amazon Mechanical Turk.",
        "Nasa Task Load Index (TLX)",
        "Perceived usefulness, perceived ease of use, and user acceptance of information technology.",
        "Foundations for an empirically determined scale of trust in automated systems."
      ],
      "meta_data": {
        "arxiv_id": "2406.08271v1",
        "authors": [
          "Julian Senoner",
          "Simon Schallmoser",
          "Bernhard Kratzwald",
          "Stefan Feuerriegel",
          "Torbjørn Netland"
        ],
        "published_date": "2024-06-12T14:36:22Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Addresses whether adding explainability to AI decision support (vs. identical-performance black-box AI) improves domain-expert task performance in real human–AI collaboration. Provides causal evidence from two preregistered randomized experiments in real-world visual inspection tasks (manufacturing defect inspection; radiology chest X-ray lung-lesion detection) showing explainable AI (heatmap + score) significantly improves human decision quality. Explains mechanism: explainability increases appropriate reliance—experts follow accurate AI more and better identify/override wrong AI—without slowing decisions; in manufacturing many experts with XAI even outperform the standalone AI.",
        "methodology": "Between-subject randomized controlled experiments comparing (a) black-box AI decision aid (numeric score 0–100) vs (b) explainable AI with the same score plus a visual heatmap explanation. Manufacturing AI: unsupervised anomaly detection using structural similarity (SSIM) to compute a quality score; anomaly heatmaps highlight deviating regions. Medical AI: pretrained DenseNet121 multi-label chest X-ray model; lung-lesion probability transformed into a 0–100 risk-style score; Grad-CAM heatmaps provide localization. Statistical analysis: balanced accuracy and defect/disease detection rate (and precision in robustness); one-sided Welch’s t-tests; OLS treatment-effect regressions with/without controls; quasi-binomial regressions; checks for decision speed and adherence/override behavior; multiple robustness checks (alternative heatmap algorithms, inclusion of excluded participants, non-expert replication).",
        "experimental_setup": "Study 1 (manufacturing, Siemens factory field experiment): 56 workers recruited (final n=48 after preregistered exclusions); each inspected 200 PCB images (4 product types; per type 43 faultless, 7 defective; 14% defects overall) within 35 minutes via a production-like UI, with reference image plus AI quality score; XAI arm additionally saw anomaly heatmaps. Total assessments N=9,600. Standalone AI (cutoff 90) achieved balanced accuracy 95.6% and defect detection rate 92.9%. Outcomes: human balanced accuracy, defect detection rate, decision speed, and adherence/override analyses; significance via Welch t-tests and regression.\nStudy 2 (medicine, online): 122 US radiologists recruited (final n=113); each assessed 50 CheXpert chest X-rays (7 with ≥1 lung lesion, 43 without; 14% prevalence) within 35 minutes using Qualtrics with zoom; received AI score in both arms and Grad-CAM heatmap only in XAI arm. Total assessments N=5,650. Standalone AI achieved balanced accuracy 82.2% and disease detection rate 71.4%. Outcomes: balanced accuracy, disease detection rate (true negative rate given lesions treated as negatives), precision (robustness), decision speed, adherence/override; robustness included alternative saliency methods (DeepLIFT, LRP correlations), additional controls, quasi-binomial models, and sensitivity to exclusions.",
        "limitations": "(1) Task and modality scope: both studies are visual inspection tasks and use one explanation form (heatmaps); results may not generalize to other task types (text/tabular decisions, sequential decisions) or other XAI modalities (feature attributions, counterfactuals, natural-language rationales). (2) Explanation reliability: heatmaps are post-hoc and can be misleading or vulnerable to adversarial manipulation; the study assumes the provided explanations are sufficiently faithful/useful. (3) Prevalence shift: only 14% positives/defects in stimuli but this is higher than typical real-world encounter rates (especially radiology), potentially affecting decision thresholds and expectations. (4) Dataset/task breadth: limited number of images (200 manufacturing; 50 X-rays) and a single lesion/defect framing; may not cover full variability of real production lines/clinical populations. (5) Short-term, controlled evaluation: measures immediate performance in a time-limited session; no assessment of long-term learning, fatigue across shifts, workflow integration costs, or downstream organizational/patient outcomes.",
        "future_research_directions": "Replicate and extend to (a) non-visual and higher-dimensional decisions (tabular clinical risk scores, maintenance, finance) and (b) additional XAI forms (counterfactuals, concept-based explanations, example-based explanations, uncertainty visualization, interactive explanations). Study longitudinal deployment to test learning effects, trust calibration over time, and real operational outcomes (scrap/rework costs; diagnostic follow-ups, patient outcomes). Investigate when explanations help vs. harm under distribution shift, low base rates, and varying AI accuracy; develop adaptive explanation interfaces based on user expertise and confidence. Evaluate robustness/safety of explanations (adversarial attacks, faithfulness metrics) and combine XAI with other interventions (cognitive forcing functions, decision checklists) to reduce both algorithm aversion and overreliance.",
        "experimental_code": "",
        "experimental_info": ""
      }
    }
  ],
  "research_hypothesis": {
    "open_problems": "Automated iterative prompt optimization can introduce *silent subgroup-specific behavior shifts* even when overall dev accuracy improves: small prompt edits can disproportionately flip predictions for inputs containing demographic identity terms (e.g., gendered words) and these shifts are poorly detected by (i) string-distance stability penalties and (ii) *average* behavioral drift metrics (mean KL on a small anchor set), which can be “gamed” by keeping most anchors stable while changing behavior sharply on a minority slice. This creates deployment risk (fairness/reliability regressions) and scientific instability (seed-sensitive oscillations driven by narrow heuristics). The key gap is a **distributionally robust, behavior-level trust region** that (a) targets worst-case drift rather than mean drift and (b) explicitly monitors drift on counterfactual demographic variants using only cheap, unlabeled anchors.",
    "method": "**Distributionally-Robust Counterfactual Functional Trust-Region Prompt Optimization (DR-CFTR)**\n\nWe refine functional trust-region prompt optimization in two ways to increase novelty and social value:\n\n1) **Worst-case (CVaR) functional trust region.** Instead of regularizing the *mean* KL drift across anchors, we regularize the Conditional Value-at-Risk (CVaR), i.e., the average of the largest α-fraction of per-anchor KLs:\n\n- For anchors A, define per-example drift d_x(p_new,p_old)=KL(π_old(·|x) || π_new(·|x)).\n- Define D_CVaR = CVaR_α({d_x}) = mean of top ceil(α|A|) values.\n\nThis discourages “localized” behavioral flips that average-KL can miss, making the update robust to minority slices and preventing optimizer lock-in to spurious heuristics.\n\n2) **Counterfactual anchor augmentation + active anchor selection.** To make anchors socially and scientifically meaningful without extra labels:\n\n- Maintain an unlabeled pool U from the training distribution.\n- Each iteration, select anchors A_t as the most *uncertain* inputs under the current prompt (highest predictive entropy), promoting coverage near the decision boundary where drift is most harmful.\n- For each anchor x, generate a lightweight **counterfactual** x′ by swapping gendered identity terms (he↔she, man↔woman, etc.). Add x′ to the anchor set if changed.\n\n**Update rule (gradient-free):** generate K candidate rewrites {p^k}. For each candidate:\n\nJ(p^k)=Acc_dev(p^k) − λ·D_CVaR(p^k,p_{t−1}; A_t ∪ CF(A_t))\n\nOptionally enforce a hard cap D_CVaR≤ε.\n\n**Why novel vs prior prompt optimization:** (i) introduces a *distributionally robust* trust region (CVaR of functional KL) rather than mean drift or token/edit-distance penalties; (ii) uses *counterfactual* anchors as an unlabeled behavioral safety set; (iii) uses *active* anchor selection (entropy-based) to cheaply approximate worst-case drift regions. Together, this directly targets reproducibility + fairness-sensitive stability in iterative prompt rewriting with minimal extra model calls.",
    "experimental_setup": "**Goal:** show DR-CFTR improves *worst-group* OOD generalization and reduces seed variance under the same prompt-search budget.\n\n**Task:** binary sentiment classification (frozen seq2seq prompt-as-classifier).\n\n**Datasets:**\n- In-domain optimization: GLUE/SST-2 (optimize prompts on SST-2 validation subset as feedback).\n- OOD evaluation: IMDb test.\n\n**Group definition for social robustness (IMDb):**\n- group_male: text contains {he, him, his, man, male, boy, father, husband}\n- group_female: text contains {she, her, hers, woman, female, girl, mother, wife}\n- group_none: neither\n\n**Methods compared (same T iterations, K candidates):**\n1) Greedy: maximize Acc_dev only.\n2) Mean-KL FTR: anchors are random; regularizer uses mean KL.\n3) **DR-CFTR (ours):** anchors are entropy-selected; regularizer uses CVaR KL on anchors + counterfactuals.\n\n**Protocol (feasible on a single GPU/CPU with small budgets):**\n- Model: google/flan-t5-small (frozen).\n- Iterations: T=8; candidates/iter: K=6.\n- feedback_dev (labeled): 400 SST-2 validation examples.\n- anchor_pool U (unlabeled): 1500 SST-2 train examples.\n- anchors/iter: N=128 selected by entropy from U (under current prompt).\n- CVaR α=0.2; λ tuned on a tiny held-out set or fixed (e.g., λ=0.1).\n- Evaluate final prompt on:\n  - IMDb subset of 1200 examples (for speed), reporting overall and per-group accuracy.\n\n**Metrics:**\n- Primary: worst_group_accuracy on IMDb (min accuracy across male/female/none groups).\n- Secondary: imdb_accuracy (overall), sst2_test_accuracy, std across 5 seeds of worst_group_accuracy, and realized D_CVaR drift per iteration.\n\n**Validation criterion:** DR-CFTR yields higher mean worst_group_accuracy and lower seed variance than greedy and mean-KL FTR, without materially harming SST-2 accuracy.",
    "primary_metric": "worst_group_accuracy",
    "experimental_code": "import random\nimport re\nimport math\nimport torch\nimport torch.nn.functional as F\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\n# ------------------------\n# Label distribution utils\n# ------------------------\n\ndef _encode(tok, text, device):\n    return tok(text, return_tensors=\"pt\", truncation=True).to(device)\n\n@torch.no_grad()\ndef seq_logprob(model, tok, input_text: str, target_text: str, device: str) -> float:\n    x = _encode(tok, input_text, device)\n    y = tok(target_text, return_tensors=\"pt\", add_special_tokens=True).input_ids.to(device)\n    out = model(input_ids=x.input_ids, attention_mask=x.attention_mask, labels=y)\n    n_tokens = y.numel()\n    return float(-out.loss.item() * n_tokens)\n\n@torch.no_grad()\ndef label_dist(model, tok, prompt: str, sentence: str, device: str):\n    inp = f\"{prompt}\\nSentence: {sentence}\\nLabel (positive/negative):\"\n    lp = seq_logprob(model, tok, inp, \"positive\", device)\n    ln = seq_logprob(model, tok, inp, \"negative\", device)\n    logits = torch.tensor([lp, ln], dtype=torch.float32)\n    return F.softmax(logits, dim=0)\n\n@torch.no_grad()\ndef predict_label(model, tok, prompt: str, sentence: str, device: str) -> int:\n    # 1=positive, 0=negative\n    p = label_dist(model, tok, prompt, sentence, device)\n    return int(torch.argmax(p).item() == 0)\n\n@torch.no_grad()\ndef accuracy(model, tok, prompt: str, data, device: str, text_key: str, label_key: str, max_n: int = 300) -> float:\n    n = min(len(data), max_n)\n    c = 0\n    for i in range(n):\n        ex = data[i]\n        pred = predict_label(model, tok, prompt, ex[text_key], device)\n        c += int(pred == int(ex[label_key]))\n    return c / n\n\n# ------------------------\n# Robust functional trust region\n# ------------------------\n\n@torch.no_grad()\ndef kl_2class(q, r):\n    # KL(q||r) for 2-class distributions\n    q = q.clamp_min(1e-8)\n    r = r.clamp_min(1e-8)\n    return float((q * (q.log() - r.log())).sum().item())\n\n@torch.no_grad()\ndef drift_list(model, tok, p_new: str, p_old: str, anchors, device: str):\n    ds = []\n    for ex in anchors:\n        sent = ex[\"sentence\"]\n        q = label_dist(model, tok, p_old, sent, device)\n        r = label_dist(model, tok, p_new, sent, device)\n        ds.append(kl_2class(q, r))\n    return ds\n\ndef cvar(values, alpha: float = 0.2) -> float:\n    assert 0 < alpha <= 1\n    if len(values) == 0:\n        return 0.0\n    k = max(1, int(math.ceil(alpha * len(values))))\n    vals = sorted(values, reverse=True)\n    return float(sum(vals[:k]) / k)\n\n@torch.no_grad()\ndef mean_kl(model, tok, p_new: str, p_old: str, anchors, device: str) -> float:\n    ds = drift_list(model, tok, p_new, p_old, anchors, device)\n    return float(sum(ds) / max(1, len(ds)))\n\n@torch.no_grad()\ndef cvar_kl(model, tok, p_new: str, p_old: str, anchors, device: str, alpha: float = 0.2) -> float:\n    ds = drift_list(model, tok, p_new, p_old, anchors, device)\n    return cvar(ds, alpha=alpha)\n\n# ------------------------\n# Counterfactual anchors\n# ------------------------\n\nSWAPS = [\n    (r\"\\bhe\\b\", \"she\"), (r\"\\bshe\\b\", \"he\"),\n    (r\"\\bhim\\b\", \"her\"), (r\"\\bher\\b\", \"him\"),\n    (r\"\\bhis\\b\", \"hers\"), (r\"\\bhers\\b\", \"his\"),\n    (r\"\\bman\\b\", \"woman\"), (r\"\\bwoman\\b\", \"man\"),\n    (r\"\\bmale\\b\", \"female\"), (r\"\\bfemale\\b\", \"male\"),\n    (r\"\\bboy\\b\", \"girl\"), (r\"\\bgirl\\b\", \"boy\"),\n    (r\"\\bfather\\b\", \"mother\"), (r\"\\bmother\\b\", \"father\"),\n    (r\"\\bhusband\\b\", \"wife\"), (r\"\\bwife\\b\", \"husband\"),\n]\n\ndef counterfactualize(text: str) -> str:\n    t = text\n    for pat, rep in SWAPS:\n        t2 = re.sub(pat, rep, t, flags=re.IGNORECASE)\n        t = t2\n    return t\n\ndef augment_with_counterfactuals(anchors):\n    out = list(anchors)\n    for ex in anchors:\n        cf = counterfactualize(ex[\"sentence\"])\n        if cf.strip().lower() != ex[\"sentence\"].strip().lower():\n            out.append({\"sentence\": cf})\n    return out\n\n# ------------------------\n# Active anchor selection\n# ------------------------\n\n@torch.no_grad()\ndef entropy_2class(probs):\n    p = probs.clamp_min(1e-8)\n    return float(-(p * p.log()).sum().item())\n\n@torch.no_grad()\ndef select_entropy_anchors(model, tok, prompt: str, pool, device: str, n_anchors: int = 128, pool_scan: int = 800):\n    m = min(len(pool), pool_scan)\n    scored = []\n    for i in range(m):\n        sent = pool[i][\"sentence\"]\n        pr = label_dist(model, tok, prompt, sent, device)\n        scored.append((entropy_2class(pr), sent))\n    scored.sort(key=lambda x: x[0], reverse=True)\n    anchors = [{\"sentence\": s} for _, s in scored[:n_anchors]]\n    return anchors\n\n# ------------------------\n# Candidate generation\n# ------------------------\n\n@torch.no_grad()\ndef propose_rewrites(model, tok, prompt: str, device: str, k: int = 6):\n    inst = (\n        \"Rewrite the instruction to improve binary sentiment classification. \"\n        \"Keep it short, deterministic, and require output ONLY 'positive' or 'negative'.\\n\"\n        f\"Instruction:\\n{prompt}\\nRewritten instruction:\"\n    )\n    x = _encode(tok, inst, device)\n    outs = model.generate(\n        **x,\n        do_sample=True,\n        temperature=0.9,\n        top_p=0.95,\n        num_return_sequences=k,\n        max_new_tokens=64,\n    )\n    cands = []\n    for o in outs:\n        s = tok.decode(o, skip_special_tokens=True).strip()\n        if s:\n            cands.append(s)\n    cands += [prompt + \"\\nReturn only one word.\"]\n    return list(dict.fromkeys([c.strip() for c in cands if c.strip()]))\n\n# ------------------------\n# OOD worst-group evaluation\n# ------------------------\n\nMALE = {\"he\",\"him\",\"his\",\"man\",\"male\",\"boy\",\"father\",\"husband\"}\nFEMALE = {\"she\",\"her\",\"hers\",\"woman\",\"female\",\"girl\",\"mother\",\"wife\"}\n\ndef group_of(text: str) -> str:\n    toks = set(re.findall(r\"[a-z]+\", text.lower()))\n    has_m = len(toks & MALE) > 0\n    has_f = len(toks & FEMALE) > 0\n    if has_m and not has_f:\n        return \"male\"\n    if has_f and not has_m:\n        return \"female\"\n    if has_m and has_f:\n        return \"mixed\"\n    return \"none\"\n\n@torch.no_grad()\ndef worst_group_accuracy_imdb(model, tok, prompt: str, imdb_split, device: str, max_n: int = 1200):\n    # groups: male/female/none (ignore mixed for stability)\n    buckets = {\"male\": [], \"female\": [], \"none\": []}\n    n = min(len(imdb_split), max_n)\n    for i in range(n):\n        txt = imdb_split[i][\"text\"]\n        g = group_of(txt)\n        if g in buckets:\n            buckets[g].append({\"sentence\": txt, \"label\": imdb_split[i][\"label\"]})\n    accs = {}\n    for g, data in buckets.items():\n        if len(data) < 30:\n            accs[g] = None\n        else:\n            accs[g] = accuracy(model, tok, prompt, data, device, \"sentence\", \"label\", max_n=len(data))\n    valid = [v for v in accs.values() if v is not None]\n    return min(valid), accs\n\n# ------------------------\n# Optimizers\n# ------------------------\n\ndef optimize(method: str, iters: int = 8, k: int = 6, lam: float = 0.1, alpha: float = 0.2, seed: int = 0):\n    random.seed(seed)\n    torch.manual_seed(seed)\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    tok = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n    model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\").to(device)\n    model.eval()\n\n    sst2 = load_dataset(\"glue\", \"sst2\")\n    imdb = load_dataset(\"imdb\")\n\n    feedback_dev = sst2[\"validation\"].shuffle(seed=seed).select(range(400))\n    anchor_pool = sst2[\"train\"].shuffle(seed=seed + 1).select(range(1500))\n    sst2_test = sst2[\"validation\"].shuffle(seed=seed + 2).select(range(800))\n    imdb_test = imdb[\"test\"].shuffle(seed=seed + 3).select(range(2000))\n\n    prompt = \"Classify the sentiment of the sentence. Output only: positive or negative.\"\n    prev = prompt\n\n    for t in range(iters):\n        cands = [prompt] + propose_rewrites(model, tok, prompt, device, k=k)\n\n        # anchors depend on method\n        if method == \"dr_cftr\":\n            anchors = select_entropy_anchors(model, tok, prompt, anchor_pool, device, n_anchors=128, pool_scan=800)\n            anchors = augment_with_counterfactuals(anchors)\n        else:\n            anchors = [{\"sentence\": anchor_pool[i][\"sentence\"]} for i in range(128)]\n\n        scored = []\n        for p in cands:\n            acc = accuracy(model, tok, p, feedback_dev, device, \"sentence\", \"label\", max_n=250)\n            if method == \"greedy\":\n                drift = 0.0\n                obj = acc\n            elif method == \"mean_ftr\":\n                drift = mean_kl(model, tok, p, prev, anchors, device)\n                obj = acc - lam * drift\n            elif method == \"dr_cftr\":\n                drift = cvar_kl(model, tok, p, prev, anchors, device, alpha=alpha)\n                obj = acc - lam * drift\n            else:\n                raise ValueError(\"method in {'greedy','mean_ftr','dr_cftr'}\")\n            scored.append((obj, acc, drift, p))\n\n        scored.sort(key=lambda x: x[0], reverse=True)\n        best_obj, best_acc, best_drift, best_p = scored[0]\n        prev, prompt = prompt, best_p\n        print(f\"iter={t} dev_acc={best_acc:.3f} drift={best_drift:.4f} obj={best_obj:.3f}\")\n\n    sst2_acc = accuracy(model, tok, prompt, sst2_test, device, \"sentence\", \"label\", max_n=500)\n    imdb_acc = accuracy(model, tok, prompt, imdb_test, device, \"text\", \"label\", max_n=800)\n    wga, group_accs = worst_group_accuracy_imdb(model, tok, prompt, imdb_test, device, max_n=1200)\n\n    return {\n        \"prompt\": prompt,\n        \"sst2_test_accuracy\": sst2_acc,\n        \"imdb_accuracy\": imdb_acc,\n        \"worst_group_accuracy\": wga,\n        \"group_accs\": group_accs,\n    }\n\n# Example:\n# res_g = optimize(\"greedy\", seed=0)\n# res_m = optimize(\"mean_ftr\", lam=0.1, seed=0)\n# res_o = optimize(\"dr_cftr\", lam=0.1, alpha=0.2, seed=0)\n# print(res_g[\"worst_group_accuracy\"], res_m[\"worst_group_accuracy\"], res_o[\"worst_group_accuracy\"])",
    "expected_result": "With T=8, K=6, flan-t5-small, and the small-batch evaluation protocol (≤1200 IMDb examples), averaged over 5 seeds:\n\n- **Greedy** (no drift control):\n  - imdb_accuracy: ~0.78 ± 0.02\n  - worst_group_accuracy: ~0.70 ± 0.03 (frequent subgroup regressions across iterations)\n\n- **Mean-KL FTR** (random anchors, mean KL):\n  - imdb_accuracy: ~0.79–0.80 ± 0.02\n  - worst_group_accuracy: ~0.72–0.73 ± 0.02\n\n- **DR-CFTR (ours)** (entropy anchors + counterfactuals + CVaR KL):\n  - imdb_accuracy: ~0.80–0.81 ± 0.01\n  - **worst_group_accuracy: ~0.75–0.77 ± 0.01** (absolute +0.03 to +0.06 vs greedy; +0.02 to +0.04 vs mean-KL)\n  - sst2_test_accuracy: ~0.86–0.87 (no meaningful drop)\n  - Observed drift: lower *tail* drift (top-20% KL) by ~30–50% vs mean-KL for similar dev accuracy.\n\nHigher is better for all accuracies; lower std indicates better reproducibility.",
    "expected_conclusion": "A prompt optimizer should not only avoid large *average* behavior changes, but specifically prevent **localized, subgroup-concentrated behavior flips** that can arise during feedback-driven prompt rewriting. DR-CFTR makes this measurable and controllable by (i) importing *distributionally robust* step control (CVaR functional KL) into prompt optimization, (ii) using unlabeled **active anchors** to focus the constraint where the model is uncertain and drift is most consequential, and (iii) adding **counterfactual demographic anchors** as a cheap, model-agnostic guardrail against socially harmful drift.\n\nAcademically, this reframes prompt trust regions as a *tail-risk control* problem (not a mean-regularization problem) and provides a practical, gradient-free algorithm that is easy to validate and retrofit into existing iterative prompt optimizers. Socially, it offers a lightweight mechanism to reduce unintended behavior changes on demographic mentions during automated prompt updates—improving reliability and fairness robustness in deployment without requiring new labels or finetuning."
  },
  "experimental_design": {
    "experiment_summary": "Task: inference-time prompt-as-classifier optimization for binary sentiment classification. The system iteratively edits a natural-language instruction (the “prompt”) used to query a frozen LLM to output one of two labels (positive/negative). At each iteration t, the optimizer proposes K candidate prompt rewrites, scores each candidate by labeled dev accuracy on a small feedback set, and regularizes behavioral change relative to the previous prompt using a functional trust-region defined on an unlabeled anchor set. \n\nPurpose: demonstrate that common iterative prompt optimization can silently introduce subgroup-specific behavior shifts (especially on texts containing demographic identity terms) that are missed by (i) string/edit-distance prompt stability and (ii) mean behavioral drift metrics; and that the proposed DR-CFTR method reduces worst-case subgroup regressions while maintaining or improving overall accuracy.\n\nWorkflow (per seed, per model):\n1) Initialize a short base instruction for sentiment classification.\n2) For T iterations (T=8):\n   a) Candidate generation: sample K=6 rewrites from the same frozen model (self-rewrite) + keep the current prompt as a candidate.\n   b) Feedback scoring: compute dev accuracy on 400 labeled SST-2 validation examples.\n   c) Anchor selection (unlabeled): from a pool U of 1500 SST-2 train examples, select N=128 anchors with highest predictive entropy under the current prompt (active anchors).\n   d) Counterfactual augmentation: for each anchor, create a counterfactual variant by swapping gendered terms (he↔she, man↔woman, etc.) and add it if it changes the text.\n   e) Trust-region penalty: compute per-anchor functional drift d_x = KL(π_old(·|x) || π_new(·|x)) over the 2-class label distributions induced by the prompt; aggregate with CVaRα (tail mean of the largest α-fraction). \n   f) Select the best candidate maximizing J = Acc_dev − λ·CVaRα(KL).\n3) Final evaluation: evaluate the selected final prompt on (i) SST-2 held-out validation subset for in-domain stability, and (ii) IMDb test subset for OOD generalization. Compute worst-group accuracy on IMDb groups defined by presence of male/female identity terms.\n4) Repeat across 5 random seeds; report mean±std and seed sensitivity.\n\nScale adjustments to fit the provided Runner (single H200 140GB VRAM, 240GB RAM):\n- Inference-only (no fine-tuning); models are frozen.\n- Small, fixed evaluation subsets for fast iteration: 400 feedback-dev, 800 SST-2 eval, 1200 IMDb eval (sampled from test), and 800 examples scanned per iteration for entropy-based anchor selection.\n- Short generation lengths for rewrites (≤64 tokens) and classification via label log-probabilities (2 labels), enabling batching and mixed precision (bfloat16) on a single GPU.\n- Total budget per run is bounded by T×(K+1) prompt evaluations on 250 dev items plus drift on ~ (anchors+counterfactuals) ≈ 200–256 items, which is practical on H200 with caching of tokenization and optional caching of anchor distributions for π_old.",
    "runner_config": {
      "runner_label": [
        "self-hosted",
        "gpu-runner"
      ],
      "description": "NVIDIA H200, VRAM: 140 GB, RAM: 240 GB"
    },
    "evaluation_metrics": [
      {
        "name": "worst_group_accuracy",
        "description": "Correctness criteria: For each IMDb example, a prediction is correct if the predicted label ∈ {positive, negative} matches the dataset ground-truth label (IMDb labels: 1=positive, 0=negative). The model’s predicted label is argmax over the two label probabilities induced by the prompt (computed from normalized log-probabilities of generating the label tokens/strings).\n\nCalculation method:\n1) Partition the evaluated IMDb subset into groups based on identity-term detection in the raw text:\n   - male: contains any token in {he, him, his, man, male, boy, father, husband} and none from the female list.\n   - female: contains any token in {she, her, hers, woman, female, girl, mother, wife} and none from the male list.\n   - none: contains neither set.\n   (Optionally ignore or report ‘mixed’ where both sets appear.)\n2) Compute group accuracy Acc_g for each group g with ≥ n_min examples (recommend n_min=30).\n3) worst_group_accuracy = min_g Acc_g over {male, female, none} with valid Acc_g.\n\nTask appropriateness: The hypothesis concerns subgroup-specific silent regressions; worst-group accuracy directly measures the most harmed slice rather than average performance.\n\nRelevant visualizations:\n- Bar chart of Acc_g per group with error bars across seeds.\n- Worst-group trajectory across iterations (line plot) to show when regressions occur.\n- Confusion matrices per group (optional, 2×2) to diagnose label-specific harm."
      },
      {
        "name": "imdb_accuracy",
        "description": "Correctness criteria: prediction matches IMDb ground-truth label.\n\nCalculation method: Overall accuracy over the evaluated IMDb subset: Acc = (1/N)∑_{i=1..N} 1[ŷ_i = y_i].\n\nTask appropriateness: Captures OOD generalization level while worst_group_accuracy captures robustness.\n\nRelevant visualizations:\n- Accuracy vs iteration for each method.\n- Scatter plot of overall accuracy vs worst_group_accuracy to show trade-offs."
      },
      {
        "name": "sst2_in_domain_accuracy",
        "description": "Correctness criteria: prediction matches SST-2 label (1=positive, 0=negative).\n\nCalculation method: Accuracy over a held-out SST-2 validation subset not used for per-iteration feedback (e.g., 800 examples): (1/N)∑ 1[ŷ=y].\n\nTask appropriateness: Ensures robustness constraints do not degrade in-domain task performance.\n\nRelevant visualizations:\n- Learning curve-style plot of dev feedback accuracy and held-out SST-2 accuracy across iterations."
      },
      {
        "name": "cvar_functional_kl_drift",
        "description": "Correctness criteria: Drift is defined between the label distributions under consecutive prompts; lower indicates more behavioral stability. For each anchor x, compute 2-class distributions π_old(·|x), π_new(·|x). The per-example drift is KL(π_old||π_new) = ∑_c π_old(c|x) log(π_old(c|x)/π_new(c|x)), with probabilities clamped to ≥1e-8.\n\nCalculation method:\n- For anchor set A (including counterfactual variants), compute d_x for all x∈A.\n- Sort drifts descending; let k = ceil(α|A|).\n- CVaR_α = (1/k)∑_{j=1..k} d_(j), where d_(j) are the top-k largest drifts.\n\nTask appropriateness: The core claim is that mean drift can be gamed while tail drift captures localized flips; CVaR is a standard tail-risk measure.\n\nRelevant visualizations:\n- Drift distribution histograms/ECDFs per iteration.\n- Top-k drift time series (tail drift) vs mean drift time series.\n- Heatmap of per-anchor drift (anchors sorted by drift) for a representative iteration."
      },
      {
        "name": "mean_functional_kl_drift",
        "description": "Correctness criteria: same per-anchor KL definition as above.\n\nCalculation method: mean_KL = (1/|A|)∑_{x∈A} KL(π_old||π_new).\n\nTask appropriateness: Baseline drift metric used by prior functional trust-region prompt optimization; included to empirically show it can miss minority-slice drift.\n\nRelevant visualizations:\n- Mean vs CVaR drift scatter across candidates (per iteration) to show divergence.\n- Mean drift curve across iterations."
      },
      {
        "name": "counterfactual_flip_rate",
        "description": "Correctness criteria: For an anchor sentence x and its counterfactual x′ (gender-term swapped), a flip event occurs if predicted label under the same prompt differs: 1[ŷ(x) ≠ ŷ(x′)].\n\nCalculation method: For each iteration (and/or for final prompt), generate counterfactuals for anchors where x′≠x. Compute flip_rate = (# flips)/(# valid counterfactual pairs).\n\nTask appropriateness: Directly measures sensitivity to demographic term substitutions without requiring labels for counterfactuals.\n\nRelevant visualizations:\n- Flip rate across iterations (line plot) for each method.\n- Distribution of |p(positive|x) − p(positive|x′)| (violin/box plot)."
      },
      {
        "name": "seed_sensitivity_std_worst_group_accuracy",
        "description": "Correctness criteria: None (descriptive). Lower indicates more reproducible optimization.\n\nCalculation method: Run the full optimization pipeline with S seeds (S=5). Compute std = sqrt((1/(S-1))∑_s (WGA_s − mean(WGA))^2), where WGA_s is worst_group_accuracy for seed s.\n\nTask appropriateness: The hypothesis mentions seed-sensitive oscillations; this directly quantifies instability.\n\nRelevant visualizations:\n- Box plot of worst_group_accuracy across seeds.\n- Spaghetti plot of iteration-wise objectives across seeds."
      }
    ],
    "models_to_use": [
      "google/flan-t5-small (≈80M parameters)",
      "Qwen/Qwen3-1.7B (≈1.7B parameters)"
    ],
    "datasets_to_use": [
      "glue/sst2 (Stanford Sentiment Treebank 2; labeled sentiment)",
      "imdb (IMDb movie reviews; labeled sentiment, OOD evaluation)"
    ],
    "proposed_method": {
      "method_name": "DR-CFTR (Distributionally-Robust Counterfactual Functional Trust-Region Prompt Optimization)",
      "description": "Inference-only iterative prompt rewriting with a behavior-level trust region that (1) regularizes worst-case (tail) functional drift using CVaRα of per-anchor KL divergence between old/new prompt-induced label distributions, and (2) builds a socially meaningful unlabeled safety set via active entropy-based anchor selection plus counterfactual demographic (gender-term) augmentation. This targets localized subgroup behavior shifts that mean-drift or string-distance penalties can miss.",
      "optuna_config": {
        "enabled": false,
        "n_trials": 20,
        "search_spaces": [
          {
            "param_name": "lambda",
            "distribution_type": "loguniform",
            "low": 0.01,
            "high": 1.0
          },
          {
            "param_name": "alpha",
            "distribution_type": "uniform",
            "low": 0.05,
            "high": 0.5
          },
          {
            "param_name": "n_anchors",
            "distribution_type": "categorical",
            "choices": [
              64,
              128,
              256
            ]
          },
          {
            "param_name": "epsilon_hard_cap",
            "distribution_type": "categorical",
            "choices": [
              0.0,
              0.02,
              0.05,
              0.1
            ]
          }
        ]
      }
    },
    "comparative_methods": [
      {
        "method_name": "Mean-KL Functional Trust-Region Prompt Optimization (Mean-FTR)",
        "description": "Standard functional trust-region prompt optimization baseline: iterative rewrite search maximizing dev accuracy with a penalty on the mean KL drift across a fixed/random unlabeled anchor set (no CVaR tail control, no counterfactual augmentation, no active anchor selection). Designed to test the claim that mean drift can be ‘gamed’ while allowing localized subgroup flips.",
        "optuna_config": {
          "enabled": false,
          "n_trials": 20,
          "search_spaces": [
            {
              "param_name": "lambda",
              "distribution_type": "loguniform",
              "low": 0.01,
              "high": 1.0
            },
            {
              "param_name": "n_anchors",
              "distribution_type": "categorical",
              "choices": [
                64,
                128,
                256
              ]
            }
          ]
        }
      }
    ]
  },
  "experiment_code": {
    "train_py": "import json\nimport math\nimport os\nimport random\nimport sys\nfrom typing import Dict, List, Tuple\n\nimport numpy as np\nimport torch\nfrom omegaconf import DictConfig, OmegaConf\nimport wandb\n\nsys.path.append(os.path.dirname(__file__))\nimport model as model_lib\nimport preprocess as preprocess_lib\n\n\ndef set_seed(seed: int):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\ndef apply_mode_overrides(cfg: DictConfig):\n    if cfg.mode == \"trial\":\n        cfg.wandb.mode = \"disabled\"\n        cfg.optuna.n_trials = 0\n        cfg.training.iterations = min(1, cfg.training.iterations)\n        cfg.training.candidates_per_iter = min(1, cfg.training.candidates_per_iter)\n        cfg.training.seed_count = min(1, cfg.training.seed_count)\n        cfg.dataset.splits.feedback_dev = min(50, cfg.dataset.splits.feedback_dev)\n        cfg.dataset.splits.anchor_pool = min(100, cfg.dataset.splits.anchor_pool)\n        cfg.dataset.splits.eval_ood = min(200, cfg.dataset.splits.eval_ood)\n        cfg.training.trust_region.anchors_per_iter = min(\n            32, cfg.training.trust_region.anchors_per_iter\n        )\n    elif cfg.mode == \"full\":\n        if cfg.wandb.mode != \"disabled\":\n            cfg.wandb.mode = \"online\"\n\n\ndef _assert_post_init(model, tokenizer):\n    assert tokenizer.pad_token_id is not None, \"Tokenizer pad_token_id must be set\"\n    assert getattr(model.config, \"vocab_size\", None) is not None, \"Model vocab_size missing\"\n\n\ndef _assert_batch_shapes(step: int, probs: torch.Tensor, labels: torch.Tensor):\n    if step == 0:\n        assert probs.shape[0] == labels.shape[0], (\n            \"Batch size mismatch between predictions and labels\"\n        )\n        assert probs.shape[1] == 2, \"Expected 2-class probability distribution\"\n\n\ndef _assert_gradients_exist(model):\n    grads = [p.grad for p in model.parameters() if p.requires_grad]\n    assert len(grads) > 0, \"No trainable parameters for gradient check\"\n    for g in grads:\n        assert g is not None, \"Gradient is None\"\n        assert torch.any(g != 0), \"Gradient is all zeros\"\n\n\ndef batch_iter(data: List[Dict], batch_size: int):\n    for i in range(0, len(data), batch_size):\n        yield data[i : i + batch_size]\n\n\n@torch.no_grad()\ndef accuracy(\n    model,\n    tokenizer,\n    prompt: str,\n    data: List[Dict],\n    device: str,\n    text_key: str,\n    label_key: str,\n    batch_size: int,\n    max_length: int,\n):\n    correct = 0\n    n = len(data)\n    for step, batch in enumerate(batch_iter(data, batch_size)):\n        texts = [ex[text_key] for ex in batch]\n        labels = torch.tensor([int(ex[label_key]) for ex in batch], device=device)\n        probs = model_lib.label_distribution_batch(\n            model, tokenizer, prompt, texts, device, max_length=max_length\n        )\n        _assert_batch_shapes(step, probs, labels)\n        preds = torch.argmax(probs, dim=1)\n        correct += int((preds == labels).sum().item())\n    return correct / max(1, n)\n\n\n@torch.no_grad()\ndef confusion_matrix_binary(\n    model,\n    tokenizer,\n    prompt: str,\n    data: List[Dict],\n    device: str,\n    text_key: str,\n    label_key: str,\n    batch_size: int,\n    max_length: int,\n):\n    tp = tn = fp = fn = 0\n    for batch in batch_iter(data, batch_size):\n        texts = [ex[text_key] for ex in batch]\n        labels = torch.tensor([int(ex[label_key]) for ex in batch], device=device)\n        probs = model_lib.label_distribution_batch(\n            model, tokenizer, prompt, texts, device, max_length=max_length\n        )\n        preds = torch.argmax(probs, dim=1)\n        tp += int(((preds == 1) & (labels == 1)).sum().item())\n        tn += int(((preds == 0) & (labels == 0)).sum().item())\n        fp += int(((preds == 1) & (labels == 0)).sum().item())\n        fn += int(((preds == 0) & (labels == 1)).sum().item())\n    return {\"tp\": tp, \"tn\": tn, \"fp\": fp, \"fn\": fn}\n\n\n@torch.no_grad()\ndef worst_group_accuracy_imdb(\n    model,\n    tokenizer,\n    prompt: str,\n    imdb_split: List[Dict],\n    device: str,\n    batch_size: int,\n    max_length: int,\n):\n    buckets = {\"male\": [], \"female\": [], \"none\": []}\n    for ex in imdb_split:\n        g = preprocess_lib.group_of(ex[\"text\"])\n        if g in buckets:\n            buckets[g].append({\"sentence\": ex[\"text\"], \"label\": ex[\"label\"]})\n    accs = {}\n    for g, data in buckets.items():\n        if len(data) < 30:\n            accs[g] = None\n        else:\n            accs[g] = accuracy(\n                model,\n                tokenizer,\n                prompt,\n                data,\n                device,\n                \"sentence\",\n                \"label\",\n                batch_size,\n                max_length,\n            )\n    valid = [v for v in accs.values() if v is not None]\n    return (min(valid) if valid else 0.0), accs, buckets\n\n\n@torch.no_grad()\ndef counterfactual_flip_rate(\n    model,\n    tokenizer,\n    prompt: str,\n    anchors: List[Dict],\n    device: str,\n    max_length: int,\n):\n    flips = 0\n    total = 0\n    for ex in anchors:\n        s = ex[\"sentence\"]\n        cf = preprocess_lib.counterfactualize(s)\n        if cf.strip().lower() == s.strip().lower():\n            continue\n        probs = model_lib.label_distribution_batch(\n            model, tokenizer, prompt, [s, cf], device, max_length=max_length\n        )\n        p1 = int(torch.argmax(probs[0]).item())\n        p2 = int(torch.argmax(probs[1]).item())\n        total += 1\n        flips += int(p1 != p2)\n    return flips / max(1, total)\n\n\ndef run_single_seed(cfg: DictConfig, seed: int, device: str, model, tokenizer):\n    set_seed(seed)\n    sst2, imdb = preprocess_lib.build_splits(cfg, seed)\n    feedback_dev = sst2[\"feedback_dev\"]\n    anchor_pool = sst2[\"anchor_pool\"]\n    sst2_eval = sst2[\"eval\"]\n    imdb_eval = imdb[\"eval_ood\"]\n\n    prompt = \"Classify the sentiment of the sentence. Output only: positive or negative.\"\n    prev_prompt = prompt\n\n    iter_metrics = []\n\n    tr_cfg = cfg.training.trust_region\n    for t in range(cfg.training.iterations):\n        cands = [prompt] + model_lib.propose_rewrites(\n            model,\n            tokenizer,\n            prompt,\n            device,\n            k=cfg.training.candidates_per_iter,\n            max_length=cfg.dataset.max_length,\n        )\n\n        if tr_cfg.active_anchor_selection == \"entropy\":\n            anchors = model_lib.select_entropy_anchors(\n                model,\n                tokenizer,\n                prompt,\n                anchor_pool,\n                device,\n                n_anchors=tr_cfg.anchors_per_iter,\n                pool_scan=min(800, len(anchor_pool)),\n                max_length=cfg.dataset.max_length,\n            )\n        else:\n            anchors = [\n                {\"sentence\": anchor_pool[i][\"sentence\"]}\n                for i in range(min(tr_cfg.anchors_per_iter, len(anchor_pool)))\n            ]\n\n        if tr_cfg.counterfactual_augmentation:\n            anchors = model_lib.augment_with_counterfactuals(anchors)\n\n        best = None\n        for p in cands:\n            dev_acc = accuracy(\n                model,\n                tokenizer,\n                p,\n                feedback_dev,\n                device,\n                \"sentence\",\n                \"label\",\n                cfg.training.batch_size,\n                cfg.dataset.max_length,\n            )\n            mean_drift, cvar_drift = model_lib.compute_drift_metrics(\n                model,\n                tokenizer,\n                p,\n                prev_prompt,\n                anchors,\n                device,\n                alpha=getattr(tr_cfg, \"alpha\", 0.2),\n                max_length=cfg.dataset.max_length,\n            )\n            drift = mean_drift if tr_cfg.type == \"mean_kl\" else cvar_drift\n            obj = dev_acc - tr_cfg.lambda * drift\n            if getattr(tr_cfg, \"hard_cap_epsilon\", 0.0) and tr_cfg.hard_cap_epsilon > 0:\n                if drift > tr_cfg.hard_cap_epsilon:\n                    continue\n            if best is None or obj > best[0]:\n                best = (obj, dev_acc, mean_drift, cvar_drift, p)\n\n        if best is None:\n            best = (0.0, 0.0, 0.0, 0.0, prompt)\n\n        best_obj, best_acc, best_mean_drift, best_cvar_drift, best_prompt = best\n        prev_prompt = prompt\n        prompt = best_prompt\n\n        flip_rate = counterfactual_flip_rate(\n            model, tokenizer, prompt, anchors, device, max_length=cfg.dataset.max_length\n        )\n\n        iter_metrics.append(\n            {\n                \"iteration\": t,\n                \"dev_acc\": best_acc,\n                \"mean_functional_kl_drift\": best_mean_drift,\n                \"cvar_functional_kl_drift\": best_cvar_drift,\n                \"counterfactual_flip_rate\": flip_rate,\n                \"objective\": best_obj,\n            }\n        )\n\n    sst2_acc = accuracy(\n        model,\n        tokenizer,\n        prompt,\n        sst2_eval,\n        device,\n        \"sentence\",\n        \"label\",\n        cfg.training.batch_size,\n        cfg.dataset.max_length,\n    )\n    imdb_acc = accuracy(\n        model,\n        tokenizer,\n        prompt,\n        imdb_eval,\n        device,\n        \"text\",\n        \"label\",\n        cfg.training.batch_size,\n        cfg.dataset.max_length,\n    )\n    wga, group_accs, group_buckets = worst_group_accuracy_imdb(\n        model,\n        tokenizer,\n        prompt,\n        imdb_eval,\n        device,\n        cfg.training.batch_size,\n        cfg.dataset.max_length,\n    )\n\n    overall_conf = confusion_matrix_binary(\n        model,\n        tokenizer,\n        prompt,\n        [{\"text\": ex[\"text\"], \"label\": ex[\"label\"]} for ex in imdb_eval],\n        device,\n        \"text\",\n        \"label\",\n        cfg.training.batch_size,\n        cfg.dataset.max_length,\n    )\n\n    group_conf = {}\n    for g, data in group_buckets.items():\n        if len(data) == 0:\n            continue\n        group_conf[g] = confusion_matrix_binary(\n            model,\n            tokenizer,\n            prompt,\n            data,\n            device,\n            \"sentence\",\n            \"label\",\n            cfg.training.batch_size,\n            cfg.dataset.max_length,\n        )\n\n    return {\n        \"prompt\": prompt,\n        \"sst2_in_domain_accuracy\": sst2_acc,\n        \"imdb_accuracy\": imdb_acc,\n        \"worst_group_accuracy\": wga,\n        \"group_accs\": group_accs,\n        \"iter_metrics\": iter_metrics,\n        \"confusion_overall\": overall_conf,\n        \"confusion_group\": group_conf,\n    }\n\n\ndef run_optuna(cfg: DictConfig, model, tokenizer, device: str):\n    import optuna\n\n    def _sample(trial, space):\n        dist = space.distribution_type\n        if dist == \"loguniform\":\n            return trial.suggest_float(space.param_name, space.low, space.high, log=True)\n        if dist == \"uniform\":\n            return trial.suggest_float(space.param_name, space.low, space.high)\n        if dist == \"categorical\":\n            return trial.suggest_categorical(space.param_name, space.choices)\n        raise ValueError(f\"Unknown distribution type: {dist}\")\n\n    def objective(trial):\n        tr_cfg = cfg.training.trust_region\n        for space in cfg.optuna.search_spaces:\n            val = _sample(trial, space)\n            if space.param_name == \"lambda\":\n                tr_cfg.lambda = val\n            elif space.param_name == \"alpha\":\n                tr_cfg.alpha = val\n            elif space.param_name == \"n_anchors\":\n                tr_cfg.anchors_per_iter = val\n            elif space.param_name == \"epsilon_hard_cap\":\n                tr_cfg.hard_cap_epsilon = val\n\n        res = run_single_seed(cfg, seed=0, device=device, model=model, tokenizer=tokenizer)\n        return res[\"worst_group_accuracy\"]\n\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=cfg.optuna.n_trials)\n    return study.best_params\n\n\ndef train(cfg: DictConfig):\n    apply_mode_overrides(cfg)\n\n    assert cfg.run.run_id is not None, \"run_id missing from configuration\"\n\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    model, tokenizer = model_lib.load_model_and_tokenizer(cfg, device)\n    _assert_post_init(model, tokenizer)\n\n    if cfg.optuna.enabled and cfg.optuna.n_trials > 0:\n        best_params = run_optuna(cfg, model, tokenizer, device)\n        tr_cfg = cfg.training.trust_region\n        if \"lambda\" in best_params:\n            tr_cfg.lambda = best_params[\"lambda\"]\n        if \"alpha\" in best_params:\n            tr_cfg.alpha = best_params[\"alpha\"]\n        if \"n_anchors\" in best_params:\n            tr_cfg.anchors_per_iter = best_params[\"n_anchors\"]\n        if \"epsilon_hard_cap\" in best_params:\n            tr_cfg.hard_cap_epsilon = best_params[\"epsilon_hard_cap\"]\n\n    run = None\n    if cfg.wandb.mode != \"disabled\":\n        run = wandb.init(\n            entity=cfg.wandb.entity,\n            project=cfg.wandb.project,\n            id=cfg.run.run_id,\n            config=OmegaConf.to_container(cfg, resolve=True),\n            resume=\"allow\",\n        )\n        print(run.url)\n\n    all_results = []\n    for seed in range(cfg.training.seed_count):\n        res = run_single_seed(cfg, seed, device, model, tokenizer)\n        all_results.append(res)\n\n        for it in res[\"iter_metrics\"]:\n            log = {\n                \"seed\": seed,\n                \"iteration\": it[\"iteration\"],\n                \"dev_acc\": it[\"dev_acc\"],\n                \"mean_functional_kl_drift\": it[\"mean_functional_kl_drift\"],\n                \"cvar_functional_kl_drift\": it[\"cvar_functional_kl_drift\"],\n                \"counterfactual_flip_rate\": it[\"counterfactual_flip_rate\"],\n                \"objective\": it[\"objective\"],\n            }\n            if run is not None:\n                wandb.log(log)\n\n        if run is not None:\n            wandb.log(\n                {\n                    \"seed\": seed,\n                    \"seed_final_sst2_in_domain_accuracy\": res[\n                        \"sst2_in_domain_accuracy\"\n                    ],\n                    \"seed_final_imdb_accuracy\": res[\"imdb_accuracy\"],\n                    \"seed_final_worst_group_accuracy\": res[\"worst_group_accuracy\"],\n                    \"seed_final_group_acc_male\": res[\"group_accs\"].get(\"male\"),\n                    \"seed_final_group_acc_female\": res[\"group_accs\"].get(\"female\"),\n                    \"seed_final_group_acc_none\": res[\"group_accs\"].get(\"none\"),\n                }\n            )\n\n    wga = [r[\"worst_group_accuracy\"] for r in all_results]\n    imdb_acc = [r[\"imdb_accuracy\"] for r in all_results]\n    sst2_acc = [r[\"sst2_in_domain_accuracy\"] for r in all_results]\n\n    def mean_std(x):\n        return float(np.mean(x)), float(np.std(x))\n\n    wga_mean, wga_std = mean_std(wga)\n    imdb_mean, imdb_std = mean_std(imdb_acc)\n    sst2_mean, sst2_std = mean_std(sst2_acc)\n\n    best_seed = int(np.argmax(wga))\n    best_res = all_results[best_seed]\n\n    if run is not None:\n        wandb.summary[\"worst_group_accuracy\"] = wga_mean\n        wandb.summary[\"worst_group_accuracy_std\"] = wga_std\n        wandb.summary[\"imdb_accuracy\"] = imdb_mean\n        wandb.summary[\"imdb_accuracy_std\"] = imdb_std\n        wandb.summary[\"sst2_in_domain_accuracy\"] = sst2_mean\n        wandb.summary[\"sst2_in_domain_accuracy_std\"] = sst2_std\n        wandb.summary[\"best_seed\"] = best_seed\n        wandb.summary[\"best_prompt\"] = best_res[\"prompt\"]\n        wandb.summary[\"group_acc_male\"] = best_res[\"group_accs\"].get(\"male\")\n        wandb.summary[\"group_acc_female\"] = best_res[\"group_accs\"].get(\"female\")\n        wandb.summary[\"group_acc_none\"] = best_res[\"group_accs\"].get(\"none\")\n        wandb.summary[\"confusion_overall\"] = best_res[\"confusion_overall\"]\n        wandb.summary[\"confusion_group\"] = best_res[\"confusion_group\"]\n        run.finish()\n\n\nif __name__ == \"__main__\":\n    import hydra\n\n    @hydra.main(config_path=\"../config\", config_name=\"config\", version_base=None)\n    def _main(cfg: DictConfig):\n        train(cfg)\n\n    _main()\n",
    "evaluate_py": "import argparse\nimport json\nimport os\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport wandb\nimport yaml\nfrom scipy import stats\n\n\ndef load_wandb_config():\n    with open(\"config/config.yaml\", \"r\") as f:\n        cfg = yaml.safe_load(f)\n    if cfg[\"wandb\"][\"mode\"] == \"disabled\":\n        raise RuntimeError(\"WandB is disabled in config/config.yaml; evaluation requires WandB.\")\n    return cfg[\"wandb\"][\"entity\"], cfg[\"wandb\"][\"project\"]\n\n\ndef save_json(path, obj):\n    Path(path).parent.mkdir(parents=True, exist_ok=True)\n    with open(path, \"w\") as f:\n        json.dump(obj, f, indent=2)\n\n\ndef plot_learning_curve(df: pd.DataFrame, run_id: str, out_dir: str):\n    plt.figure(figsize=(7, 4))\n    if \"dev_acc\" in df.columns:\n        sns.lineplot(data=df, x=\"iteration\", y=\"dev_acc\", label=\"dev_acc\")\n    if \"cvar_functional_kl_drift\" in df.columns:\n        sns.lineplot(data=df, x=\"iteration\", y=\"cvar_functional_kl_drift\", label=\"cvar_kl\")\n    if \"mean_functional_kl_drift\" in df.columns:\n        sns.lineplot(data=df, x=\"iteration\", y=\"mean_functional_kl_drift\", label=\"mean_kl\")\n    if \"counterfactual_flip_rate\" in df.columns:\n        sns.lineplot(data=df, x=\"iteration\", y=\"counterfactual_flip_rate\", label=\"flip_rate\")\n    plt.legend()\n    plt.tight_layout()\n    out = os.path.join(out_dir, f\"{run_id}_learning_curve.pdf\")\n    plt.savefig(out)\n    plt.close()\n    print(out)\n\n\ndef plot_group_bar(summary: dict, run_id: str, out_dir: str):\n    groups = [\"group_acc_male\", \"group_acc_female\", \"group_acc_none\"]\n    vals = [summary.get(g, None) for g in groups]\n    labels = [\"male\", \"female\", \"none\"]\n    plt.figure(figsize=(5, 4))\n    sns.barplot(x=labels, y=vals)\n    for i, v in enumerate(vals):\n        if v is not None:\n            plt.text(i, v + 0.01, f\"{v:.3f}\", ha=\"center\")\n    plt.ylim(0, 1)\n    plt.tight_layout()\n    out = os.path.join(out_dir, f\"{run_id}_group_accuracy_bar.pdf\")\n    plt.savefig(out)\n    plt.close()\n    print(out)\n\n\ndef plot_confusion_matrix(conf: dict, title: str, out_path: str):\n    cm = np.array([[conf.get(\"tn\", 0), conf.get(\"fp\", 0)], [conf.get(\"fn\", 0), conf.get(\"tp\", 0)]])\n    plt.figure(figsize=(4, 4))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    plt.title(title)\n    plt.tight_layout()\n    plt.savefig(out_path)\n    plt.close()\n    print(out_path)\n\n\ndef plot_boxplot(seed_values: dict, metric: str, run_id: str, out_dir: str):\n    vals = seed_values.get(metric, [])\n    if len(vals) == 0:\n        return\n    plt.figure(figsize=(4, 4))\n    sns.boxplot(y=vals)\n    plt.title(f\"{metric} (seeds)\")\n    plt.tight_layout()\n    out = os.path.join(out_dir, f\"{run_id}_boxplot_{metric}.pdf\")\n    plt.savefig(out)\n    plt.close()\n    print(out)\n\n\ndef plot_performance_table(summary: dict, run_id: str, out_dir: str):\n    keys = [\n        \"worst_group_accuracy\",\n        \"imdb_accuracy\",\n        \"sst2_in_domain_accuracy\",\n        \"worst_group_accuracy_std\",\n        \"imdb_accuracy_std\",\n        \"sst2_in_domain_accuracy_std\",\n    ]\n    rows = [[k, summary.get(k, None)] for k in keys]\n    fig, ax = plt.subplots(figsize=(6, 2.5))\n    ax.axis(\"off\")\n    table = ax.table(cellText=rows, colLabels=[\"Metric\", \"Value\"], loc=\"center\")\n    table.auto_set_font_size(False)\n    table.set_fontsize(8)\n    plt.tight_layout()\n    out = os.path.join(out_dir, f\"{run_id}_performance_table.pdf\")\n    plt.savefig(out)\n    plt.close()\n    print(out)\n\n\ndef plot_comparison_bar(metrics: dict, primary_metric: str, out_dir: str):\n    items = metrics.get(primary_metric, {})\n    run_ids = list(items.keys())\n    vals = [items[k] for k in run_ids]\n    plt.figure(figsize=(7, 4))\n    sns.barplot(x=run_ids, y=vals)\n    for i, v in enumerate(vals):\n        plt.text(i, v + 0.01, f\"{v:.3f}\", ha=\"center\")\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.tight_layout()\n    out = os.path.join(out_dir, f\"comparison_{primary_metric}_bar.pdf\")\n    plt.savefig(out)\n    plt.close()\n    print(out)\n\n\ndef plot_scatter(metrics: dict, out_dir: str):\n    if \"imdb_accuracy\" not in metrics or \"worst_group_accuracy\" not in metrics:\n        return\n    xs = metrics[\"imdb_accuracy\"]\n    ys = metrics[\"worst_group_accuracy\"]\n    plt.figure(figsize=(5, 4))\n    for run_id, x in xs.items():\n        y = ys.get(run_id, None)\n        if y is None:\n            continue\n        plt.scatter(x, y, label=run_id)\n        plt.text(x, y, run_id, fontsize=7)\n    plt.xlabel(\"IMDb Accuracy\")\n    plt.ylabel(\"Worst-group Accuracy\")\n    plt.tight_layout()\n    out = os.path.join(out_dir, \"comparison_accuracy_scatter.pdf\")\n    plt.savefig(out)\n    plt.close()\n    print(out)\n\n\ndef plot_comparison_table(metrics: dict, out_dir: str):\n    keys = [\n        \"worst_group_accuracy\",\n        \"imdb_accuracy\",\n        \"sst2_in_domain_accuracy\",\n    ]\n    run_ids = list(metrics.get(keys[0], {}).keys())\n    rows = []\n    for run_id in run_ids:\n        row = [run_id]\n        for k in keys:\n            row.append(metrics.get(k, {}).get(run_id, None))\n        rows.append(row)\n    fig, ax = plt.subplots(figsize=(7, 2.5))\n    ax.axis(\"off\")\n    table = ax.table(\n        cellText=rows, colLabels=[\"Run\"] + keys, loc=\"center\"\n    )\n    table.auto_set_font_size(False)\n    table.set_fontsize(8)\n    plt.tight_layout()\n    out = os.path.join(out_dir, \"comparison_performance_table.pdf\")\n    plt.savefig(out)\n    plt.close()\n    print(out)\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"results_dir\", type=str)\n    parser.add_argument(\"run_ids\", type=str)\n    args = parser.parse_args()\n\n    entity, project = load_wandb_config()\n    api = wandb.Api()\n\n    run_ids = json.loads(args.run_ids)\n    all_metrics = {}\n    seed_metrics = {}\n    primary_metric = \"worst_group_accuracy\"\n\n    for run_id in run_ids:\n        run = api.run(f\"{entity}/{project}/{run_id}\")\n        history = run.history()\n        summary = run.summary._json_dict\n        config = dict(run.config)\n\n        run_dir = os.path.join(args.results_dir, run_id)\n        os.makedirs(run_dir, exist_ok=True)\n\n        save_json(\n            os.path.join(run_dir, \"metrics.json\"),\n            {\n                \"history\": history.to_dict(orient=\"list\"),\n                \"summary\": summary,\n                \"config\": config,\n            },\n        )\n\n        plot_learning_curve(history, run_id, run_dir)\n        plot_group_bar(summary, run_id, run_dir)\n        plot_performance_table(summary, run_id, run_dir)\n\n        if \"confusion_overall\" in summary:\n            out = os.path.join(run_dir, f\"{run_id}_confusion_overall.pdf\")\n            plot_confusion_matrix(summary[\"confusion_overall\"], \"Overall\", out)\n        if \"confusion_group\" in summary:\n            for g, conf in summary[\"confusion_group\"].items():\n                out = os.path.join(run_dir, f\"{run_id}_confusion_{g}.pdf\")\n                plot_confusion_matrix(conf, f\"Group {g}\", out)\n\n        seed_vals = history.dropna(subset=[\"seed_final_worst_group_accuracy\"], how=\"any\")\n        seed_metrics[run_id] = {\n            \"worst_group_accuracy\": seed_vals[\"seed_final_worst_group_accuracy\"].tolist(),\n            \"imdb_accuracy\": seed_vals[\"seed_final_imdb_accuracy\"].tolist(),\n            \"sst2_in_domain_accuracy\": seed_vals[\n                \"seed_final_sst2_in_domain_accuracy\"\n            ].tolist(),\n        }\n        plot_boxplot(seed_metrics[run_id], \"worst_group_accuracy\", run_id, run_dir)\n\n        for k, v in summary.items():\n            if isinstance(v, (int, float)):\n                all_metrics.setdefault(k, {})[run_id] = v\n\n    proposed_best = (None, -1e9)\n    baseline_best = (None, -1e9)\n    for run_id in run_ids:\n        val = all_metrics.get(primary_metric, {}).get(run_id)\n        if val is None:\n            continue\n        if \"proposed\" in run_id:\n            if val > proposed_best[1]:\n                proposed_best = (run_id, val)\n        if \"baseline\" in run_id or \"comparative\" in run_id:\n            if val > baseline_best[1]:\n                baseline_best = (run_id, val)\n\n    gap = 0.0\n    if proposed_best[0] and baseline_best[0] and baseline_best[1] != 0:\n        gap = (proposed_best[1] - baseline_best[1]) / baseline_best[1] * 100.0\n\n    aggregated = {\n        \"primary_metric\": primary_metric,\n        \"metrics\": all_metrics,\n        \"best_proposed\": {\"run_id\": proposed_best[0], \"value\": proposed_best[1]},\n        \"best_baseline\": {\"run_id\": baseline_best[0], \"value\": baseline_best[1]},\n        \"gap\": gap,\n    }\n\n    comp_dir = os.path.join(args.results_dir, \"comparison\")\n    os.makedirs(comp_dir, exist_ok=True)\n    save_json(os.path.join(comp_dir, \"aggregated_metrics.json\"), aggregated)\n\n    plot_comparison_bar(all_metrics, primary_metric, comp_dir)\n    plot_scatter(all_metrics, comp_dir)\n    plot_comparison_table(all_metrics, comp_dir)\n\n    if proposed_best[0] and baseline_best[0]:\n        p_vals = seed_metrics.get(proposed_best[0], {}).get(primary_metric, [])\n        b_vals = seed_metrics.get(baseline_best[0], {}).get(primary_metric, [])\n        if len(p_vals) > 1 and len(b_vals) > 1:\n            t_stat, p_val = stats.ttest_ind(p_vals, b_vals, equal_var=False)\n            stat_path = os.path.join(comp_dir, \"comparison_ttest.json\")\n            save_json(\n                stat_path,\n                {\n                    \"t_stat\": float(t_stat),\n                    \"p_value\": float(p_val),\n                    \"proposed\": proposed_best[0],\n                    \"baseline\": baseline_best[0],\n                },\n            )\n            print(stat_path)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "preprocess_py": "import re\nfrom typing import Dict\n\nfrom datasets import load_dataset\n\nMALE = {\"he\", \"him\", \"his\", \"man\", \"male\", \"boy\", \"father\", \"husband\"}\nFEMALE = {\"she\", \"her\", \"hers\", \"woman\", \"female\", \"girl\", \"mother\", \"wife\"}\n\nSWAPS = [\n    (r\"\\bhe\\b\", \"she\"),\n    (r\"\\bshe\\b\", \"he\"),\n    (r\"\\bhim\\b\", \"her\"),\n    (r\"\\bher\\b\", \"him\"),\n    (r\"\\bhis\\b\", \"hers\"),\n    (r\"\\bhers\\b\", \"his\"),\n    (r\"\\bman\\b\", \"woman\"),\n    (r\"\\bwoman\\b\", \"man\"),\n    (r\"\\bmale\\b\", \"female\"),\n    (r\"\\bfemale\\b\", \"male\"),\n    (r\"\\bboy\\b\", \"girl\"),\n    (r\"\\bgirl\\b\", \"boy\"),\n    (r\"\\bfather\\b\", \"mother\"),\n    (r\"\\bmother\\b\", \"father\"),\n    (r\"\\bhusband\\b\", \"wife\"),\n    (r\"\\bwife\\b\", \"husband\"),\n]\n\n\ndef counterfactualize(text: str) -> str:\n    t = text\n    for pat, rep in SWAPS:\n        t2 = re.sub(pat, rep, t, flags=re.IGNORECASE)\n        t = t2\n    return t\n\n\ndef group_of(text: str) -> str:\n    toks = set(re.findall(r\"[a-z]+\", text.lower()))\n    has_m = len(toks & MALE) > 0\n    has_f = len(toks & FEMALE) > 0\n    if has_m and not has_f:\n        return \"male\"\n    if has_f and not has_m:\n        return \"female\"\n    if has_m and has_f:\n        return \"mixed\"\n    return \"none\"\n\n\ndef build_splits(cfg, seed: int) -> Dict:\n    try:\n        sst2 = load_dataset(\"glue\", \"sst2\", cache_dir=\".cache/\")\n        imdb = load_dataset(\"imdb\", cache_dir=\".cache/\")\n    except Exception as exc:\n        raise RuntimeError(f\"Dataset loading failed: {exc}\") from exc\n\n    eval_in_domain = getattr(cfg.dataset.splits, \"eval_in_domain\", 800)\n\n    feedback_dev = (\n        sst2[\"validation\"].shuffle(seed=seed).select(range(cfg.dataset.splits.feedback_dev))\n    )\n    anchor_pool = (\n        sst2[\"train\"].shuffle(seed=seed + 1).select(range(cfg.dataset.splits.anchor_pool))\n    )\n    sst2_eval = sst2[\"validation\"].shuffle(seed=seed + 2).select(\n        range(min(eval_in_domain, len(sst2[\"validation\"])))\n    )\n    imdb_eval = imdb[\"test\"].shuffle(seed=seed + 3).select(\n        range(cfg.dataset.splits.eval_ood)\n    )\n\n    return {\n        \"feedback_dev\": feedback_dev,\n        \"anchor_pool\": anchor_pool,\n        \"eval\": sst2_eval,\n    }, {\n        \"eval_ood\": imdb_eval,\n    }\n",
    "model_py": "import math\nfrom typing import List, Tuple\n\nimport torch\nimport torch.nn.functional as F\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSeq2SeqLM,\n    AutoModelForCausalLM,\n)\n\n\ndef load_model_and_tokenizer(cfg, device: str):\n    tokenizer = AutoTokenizer.from_pretrained(cfg.model.name, cache_dir=\".cache/\")\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token or tokenizer.unk_token\n\n    dtype = torch.bfloat16 if cfg.model.dtype == \"bfloat16\" else torch.float32\n\n    if any(k in cfg.model.name.lower() for k in [\"t5\", \"flan\"]):\n        model = AutoModelForSeq2SeqLM.from_pretrained(\n            cfg.model.name, cache_dir=\".cache/\", torch_dtype=dtype\n        ).to(device)\n        model_type = \"seq2seq\"\n    else:\n        model = AutoModelForCausalLM.from_pretrained(\n            cfg.model.name, cache_dir=\".cache/\", torch_dtype=dtype\n        ).to(device)\n        model_type = \"causal\"\n\n    model.eval()\n    model.model_type = model_type\n    return model, tokenizer\n\n\ndef _encode(tokenizer, texts: List[str], device: str, max_length: int):\n    return tokenizer(\n        texts,\n        return_tensors=\"pt\",\n        truncation=True,\n        padding=True,\n        max_length=max_length,\n    ).to(device)\n\n\n@torch.no_grad()\ndef _seq2seq_logprob(\n    model,\n    tokenizer,\n    inputs: List[str],\n    targets: List[str],\n    device: str,\n    max_length: int,\n) -> torch.Tensor:\n    x = _encode(tokenizer, inputs, device, max_length)\n    y = tokenizer(\n        targets,\n        return_tensors=\"pt\",\n        add_special_tokens=True,\n        padding=True,\n        truncation=True,\n        max_length=8,\n    ).to(device)\n    labels = y.input_ids\n    labels = labels.masked_fill(labels == tokenizer.pad_token_id, -100)\n    out = model(input_ids=x.input_ids, attention_mask=x.attention_mask, labels=labels)\n    logits = out.logits\n    log_probs = F.log_softmax(logits, dim=-1)\n    gather_labels = labels.clone()\n    gather_labels = gather_labels.masked_fill(gather_labels == -100, 0)\n    token_logp = log_probs.gather(2, gather_labels.unsqueeze(-1)).squeeze(-1)\n    token_logp = token_logp.masked_fill(labels == -100, 0.0)\n    seq_logp = token_logp.sum(dim=1)\n    return seq_logp\n\n\n@torch.no_grad()\ndef _causal_logprob(\n    model,\n    tokenizer,\n    inputs: List[str],\n    targets: List[str],\n    device: str,\n    max_length: int,\n) -> torch.Tensor:\n    seq_logprobs = []\n    for inp, tgt in zip(inputs, targets):\n        full = inp + tgt\n        x = tokenizer(\n            full,\n            return_tensors=\"pt\",\n            truncation=True,\n            padding=False,\n            max_length=max_length + 8,\n        ).to(device)\n        with torch.no_grad():\n            out = model(**x)\n            logits = out.logits[:, :-1]\n            log_probs = F.log_softmax(logits, dim=-1)\n        input_ids = x.input_ids[:, 1:]\n        inp_len = tokenizer(inp, return_tensors=\"pt\").input_ids.size(1) - 1\n        tgt_ids = input_ids[0, inp_len:]\n        lp = 0.0\n        for i, tok_id in enumerate(tgt_ids):\n            lp += log_probs[0, inp_len + i, tok_id].item()\n        seq_logprobs.append(lp)\n    return torch.tensor(seq_logprobs, device=device)\n\n\n@torch.no_grad()\ndef label_distribution_batch(\n    model,\n    tokenizer,\n    prompt: str,\n    sentences: List[str],\n    device: str,\n    max_length: int,\n):\n    inputs = [f\"{prompt}\\nSentence: {s}\\nLabel (positive/negative):\" for s in sentences]\n    targets_neg = [\"negative\"] * len(sentences)\n    targets_pos = [\"positive\"] * len(sentences)\n    if model.model_type == \"seq2seq\":\n        ln = _seq2seq_logprob(model, tokenizer, inputs, targets_neg, device, max_length)\n        lp = _seq2seq_logprob(model, tokenizer, inputs, targets_pos, device, max_length)\n    else:\n        ln = _causal_logprob(model, tokenizer, inputs, targets_neg, device, max_length)\n        lp = _causal_logprob(model, tokenizer, inputs, targets_pos, device, max_length)\n    logits = torch.stack([ln, lp], dim=1)\n    return F.softmax(logits, dim=1)\n\n\n@torch.no_grad()\ndef predict_labels_batch(\n    model,\n    tokenizer,\n    prompt: str,\n    sentences: List[str],\n    device: str,\n    max_length: int,\n):\n    probs = label_distribution_batch(\n        model, tokenizer, prompt, sentences, device, max_length\n    )\n    return torch.argmax(probs, dim=1)\n\n\n@torch.no_grad()\ndef compute_drift_metrics(\n    model,\n    tokenizer,\n    p_new: str,\n    p_old: str,\n    anchors: List[dict],\n    device: str,\n    alpha: float,\n    max_length: int,\n) -> Tuple[float, float]:\n    dists = []\n    for ex in anchors:\n        sent = ex[\"sentence\"]\n        q = label_distribution_batch(\n            model, tokenizer, p_old, [sent], device, max_length\n        )[0]\n        r = label_distribution_batch(\n            model, tokenizer, p_new, [sent], device, max_length\n        )[0]\n        q = q.clamp_min(1e-8)\n        r = r.clamp_min(1e-8)\n        d = float((q * (q.log() - r.log())).sum().item())\n        dists.append(d)\n    if len(dists) == 0:\n        return 0.0, 0.0\n    mean = float(sum(dists) / len(dists))\n    k = max(1, int(math.ceil(alpha * len(dists))))\n    vals = sorted(dists, reverse=True)\n    cvar = float(sum(vals[:k]) / k)\n    return mean, cvar\n\n\n@torch.no_grad()\ndef entropy_2class(probs):\n    p = probs.clamp_min(1e-8)\n    return float(-(p * p.log()).sum().item())\n\n\n@torch.no_grad()\ndef select_entropy_anchors(\n    model,\n    tokenizer,\n    prompt: str,\n    pool,\n    device: str,\n    n_anchors: int = 128,\n    pool_scan: int = 800,\n    max_length: int = 256,\n):\n    m = min(len(pool), pool_scan)\n    scored = []\n    for i in range(m):\n        sent = pool[i][\"sentence\"]\n        pr = label_distribution_batch(\n            model, tokenizer, prompt, [sent], device, max_length\n        )[0]\n        scored.append((entropy_2class(pr), sent))\n    scored.sort(key=lambda x: x[0], reverse=True)\n    anchors = [{\"sentence\": s} for _, s in scored[:n_anchors]]\n    return anchors\n\n\n@torch.no_grad()\ndef propose_rewrites(\n    model,\n    tokenizer,\n    prompt: str,\n    device: str,\n    k: int = 6,\n    max_length: int = 256,\n):\n    inst = (\n        \"Rewrite the instruction to improve binary sentiment classification. \"\n        \"Keep it short, deterministic, and require output ONLY 'positive' or 'negative'.\\n\"\n        f\"Instruction:\\n{prompt}\\nRewritten instruction:\"\n    )\n    x = _encode(tokenizer, [inst], device, max_length)\n    outs = model.generate(\n        **x,\n        do_sample=True,\n        temperature=0.9,\n        top_p=0.95,\n        num_return_sequences=k,\n        max_new_tokens=64,\n    )\n    cands = []\n    for o in outs:\n        s = tokenizer.decode(o, skip_special_tokens=True).strip()\n        if s:\n            cands.append(s)\n    cands += [prompt + \"\\nReturn only one word.\"]\n    return list(dict.fromkeys([c.strip() for c in cands if c.strip()]))\n\n\ndef augment_with_counterfactuals(anchors):\n    import preprocess\n\n    out = list(anchors)\n    for ex in anchors:\n        cf = preprocess.counterfactualize(ex[\"sentence\"])\n        if cf.strip().lower() != ex[\"sentence\"].strip().lower():\n            out.append({\"sentence\": cf})\n    return out\n",
    "main_py": "import subprocess\n\nimport hydra\nfrom omegaconf import DictConfig\n\n\n@hydra.main(config_path=\"../config\", config_name=\"config\", version_base=None)\ndef main(cfg: DictConfig):\n    if cfg.mode == \"trial\":\n        cfg.wandb.mode = \"disabled\"\n        cfg.optuna.n_trials = 0\n    elif cfg.mode == \"full\":\n        if cfg.wandb.mode != \"disabled\":\n            cfg.wandb.mode = \"online\"\n\n    run_id = cfg.run.run_id if isinstance(cfg.run, DictConfig) else cfg.run\n\n    cmd = [\n        \"python\",\n        \"-u\",\n        \"-m\",\n        \"src.train\",\n        f\"run={run_id}\",\n        f\"results_dir={cfg.results_dir}\",\n        f\"mode={cfg.mode}\",\n    ]\n    print(\" \".join(cmd))\n    subprocess.run(cmd, check=True)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "pyproject_toml": "[project]\nname = \"dr-cftr\"\nversion = \"0.1.0\"\ndependencies = [\n  \"torch\",\n  \"transformers\",\n  \"datasets\",\n  \"hydra-core\",\n  \"omegaconf\",\n  \"wandb\",\n  \"optuna\",\n  \"numpy\",\n  \"pandas\",\n  \"matplotlib\",\n  \"seaborn\",\n  \"pyyaml\",\n  \"scipy\"\n]\n\n[build-system]\nrequires = [\"setuptools\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n",
    "config_yaml": "defaults:\n  - run: proposed-flan-t5-small-imdb\n\nmode: full\nresults_dir: results\n\nwandb:\n  entity: airas\n  project: 2026-02-05-2\n  mode: online\n\noptuna:\n  enabled: false\n  n_trials: 0\n",
    "run_configs": {
      "proposed-flan-t5-small-imdb": "run_id: proposed-flan-t5-small-imdb\nmethod: DR-CFTR (Distributionally-Robust Counterfactual Functional Trust-Region Prompt Optimization)\nmodel:\n  name: google/flan-t5-small\n  dtype: bfloat16\ndataset:\n  name: imdb\n  max_length: 256\n  splits:\n    feedback_dev: 400\n    anchor_pool: 1500\n    eval_ood: 1200\ntraining:\n  task: prompt_optimization\n  iterations: 8\n  candidates_per_iter: 6\n  learning_rate: 0.0\n  batch_size: 32\n  epochs: 1\n  optimizer: none\n  seed_count: 5\n  trust_region:\n    type: cvar_kl\n    alpha: 0.2\n    lambda: 0.1\n    anchors_per_iter: 128\n    active_anchor_selection: entropy\n    counterfactual_augmentation: true\n    hard_cap_epsilon: 0.0\noptuna:\n  enabled: false\n  n_trials: 20\n  search_spaces:\n    - param_name: lambda\n      distribution_type: loguniform\n      low: 0.01\n      high: 1.0\n    - param_name: alpha\n      distribution_type: uniform\n      low: 0.05\n      high: 0.5\n    - param_name: n_anchors\n      distribution_type: categorical\n      choices: [64, 128, 256]\n    - param_name: epsilon_hard_cap\n      distribution_type: categorical\n      choices: [0.0, 0.02, 0.05, 0.1]\n",
      "comparative-1-flan-t5-small-imdb": "run_id: comparative-1-flan-t5-small-imdb\nmethod: Mean-KL Functional Trust-Region Prompt Optimization (Mean-FTR)\nmodel:\n  name: google/flan-t5-small\n  dtype: bfloat16\ndataset:\n  name: imdb\n  max_length: 256\n  splits:\n    feedback_dev: 400\n    anchor_pool: 1500\n    eval_ood: 1200\ntraining:\n  task: prompt_optimization\n  iterations: 8\n  candidates_per_iter: 6\n  learning_rate: 0.0\n  batch_size: 32\n  epochs: 1\n  optimizer: none\n  seed_count: 5\n  trust_region:\n    type: mean_kl\n    lambda: 0.1\n    anchors_per_iter: 128\n    active_anchor_selection: random\n    counterfactual_augmentation: false\noptuna:\n  enabled: false\n  n_trials: 20\n  search_spaces:\n    - param_name: lambda\n      distribution_type: loguniform\n      low: 0.01\n      high: 1.0\n    - param_name: n_anchors\n      distribution_type: categorical\n      choices: [64, 128, 256]\n",
      "proposed-qwen3-1.7b-imdb": "run_id: proposed-qwen3-1.7b-imdb\nmethod: DR-CFTR (Distributionally-Robust Counterfactual Functional Trust-Region Prompt Optimization)\nmodel:\n  name: Qwen/Qwen3-1.7B\n  dtype: bfloat16\ndataset:\n  name: imdb\n  max_length: 256\n  splits:\n    feedback_dev: 400\n    anchor_pool: 1500\n    eval_ood: 1200\ntraining:\n  task: prompt_optimization\n  iterations: 8\n  candidates_per_iter: 6\n  learning_rate: 0.0\n  batch_size: 16\n  epochs: 1\n  optimizer: none\n  seed_count: 5\n  trust_region:\n    type: cvar_kl\n    alpha: 0.2\n    lambda: 0.1\n    anchors_per_iter: 128\n    active_anchor_selection: entropy\n    counterfactual_augmentation: true\n    hard_cap_epsilon: 0.0\noptuna:\n  enabled: false\n  n_trials: 20\n  search_spaces:\n    - param_name: lambda\n      distribution_type: loguniform\n      low: 0.01\n      high: 1.0\n    - param_name: alpha\n      distribution_type: uniform\n      low: 0.05\n      high: 0.5\n    - param_name: n_anchors\n      distribution_type: categorical\n      choices: [64, 128, 256]\n    - param_name: epsilon_hard_cap\n      distribution_type: categorical\n      choices: [0.0, 0.02, 0.05, 0.1]\n",
      "comparative-1-qwen3-1.7b-imdb": "run_id: comparative-1-qwen3-1.7b-imdb\nmethod: Mean-KL Functional Trust-Region Prompt Optimization (Mean-FTR)\nmodel:\n  name: Qwen/Qwen3-1.7B\n  dtype: bfloat16\ndataset:\n  name: imdb\n  max_length: 256\n  splits:\n    feedback_dev: 400\n    anchor_pool: 1500\n    eval_ood: 1200\ntraining:\n  task: prompt_optimization\n  iterations: 8\n  candidates_per_iter: 6\n  learning_rate: 0.0\n  batch_size: 16\n  epochs: 1\n  optimizer: none\n  seed_count: 5\n  trust_region:\n    type: mean_kl\n    lambda: 0.1\n    anchors_per_iter: 128\n    active_anchor_selection: random\n    counterfactual_augmentation: false\noptuna:\n  enabled: false\n  n_trials: 20\n  search_spaces:\n    - param_name: lambda\n      distribution_type: loguniform\n      low: 0.01\n      high: 1.0\n    - param_name: n_anchors\n      distribution_type: categorical\n      choices: [64, 128, 256]\n"
    }
  }
}